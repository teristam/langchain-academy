{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with External Memory - Practice Exercises\n",
    "\n",
    "## Overview\n",
    "This notebook provides hands-on exercises to practice building conversational AI systems with persistent external memory using database checkpointers. You'll learn to create chatbots that maintain conversation state across sessions and restarts.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of these exercises, you will:\n",
    "- Understand external database checkpointers vs in-memory storage\n",
    "- Set up and use SQLite checkpointers for persistent conversation memory\n",
    "- Build chatbots that survive application restarts and maintain long-term memory\n",
    "- Implement conversation threading and user session management\n",
    "- Create backup and recovery systems for conversation data\n",
    "- Design scalable memory architectures for production chatbots\n",
    "\n",
    "## Prerequisites\n",
    "- Completed the chatbot-external-memory.ipynb tutorial\n",
    "- Understanding of conversation summarization concepts\n",
    "- Basic knowledge of databases and data persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph-checkpoint-sqlite langchain_core langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Optional: Set up LangSmith for tracing\n",
    "# _set_env(\"LANGSMITH_API_KEY\")\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic SQLite Persistent Chatbot\n",
    "\n",
    "### Task\n",
    "Create a chatbot that uses SQLite for persistent memory, allowing conversations to continue across application restarts.\n",
    "\n",
    "### TODO: Set up SQLite checkpointer and basic persistent chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage, AIMessage\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from typing import Literal\n",
    "\n",
    "# TODO: Set up SQLite database connection\n",
    "# Create a local database file for persistent storage\n",
    "db_path = \"persistent_chatbot.db\"\n",
    "# TODO: Create SQLite connection\n",
    "# TODO: Create SqliteSaver checkpointer\n",
    "\n",
    "# Initialize model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# TODO: Define persistent chatbot state (reuse SummarizationState from previous exercise)\n",
    "class PersistentChatState(MessagesState):\n",
    "    # TODO: Add summary and user_id fields\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement persistent chat nodes (similar to previous summarization exercise)\n",
    "def persistent_chat_node(state: PersistentChatState):\n",
    "    print(f\"Processing chat for user: {state.get('user_id', 'unknown')}\")\n",
    "    \n",
    "    # TODO: Get summary and messages\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # TODO: Add summary to system message if it exists\n",
    "    # TODO: Get response from model\n",
    "    # TODO: Return response\n",
    "    pass\n",
    "\n",
    "# TODO: Implement persistent summarization\n",
    "def persistent_summarize(state: PersistentChatState):\n",
    "    print(\"Creating persistent summary...\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    current_summary = state.get(\"summary\", \"\")\n",
    "    \n",
    "    # TODO: Create or extend summary\n",
    "    # TODO: Remove old messages except last 2\n",
    "    # TODO: Return updated summary and cleaned messages\n",
    "    pass\n",
    "\n",
    "# TODO: Implement decision function\n",
    "def should_summarize_persistent(state: PersistentChatState) -> Literal[\"summarize\", \"continue\"]:\n",
    "    # TODO: Return \"summarize\" if more than 6 messages, otherwise \"continue\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build persistent chatbot graph\n",
    "builder_persistent = StateGraph(PersistentChatState)\n",
    "# TODO: Add nodes and edges (similar structure to previous summarization exercise)\n",
    "# TODO: Compile with SQLite checkpointer\n",
    "\n",
    "display(Image(graph_persistent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test persistent chatbot - Part 1\n",
    "user_id = \"alice_123\"\n",
    "config = {\"configurable\": {\"thread_id\": f\"persistent_chat_{user_id}\"}}\n",
    "\n",
    "# Start a conversation\n",
    "conversation_part_1 = [\n",
    "    \"Hi, I'm Alice and I'm working on a Python web application.\",\n",
    "    \"I'm using FastAPI and need help with database integration.\",\n",
    "    \"Should I use SQLAlchemy or a simpler solution?\",\n",
    "    \"I'm storing user profiles and transaction data.\"\n",
    "]\n",
    "\n",
    "print(\"=== Conversation Part 1 ===\")\n",
    "for i, user_input in enumerate(conversation_part_1, 1):\n",
    "    print(f\"\\nTurn {i}:\")\n",
    "    print(f\"Alice: {user_input}\")\n",
    "    \n",
    "    result = graph_persistent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)], \"user_id\": user_id},\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    last_message = result[\"messages\"][-1]\n",
    "    print(f\"Assistant: {last_message.content[:100]}...\")\n",
    "    \n",
    "    if result.get(\"summary\"):\n",
    "        print(f\"Summary: {result['summary'][:100]}...\")\n",
    "\n",
    "print(f\"\\n=== Conversation state persisted to database ===\")\n",
    "print(f\"Messages in conversation: {len(result['messages'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test persistence by \"restarting\" the application\n",
    "print(\"\\n=== Simulating Application Restart ===\")\n",
    "print(\"Creating new graph instance with same database...\")\n",
    "\n",
    "# TODO: Create new connection and checkpointer (simulating app restart)\n",
    "new_conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "new_memory = SqliteSaver(new_conn)\n",
    "\n",
    "# TODO: Create new graph instance\n",
    "graph_persistent_restarted = builder_persistent.compile(checkpointer=new_memory)\n",
    "\n",
    "# TODO: Continue conversation with same thread_id\n",
    "conversation_part_2 = [\n",
    "    \"I'm back! Do you remember what we were discussing?\",\n",
    "    \"Great! Can you help me set up the database models?\",\n",
    "    \"What about handling database migrations?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Conversation Part 2 (After Restart) ===\")\n",
    "for i, user_input in enumerate(conversation_part_2, 1):\n",
    "    print(f\"\\nTurn {i}:\")\n",
    "    print(f\"Alice: {user_input}\")\n",
    "    \n",
    "    result = graph_persistent_restarted.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)], \"user_id\": user_id},\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    last_message = result[\"messages\"][-1]\n",
    "    print(f\"Assistant: {last_message.content[:100]}...\")\n",
    "    \n",
    "    if result.get(\"summary\"):\n",
    "        print(f\"Summary: {result['summary'][:100]}...\")\n",
    "\n",
    "print(\"\\n=== Persistence Test Complete! ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Multi-User Persistent Chat System\n",
    "\n",
    "### Task\n",
    "Create a system that can handle multiple users with isolated conversation threads, each with their own persistent memory.\n",
    "\n",
    "### TODO: Implement multi-user persistent system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# TODO: Define multi-user state with user management\n",
    "class MultiUserChatState(MessagesState):\n",
    "    summary: str\n",
    "    user_id: str\n",
    "    # TODO: Add user_profile, conversation_metadata, session_info\n",
    "    pass\n",
    "\n",
    "# TODO: Implement user profile management\n",
    "class UserProfileManager:\n",
    "    def __init__(self, db_connection):\n",
    "        self.conn = db_connection\n",
    "        self.create_user_tables()\n",
    "    \n",
    "    def create_user_tables(self):\n",
    "        # TODO: Create tables for user profiles and conversation metadata\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS user_profiles (\n",
    "                user_id TEXT PRIMARY KEY,\n",
    "                username TEXT,\n",
    "                preferences TEXT,\n",
    "                created_at TEXT,\n",
    "                last_active TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS conversation_metadata (\n",
    "                thread_id TEXT PRIMARY KEY,\n",
    "                user_id TEXT,\n",
    "                conversation_title TEXT,\n",
    "                created_at TEXT,\n",
    "                last_message_at TEXT,\n",
    "                message_count INTEGER\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def create_or_update_user(self, user_id: str, username: str, preferences: Dict = None):\n",
    "        # TODO: Create or update user profile\n",
    "        pass\n",
    "    \n",
    "    def get_user_conversations(self, user_id: str) -> List[Dict]:\n",
    "        # TODO: Get all conversations for a user\n",
    "        pass\n",
    "\n",
    "# TODO: Set up multi-user database\n",
    "multiuser_db_path = \"multiuser_chatbot.db\"\n",
    "multiuser_conn = sqlite3.connect(multiuser_db_path, check_same_thread=False)\n",
    "multiuser_memory = SqliteSaver(multiuser_conn)\n",
    "profile_manager = UserProfileManager(multiuser_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement multi-user chat nodes\n",
    "def multiuser_chat_node(state: MultiUserChatState):\n",
    "    print(f\"Processing chat for user: {state['user_id']}\")\n",
    "    \n",
    "    # TODO: Get user profile and customize response accordingly\n",
    "    # TODO: Add user context to system message\n",
    "    # TODO: Generate personalized response\n",
    "    pass\n",
    "\n",
    "def update_user_activity(state: MultiUserChatState):\n",
    "    print(f\"Updating activity for user: {state['user_id']}\")\n",
    "    \n",
    "    # TODO: Update user's last activity timestamp\n",
    "    # TODO: Update conversation metadata\n",
    "    # TODO: Track conversation statistics\n",
    "    pass\n",
    "\n",
    "def multiuser_summarize(state: MultiUserChatState):\n",
    "    print(f\"Creating personalized summary for user: {state['user_id']}\")\n",
    "    \n",
    "    # TODO: Create user-specific summary\n",
    "    # TODO: Consider user preferences in summarization style\n",
    "    # TODO: Update conversation metadata\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build multi-user graph\n",
    "builder_multiuser = StateGraph(MultiUserChatState)\n",
    "# TODO: Add nodes with user activity tracking\n",
    "# TODO: Include update_user_activity in the flow\n",
    "\n",
    "graph_multiuser = builder_multiuser.compile(checkpointer=multiuser_memory)\n",
    "display(Image(graph_multiuser.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test multi-user system\n",
    "# Create multiple users with different conversation threads\n",
    "users = [\n",
    "    {\"user_id\": \"alice_dev\", \"username\": \"Alice\", \"interests\": [\"Python\", \"Web Development\"]},\n",
    "    {\"user_id\": \"bob_data\", \"username\": \"Bob\", \"interests\": [\"Data Science\", \"Machine Learning\"]},\n",
    "    {\"user_id\": \"charlie_mobile\", \"username\": \"Charlie\", \"interests\": [\"Mobile Apps\", \"React Native\"]}\n",
    "]\n",
    "\n",
    "# TODO: Create user profiles\n",
    "for user in users:\n",
    "    profile_manager.create_or_update_user(\n",
    "        user[\"user_id\"], \n",
    "        user[\"username\"], \n",
    "        {\"interests\": user[\"interests\"]}\n",
    "    )\n",
    "\n",
    "# TODO: Simulate conversations for each user\n",
    "conversations = {\n",
    "    \"alice_dev\": [\n",
    "        \"Hi, I'm Alice. I'm building a REST API with Python.\",\n",
    "        \"I need help with authentication and authorization.\",\n",
    "        \"Should I use JWT tokens or session-based auth?\"\n",
    "    ],\n",
    "    \"bob_data\": [\n",
    "        \"Hello, I'm Bob. I work with data science projects.\",\n",
    "        \"I'm analyzing customer behavior data using pandas.\",\n",
    "        \"What's the best way to handle missing values?\"\n",
    "    ],\n",
    "    \"charlie_mobile\": [\n",
    "        \"Hey, I'm Charlie. I develop mobile applications.\",\n",
    "        \"I'm using React Native for cross-platform development.\",\n",
    "        \"How do I handle offline data synchronization?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# TODO: Process conversations for each user\n",
    "for user_id, messages in conversations.items():\n",
    "    print(f\"\\n=== Conversation with {user_id} ===\")\n",
    "    config = {\"configurable\": {\"thread_id\": f\"thread_{user_id}_{uuid.uuid4().hex[:8]}\"}}\n",
    "    \n",
    "    for i, message in enumerate(messages, 1):\n",
    "        print(f\"\\nTurn {i}:\")\n",
    "        print(f\"User: {message}\")\n",
    "        \n",
    "        result = graph_multiuser.invoke(\n",
    "            {\n",
    "                \"messages\": [HumanMessage(content=message)], \n",
    "                \"user_id\": user_id,\n",
    "                \"user_profile\": users[[u[\"user_id\"] for u in users].index(user_id)]\n",
    "            },\n",
    "            config\n",
    "        )\n",
    "        \n",
    "        last_message = result[\"messages\"][-1]\n",
    "        print(f\"Assistant: {last_message.content[:80]}...\")\n",
    "\n",
    "print(\"\\n=== Multi-User Test Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Conversation Backup and Recovery System\n",
    "\n",
    "### Task\n",
    "Implement a backup and recovery system for conversation data, including export/import functionality and data migration.\n",
    "\n",
    "### TODO: Implement backup and recovery system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "class ConversationBackupManager:\n",
    "    def __init__(self, db_path: str, backup_dir: str = \"backups\"):\n",
    "        self.db_path = db_path\n",
    "        self.backup_dir = Path(backup_dir)\n",
    "        self.backup_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def create_database_backup(self) -> str:\n",
    "        \"\"\"Create a full database backup.\"\"\"\n",
    "        # TODO: Create timestamped database backup\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        backup_filename = f\"chatbot_backup_{timestamp}.db\"\n",
    "        backup_path = self.backup_dir / backup_filename\n",
    "        \n",
    "        # TODO: Copy database file\n",
    "        # TODO: Return backup path\n",
    "        pass\n",
    "    \n",
    "    def export_conversations_json(self, user_id: str = None) -> str:\n",
    "        \"\"\"Export conversations to JSON format.\"\"\"\n",
    "        # TODO: Connect to database and extract conversation data\n",
    "        # TODO: Convert to JSON format with metadata\n",
    "        # TODO: Save to file and return path\n",
    "        pass\n",
    "    \n",
    "    def import_conversations_json(self, json_file_path: str) -> bool:\n",
    "        \"\"\"Import conversations from JSON file.\"\"\"\n",
    "        # TODO: Load JSON data\n",
    "        # TODO: Validate data format\n",
    "        # TODO: Insert into database with conflict resolution\n",
    "        pass\n",
    "    \n",
    "    def create_compressed_backup(self) -> str:\n",
    "        \"\"\"Create a compressed backup with metadata.\"\"\"\n",
    "        # TODO: Create ZIP archive with database and metadata\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        backup_filename = f\"chatbot_full_backup_{timestamp}.zip\"\n",
    "        backup_path = self.backup_dir / backup_filename\n",
    "        \n",
    "        # TODO: Add database file, user profiles, and metadata to ZIP\n",
    "        pass\n",
    "    \n",
    "    def restore_from_backup(self, backup_path: str) -> bool:\n",
    "        \"\"\"Restore database from backup.\"\"\"\n",
    "        # TODO: Validate backup file\n",
    "        # TODO: Restore database with confirmation\n",
    "        # TODO: Handle different backup formats (DB, ZIP, JSON)\n",
    "        pass\n",
    "    \n",
    "    def list_backups(self) -> List[Dict]:\n",
    "        \"\"\"List available backups with metadata.\"\"\"\n",
    "        # TODO: Scan backup directory\n",
    "        # TODO: Extract metadata from backup files\n",
    "        # TODO: Return sorted list with creation dates and sizes\n",
    "        pass\n",
    "\n",
    "# TODO: Initialize backup manager\n",
    "backup_manager = ConversationBackupManager(multiuser_db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test backup functionality\n",
    "print(\"=== Testing Backup System ===\")\n",
    "\n",
    "# Create database backup\n",
    "print(\"\\n1. Creating database backup...\")\n",
    "db_backup_path = backup_manager.create_database_backup()\n",
    "print(f\"Database backup created: {db_backup_path}\")\n",
    "\n",
    "# Export conversations to JSON\n",
    "print(\"\\n2. Exporting conversations to JSON...\")\n",
    "json_backup_path = backup_manager.export_conversations_json(user_id=\"alice_dev\")\n",
    "print(f\"JSON export created: {json_backup_path}\")\n",
    "\n",
    "# Create compressed backup\n",
    "print(\"\\n3. Creating compressed backup...\")\n",
    "zip_backup_path = backup_manager.create_compressed_backup()\n",
    "print(f\"Compressed backup created: {zip_backup_path}\")\n",
    "\n",
    "# List all backups\n",
    "print(\"\\n4. Listing all backups...\")\n",
    "backups = backup_manager.list_backups()\n",
    "for backup in backups:\n",
    "    print(f\"  - {backup['filename']}: {backup['size']} bytes, created {backup['created']}\")\n",
    "\n",
    "print(\"\\n=== Backup System Test Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test recovery functionality\n",
    "print(\"\\n=== Testing Recovery System ===\")\n",
    "\n",
    "# TODO: Simulate data loss by creating a new database\n",
    "recovery_test_db = \"recovery_test.db\"\n",
    "recovery_conn = sqlite3.connect(recovery_test_db, check_same_thread=False)\n",
    "recovery_memory = SqliteSaver(recovery_conn)\n",
    "\n",
    "print(\"\\n1. Created empty database for recovery test\")\n",
    "\n",
    "# TODO: Test restoration from backup\n",
    "print(\"\\n2. Restoring from backup...\")\n",
    "restoration_success = backup_manager.restore_from_backup(db_backup_path)\n",
    "print(f\"Restoration {'successful' if restoration_success else 'failed'}\")\n",
    "\n",
    "# TODO: Verify data integrity after restoration\n",
    "print(\"\\n3. Verifying data integrity...\")\n",
    "# Create new graph with restored database\n",
    "recovery_graph = builder_multiuser.compile(checkpointer=recovery_memory)\n",
    "\n",
    "# Test with existing conversation thread\n",
    "test_config = {\"configurable\": {\"thread_id\": \"thread_alice_dev_test\"}}\n",
    "test_state = recovery_graph.get_state(test_config)\n",
    "print(f\"Recovered conversation has {len(test_state.values.get('messages', []))} messages\")\n",
    "\n",
    "print(\"\\n=== Recovery System Test Complete ===\")\n",
    "\n",
    "# Cleanup\n",
    "recovery_conn.close()\n",
    "if Path(recovery_test_db).exists():\n",
    "    Path(recovery_test_db).unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Scalable Memory Architecture\n",
    "\n",
    "### Task\n",
    "Design a scalable memory architecture that can handle high-throughput conversations with automatic cleanup and optimization.\n",
    "\n",
    "### TODO: Implement scalable memory system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Callable\n",
    "\n",
    "class ScalableMemoryManager:\n",
    "    def __init__(self, db_path: str, max_connections: int = 10):\n",
    "        self.db_path = db_path\n",
    "        self.max_connections = max_connections\n",
    "        self.connection_pool = []\n",
    "        self.cleanup_thread = None\n",
    "        self.is_running = True\n",
    "        \n",
    "        # TODO: Initialize connection pool\n",
    "        self.init_connection_pool()\n",
    "        \n",
    "        # TODO: Start background cleanup thread\n",
    "        self.start_cleanup_thread()\n",
    "    \n",
    "    def init_connection_pool(self):\n",
    "        \"\"\"Initialize connection pool for concurrent access.\"\"\"\n",
    "        # TODO: Create pool of database connections\n",
    "        for i in range(self.max_connections):\n",
    "            conn = sqlite3.connect(self.db_path, check_same_thread=False)\n",
    "            conn.execute(\"PRAGMA journal_mode=WAL\")  # Enable WAL mode for better concurrency\n",
    "            self.connection_pool.append(conn)\n",
    "    \n",
    "    def get_connection(self) -> sqlite3.Connection:\n",
    "        \"\"\"Get connection from pool.\"\"\"\n",
    "        # TODO: Implement connection pool management\n",
    "        # In a real implementation, use proper connection pooling\n",
    "        return self.connection_pool[threading.current_thread().ident % len(self.connection_pool)]\n",
    "    \n",
    "    def start_cleanup_thread(self):\n",
    "        \"\"\"Start background thread for automatic cleanup.\"\"\"\n",
    "        # TODO: Implement background cleanup thread\n",
    "        def cleanup_worker():\n",
    "            while self.is_running:\n",
    "                try:\n",
    "                    self.cleanup_old_conversations()\n",
    "                    self.optimize_database()\n",
    "                    time.sleep(3600)  # Run every hour\n",
    "                except Exception as e:\n",
    "                    print(f\"Cleanup error: {e}\")\n",
    "        \n",
    "        self.cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)\n",
    "        self.cleanup_thread.start()\n",
    "    \n",
    "    def cleanup_old_conversations(self, days_old: int = 30):\n",
    "        \"\"\"Remove conversations older than specified days.\"\"\"\n",
    "        # TODO: Implement cleanup logic\n",
    "        conn = self.get_connection()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # TODO: Delete old conversation data\n",
    "        # TODO: Preserve important conversations marked by users\n",
    "        pass\n",
    "    \n",
    "    def optimize_database(self):\n",
    "        \"\"\"Optimize database performance.\"\"\"\n",
    "        # TODO: Run database optimization commands\n",
    "        conn = self.get_connection()\n",
    "        conn.execute(\"VACUUM\")\n",
    "        conn.execute(\"ANALYZE\")\n",
    "        conn.commit()\n",
    "    \n",
    "    def get_memory_stats(self) -> Dict:\n",
    "        \"\"\"Get memory usage statistics.\"\"\"\n",
    "        # TODO: Calculate database size, conversation counts, etc.\n",
    "        conn = self.get_connection()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get database size\n",
    "        db_size = Path(self.db_path).stat().st_size\n",
    "        \n",
    "        # TODO: Get conversation statistics\n",
    "        # TODO: Get active users count\n",
    "        # TODO: Return comprehensive stats\n",
    "        pass\n",
    "    \n",
    "    def shutdown(self):\n",
    "        \"\"\"Shutdown memory manager gracefully.\"\"\"\n",
    "        self.is_running = False\n",
    "        if self.cleanup_thread:\n",
    "            self.cleanup_thread.join(timeout=5)\n",
    "        \n",
    "        # Close all connections\n",
    "        for conn in self.connection_pool:\n",
    "            conn.close()\n",
    "\n",
    "# TODO: Initialize scalable memory manager\n",
    "scalable_db_path = \"scalable_chatbot.db\"\n",
    "memory_manager = ScalableMemoryManager(scalable_db_path)\n",
    "\n",
    "# Create checkpointer with connection from pool\n",
    "scalable_memory = SqliteSaver(memory_manager.get_connection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test scalable system with concurrent conversations\n",
    "def simulate_user_conversation(user_id: str, message_count: int = 5):\n",
    "    \"\"\"Simulate a user conversation for load testing.\"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": f\"load_test_{user_id}_{uuid.uuid4().hex[:8]}\"}}\n",
    "    \n",
    "    messages = [\n",
    "        f\"Hi, I'm user {user_id}. I have a question about programming.\",\n",
    "        \"Can you help me with Python best practices?\",\n",
    "        \"What about error handling strategies?\",\n",
    "        \"How do I write maintainable code?\",\n",
    "        \"Thanks for the help!\"\n",
    "    ]\n",
    "    \n",
    "    # Use connection from pool\n",
    "    local_memory = SqliteSaver(memory_manager.get_connection())\n",
    "    local_graph = builder_multiuser.compile(checkpointer=local_memory)\n",
    "    \n",
    "    for i, message in enumerate(messages[:message_count]):\n",
    "        try:\n",
    "            result = local_graph.invoke(\n",
    "                {\n",
    "                    \"messages\": [HumanMessage(content=message)],\n",
    "                    \"user_id\": user_id\n",
    "                },\n",
    "                config\n",
    "            )\n",
    "            time.sleep(0.1)  # Small delay between messages\n",
    "        except Exception as e:\n",
    "            print(f\"Error in conversation {user_id}: {e}\")\n",
    "\n",
    "# TODO: Run concurrent conversation simulation\n",
    "print(\"=== Testing Scalable Memory System ===\")\n",
    "print(\"\\n1. Running concurrent conversation simulation...\")\n",
    "\n",
    "# Create multiple concurrent conversations\n",
    "num_users = 20\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [\n",
    "        executor.submit(simulate_user_conversation, f\"user_{i}\", 3)\n",
    "        for i in range(num_users)\n",
    "    ]\n",
    "    \n",
    "    # Wait for all conversations to complete\n",
    "    for future in futures:\n",
    "        future.result()\n",
    "\n",
    "print(f\"\\n2. Completed {num_users} concurrent conversations\")\n",
    "\n",
    "# TODO: Check memory statistics\n",
    "print(\"\\n3. Memory statistics:\")\n",
    "stats = memory_manager.get_memory_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n=== Scalable Memory Test Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Production Deployment Considerations\n",
    "\n",
    "### Task\n",
    "Implement production-ready features including monitoring, logging, and error handling for persistent memory systems.\n",
    "\n",
    "### TODO: Implement production features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict\n",
    "import traceback\n",
    "from functools import wraps\n",
    "\n",
    "class ProductionMemorySystem:\n",
    "    def __init__(self, db_path: str):\n",
    "        self.db_path = db_path\n",
    "        self.setup_logging()\n",
    "        self.setup_monitoring()\n",
    "        self.connection = sqlite3.connect(db_path, check_same_thread=False)\n",
    "        self.checkpointer = SqliteSaver(self.connection)\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup production logging.\"\"\"\n",
    "        # TODO: Configure structured logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('chatbot_production.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger('ProductionMemorySystem')\n",
    "        \n",
    "    def setup_monitoring(self):\n",
    "        \"\"\"Setup monitoring and metrics collection.\"\"\"\n",
    "        # TODO: Initialize metrics tracking\n",
    "        self.metrics = {\n",
    "            'conversations_created': 0,\n",
    "            'messages_processed': 0,\n",
    "            'errors': 0,\n",
    "            'database_operations': 0\n",
    "        }\n",
    "        \n",
    "    def log_operation(self, operation: str, user_id: str, success: bool, details: Dict = None):\n",
    "        \"\"\"Log operation with structured data.\"\"\"\n",
    "        # TODO: Implement structured logging\n",
    "        log_data = {\n",
    "            'operation': operation,\n",
    "            'user_id': user_id,\n",
    "            'success': success,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        if details:\n",
    "            log_data.update(details)\n",
    "        \n",
    "        if success:\n",
    "            self.logger.info(f\"Operation successful: {log_data}\")\n",
    "        else:\n",
    "            self.logger.error(f\"Operation failed: {log_data}\")\n",
    "    \n",
    "    def with_error_handling(self, operation_name: str):\n",
    "        \"\"\"Decorator for error handling and monitoring.\"\"\"\n",
    "        def decorator(func):\n",
    "            @wraps(func)\n",
    "            def wrapper(*args, **kwargs):\n",
    "                try:\n",
    "                    result = func(*args, **kwargs)\n",
    "                    self.metrics['database_operations'] += 1\n",
    "                    user_id = kwargs.get('user_id', 'unknown')\n",
    "                    self.log_operation(operation_name, user_id, True)\n",
    "                    return result\n",
    "                except Exception as e:\n",
    "                    self.metrics['errors'] += 1\n",
    "                    user_id = kwargs.get('user_id', 'unknown')\n",
    "                    self.log_operation(\n",
    "                        operation_name, \n",
    "                        user_id, \n",
    "                        False, \n",
    "                        {'error': str(e), 'traceback': traceback.format_exc()}\n",
    "                    )\n",
    "                    # TODO: Implement fallback behavior\n",
    "                    raise\n",
    "            return wrapper\n",
    "        return decorator\n",
    "    \n",
    "    @with_error_handling('conversation_processing')\n",
    "    def process_conversation(self, messages: List, user_id: str, config: Dict) -> Dict:\n",
    "        \"\"\"Process conversation with full production monitoring.\"\"\"\n",
    "        # TODO: Implement conversation processing with monitoring\n",
    "        self.metrics['messages_processed'] += len(messages)\n",
    "        \n",
    "        # Create production graph\n",
    "        production_graph = builder_multiuser.compile(checkpointer=self.checkpointer)\n",
    "        \n",
    "        # Process with monitoring\n",
    "        result = production_graph.invoke(\n",
    "            {\"messages\": messages, \"user_id\": user_id},\n",
    "            config\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def health_check(self) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive health check.\"\"\"\n",
    "        # TODO: Implement comprehensive health monitoring\n",
    "        health_status = {\n",
    "            'status': 'healthy',\n",
    "            'database_connection': 'ok',\n",
    "            'metrics': self.metrics.copy(),\n",
    "            'database_size_mb': Path(self.db_path).stat().st_size / (1024 * 1024)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Test database connection\n",
    "            self.connection.execute(\"SELECT 1\")\n",
    "        except Exception as e:\n",
    "            health_status['status'] = 'unhealthy'\n",
    "            health_status['database_connection'] = f'error: {str(e)}'\n",
    "        \n",
    "        return health_status\n",
    "    \n",
    "    def get_performance_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance metrics.\"\"\"\n",
    "        # TODO: Calculate performance metrics\n",
    "        return {\n",
    "            'total_operations': self.metrics['database_operations'],\n",
    "            'error_rate': self.metrics['errors'] / max(1, self.metrics['database_operations']),\n",
    "            'messages_per_hour': self.metrics['messages_processed'],  # Simplified\n",
    "            'database_size_mb': Path(self.db_path).stat().st_size / (1024 * 1024)\n",
    "        }\n",
    "\n",
    "# TODO: Initialize production system\n",
    "production_db_path = \"production_chatbot.db\"\n",
    "production_system = ProductionMemorySystem(production_db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test production system\n",
    "print(\"=== Testing Production Memory System ===\")\n",
    "\n",
    "# Test health check\n",
    "print(\"\\n1. Health Check:\")\n",
    "health = production_system.health_check()\n",
    "for key, value in health.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Test error handling\n",
    "print(\"\\n2. Testing error handling...\")\n",
    "try:\n",
    "    result = production_system.process_conversation(\n",
    "        [HumanMessage(content=\"Test production system\")],\n",
    "        user_id=\"production_test_user\",\n",
    "        config={\"configurable\": {\"thread_id\": \"production_test_thread\"}}\n",
    "    )\n",
    "    print(\"   Conversation processed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error handled: {e}\")\n",
    "\n",
    "# Check performance metrics\n",
    "print(\"\\n3. Performance Metrics:\")\n",
    "metrics = production_system.get_performance_metrics()\n",
    "for key, value in metrics.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Test logging by checking log file\n",
    "print(\"\\n4. Recent log entries:\")\n",
    "try:\n",
    "    with open('chatbot_production.log', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-3:]:\n",
    "            print(f\"   {line.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"   Log file not found\")\n",
    "\n",
    "print(\"\\n=== Production System Test Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise: Distributed Memory System\n",
    "\n",
    "### Task\n",
    "Design a distributed memory system that can scale across multiple database instances and handle conversation sharding.\n",
    "\n",
    "### TODO: Implement distributed memory architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from typing import List, Tuple\n",
    "\n",
    "class DistributedMemorySystem:\n",
    "    def __init__(self, shard_configs: List[Dict]):\n",
    "        \"\"\"Initialize distributed memory with multiple database shards.\"\"\"\n",
    "        self.shards = []\n",
    "        self.num_shards = len(shard_configs)\n",
    "        \n",
    "        # TODO: Initialize multiple database shards\n",
    "        for i, config in enumerate(shard_configs):\n",
    "            shard_path = config.get('db_path', f'shard_{i}.db')\n",
    "            conn = sqlite3.connect(shard_path, check_same_thread=False)\n",
    "            checkpointer = SqliteSaver(conn)\n",
    "            \n",
    "            self.shards.append({\n",
    "                'id': i,\n",
    "                'connection': conn,\n",
    "                'checkpointer': checkpointer,\n",
    "                'config': config\n",
    "            })\n",
    "    \n",
    "    def get_shard_for_user(self, user_id: str) -> int:\n",
    "        \"\"\"Determine which shard to use for a given user.\"\"\"\n",
    "        # TODO: Implement consistent hashing for user-to-shard mapping\n",
    "        user_hash = int(hashlib.md5(user_id.encode()).hexdigest(), 16)\n",
    "        return user_hash % self.num_shards\n",
    "    \n",
    "    def get_checkpointer_for_user(self, user_id: str):\n",
    "        \"\"\"Get the appropriate checkpointer for a user.\"\"\"\n",
    "        shard_id = self.get_shard_for_user(user_id)\n",
    "        return self.shards[shard_id]['checkpointer']\n",
    "    \n",
    "    def migrate_user_conversations(self, user_id: str, from_shard: int, to_shard: int):\n",
    "        \"\"\"Migrate user conversations between shards.\"\"\"\n",
    "        # TODO: Implement conversation migration logic\n",
    "        pass\n",
    "    \n",
    "    def get_distributed_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics across all shards.\"\"\"\n",
    "        # TODO: Aggregate statistics from all shards\n",
    "        total_stats = {\n",
    "            'total_shards': self.num_shards,\n",
    "            'shard_stats': []\n",
    "        }\n",
    "        \n",
    "        for shard in self.shards:\n",
    "            # TODO: Get stats from each shard\n",
    "            shard_stat = {\n",
    "                'shard_id': shard['id'],\n",
    "                'size_mb': 0,  # TODO: Calculate actual size\n",
    "                'conversations': 0  # TODO: Count conversations\n",
    "            }\n",
    "            total_stats['shard_stats'].append(shard_stat)\n",
    "        \n",
    "        return total_stats\n",
    "\n",
    "# TODO: Initialize distributed system\n",
    "shard_configs = [\n",
    "    {'db_path': 'distributed_shard_0.db', 'region': 'us-east'},\n",
    "    {'db_path': 'distributed_shard_1.db', 'region': 'us-west'},\n",
    "    {'db_path': 'distributed_shard_2.db', 'region': 'eu-west'}\n",
    "]\n",
    "\n",
    "distributed_system = DistributedMemorySystem(shard_configs)\n",
    "\n",
    "print(\"Distributed memory system initialized with sharding\")\n",
    "print(\"This demonstrates the architecture - implement the TODOs for full functionality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cleanup resources\n",
    "print(\"=== Cleaning up resources ===\")\n",
    "\n",
    "# Close database connections\n",
    "try:\n",
    "    multiuser_conn.close()\n",
    "    memory_manager.shutdown()\n",
    "    production_system.connection.close()\n",
    "    \n",
    "    for shard in distributed_system.shards:\n",
    "        shard['connection'].close()\n",
    "        \n",
    "    print(\"All database connections closed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "print(\"\\n=== Exercise Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In these exercises, you've practiced:\n",
    "- Setting up SQLite checkpointers for persistent conversation memory\n",
    "- Building multi-user chat systems with isolated conversation threads\n",
    "- Implementing backup and recovery systems for conversation data\n",
    "- Creating scalable memory architectures with connection pooling and optimization\n",
    "- Adding production-ready features like monitoring, logging, and error handling\n",
    "- Designing distributed memory systems with sharding\n",
    "\n",
    "Key takeaways:\n",
    "- **External Memory**: Database checkpointers provide persistent conversation state across restarts\n",
    "- **SQLite Benefits**: Fast, lightweight, and perfect for single-application deployments\n",
    "- **Multi-User Systems**: Proper thread isolation and user management are essential\n",
    "- **Backup Strategies**: Multiple backup formats (DB, JSON, compressed) serve different recovery needs\n",
    "- **Scalability**: Connection pooling, background cleanup, and optimization are crucial for production\n",
    "- **Production Features**: Monitoring, logging, and error handling are non-negotiable for real systems\n",
    "- **Distributed Systems**: Sharding enables horizontal scaling but adds complexity\n",
    "\n",
    "These external memory techniques are essential for building production-ready conversational AI systems that can:\n",
    "- Maintain long-term user relationships\n",
    "- Handle high concurrency\n",
    "- Recover from failures gracefully\n",
    "- Scale horizontally when needed\n",
    "- Provide reliable service in production environments\n",
    "\n",
    "You've now completed all Module 2 exercises covering state management, schemas, reducers, memory, and message handling in LangGraph. These foundational concepts will serve you well as you build more advanced conversational AI systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}