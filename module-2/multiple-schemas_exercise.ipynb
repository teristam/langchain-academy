{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Schemas - Practice Exercises\n",
    "\n",
    "## Overview\n",
    "This notebook provides hands-on exercises to practice working with multiple schemas in LangGraph. You'll learn about private state, input/output schema filtering, and complex multi-schema architectures.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of these exercises, you will:\n",
    "- Understand how to use private state between nodes\n",
    "- Know how to define separate input and output schemas for graphs\n",
    "- Practice filtering graph inputs and outputs\n",
    "- Build complex systems with multiple schema types\n",
    "- Design clean APIs with controlled data exposure\n",
    "\n",
    "## Prerequisites\n",
    "- Completed the multiple-schemas.ipynb tutorial\n",
    "- Understanding of state schema and reducer concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Private State for Internal Processing\n",
    "\n",
    "### Task\n",
    "Create a user authentication system where sensitive information (like password hashes and session tokens) is kept private between nodes but not exposed in the final output.\n",
    "\n",
    "### TODO: Define public and private schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# TODO: Define the public state (what external users see)\n",
    "class PublicAuthState(TypedDict):\n",
    "    # TODO: Add public fields: username, is_authenticated, user_role\n",
    "    pass\n",
    "\n",
    "# TODO: Define the private state (internal processing data)\n",
    "class PrivateAuthState(TypedDict):\n",
    "    # TODO: Add private fields: password_hash, session_token, failed_attempts\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement authentication nodes\n",
    "def validate_credentials(state: PublicAuthState) -> PrivateAuthState:\n",
    "    print(f\"Validating credentials for user: {state['username']}\")\n",
    "    # TODO: Simulate password validation and return private state\n",
    "    # Return password_hash, session_token, failed_attempts\n",
    "    pass\n",
    "\n",
    "def check_security_policy(state: PrivateAuthState) -> PrivateAuthState:\n",
    "    print(f\"Checking security policy (failed attempts: {state['failed_attempts']})\")\n",
    "    # TODO: Update failed_attempts or other security metrics\n",
    "    pass\n",
    "\n",
    "def generate_session(state: PrivateAuthState) -> PublicAuthState:\n",
    "    print(f\"Generating session with token: {state['session_token'][:8]}...\")\n",
    "    # TODO: Return public authentication result\n",
    "    # Set is_authenticated=True and appropriate user_role\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build authentication graph\n",
    "builder_auth = StateGraph(PublicAuthState)\n",
    "# TODO: Add nodes with proper schema types\n",
    "# TODO: Create flow: validate_credentials -> check_security_policy -> generate_session\n",
    "\n",
    "graph_auth = builder_auth.compile()\n",
    "display(Image(graph_auth.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test authentication with private state\n",
    "auth_input = {\n",
    "    \"username\": \"alice@example.com\",\n",
    "    \"is_authenticated\": False,\n",
    "    \"user_role\": \"\"\n",
    "}\n",
    "\n",
    "result_auth = graph_auth.invoke(auth_input)\n",
    "print(\"Authentication result:\", result_auth)\n",
    "# Notice: private fields like password_hash and session_token should not appear in output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Input/Output Schema Filtering\n",
    "\n",
    "### Task\n",
    "Create a data processing pipeline with controlled input/output schemas. The internal processing uses more fields than what's exposed externally.\n",
    "\n",
    "### TODO: Define input, output, and internal schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "# TODO: Define input schema (what users provide)\n",
    "class DataInput(TypedDict):\n",
    "    # TODO: Add raw_data (List[Dict[str, Any]]) and processing_type (str)\n",
    "    pass\n",
    "\n",
    "# TODO: Define output schema (what users receive)\n",
    "class DataOutput(TypedDict):\n",
    "    # TODO: Add processed_data (List[Dict[str, Any]]) and summary (Dict[str, Any])\n",
    "    pass\n",
    "\n",
    "# TODO: Define internal schema (full processing state)\n",
    "class DataProcessingState(TypedDict):\n",
    "    # TODO: Include all fields from input and output, plus internal fields:\n",
    "    # raw_data, processing_type, processed_data, summary\n",
    "    # Plus internal: intermediate_results, error_log, processing_time\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement processing nodes\n",
    "def validate_input(state: DataInput):\n",
    "    print(f\"Validating input data ({len(state['raw_data'])} records)\")\n",
    "    # TODO: Initialize internal processing state\n",
    "    # Return intermediate_results=[], error_log=[], processing_time=0.0\n",
    "    pass\n",
    "\n",
    "def process_data(state: DataProcessingState):\n",
    "    print(f\"Processing data with type: {state['processing_type']}\")\n",
    "    # TODO: Process raw_data based on processing_type\n",
    "    # Update processed_data, intermediate_results, processing_time\n",
    "    pass\n",
    "\n",
    "def generate_summary(state: DataProcessingState) -> DataOutput:\n",
    "    print(f\"Generating summary (processing time: {state['processing_time']}s)\")\n",
    "    # TODO: Create summary from processed data\n",
    "    # Return only DataOutput fields\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build graph with input/output schema filtering\n",
    "builder_data = StateGraph(\n",
    "    DataProcessingState,  # Internal state\n",
    "    input_schema=DataInput,   # TODO: Set input schema\n",
    "    output_schema=DataOutput  # TODO: Set output schema\n",
    ")\n",
    "\n",
    "# TODO: Add nodes and edges\n",
    "# validate_input -> process_data -> generate_summary\n",
    "\n",
    "graph_data = builder_data.compile()\n",
    "display(Image(graph_data.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test with input/output filtering\n",
    "data_input = {\n",
    "    \"raw_data\": [\n",
    "        {\"name\": \"Alice\", \"age\": 30, \"score\": 85},\n",
    "        {\"name\": \"Bob\", \"age\": 25, \"score\": 92},\n",
    "        {\"name\": \"Charlie\", \"age\": 35, \"score\": 78}\n",
    "    ],\n",
    "    \"processing_type\": \"statistical_analysis\"\n",
    "}\n",
    "\n",
    "result_data = graph_data.invoke(data_input)\n",
    "print(\"Data processing result:\", result_data)\n",
    "# Notice: internal fields like intermediate_results and processing_time should be filtered out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Multi-Stage Pipeline with Different Schemas\n",
    "\n",
    "### Task\n",
    "Create a complex document processing pipeline where different stages use different schemas, simulating a real-world document analysis system.\n",
    "\n",
    "### TODO: Define schemas for different processing stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define document input schema\n",
    "class DocumentInput(TypedDict):\n",
    "    # TODO: Add document_text and document_type\n",
    "    pass\n",
    "\n",
    "# TODO: Define text analysis schema\n",
    "class TextAnalysisState(TypedDict):\n",
    "    # TODO: Add word_count, sentence_count, language, sentiment_score\n",
    "    pass\n",
    "\n",
    "# TODO: Define content extraction schema\n",
    "class ContentExtractionState(TypedDict):\n",
    "    # TODO: Add entities, keywords, topics, summary\n",
    "    pass\n",
    "\n",
    "# TODO: Define final output schema\n",
    "class DocumentAnalysisOutput(TypedDict):\n",
    "    # TODO: Add analysis_report (combining key insights) and confidence_score\n",
    "    pass\n",
    "\n",
    "# TODO: Define comprehensive internal schema\n",
    "class DocumentProcessingState(TypedDict):\n",
    "    # TODO: Include all fields from above schemas plus internal fields:\n",
    "    # document_text, document_type, word_count, sentence_count, language, \n",
    "    # sentiment_score, entities, keywords, topics, summary, analysis_report, \n",
    "    # confidence_score, processing_stage, error_messages\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement multi-stage processing nodes\n",
    "def analyze_text(state: DocumentInput) -> TextAnalysisState:\n",
    "    print(f\"Analyzing text for document type: {state['document_type']}\")\n",
    "    text = state['document_text']\n",
    "    # TODO: Perform basic text analysis\n",
    "    # Calculate word_count, sentence_count, detect language, sentiment_score\n",
    "    # Also set processing_stage=\"text_analysis\"\n",
    "    pass\n",
    "\n",
    "def extract_content(state: TextAnalysisState) -> ContentExtractionState:\n",
    "    print(f\"Extracting content (language: {state['language']})\")\n",
    "    # TODO: Extract entities, keywords, topics, and create summary\n",
    "    # Also set processing_stage=\"content_extraction\"\n",
    "    pass\n",
    "\n",
    "def generate_report(state: ContentExtractionState) -> DocumentAnalysisOutput:\n",
    "    print(f\"Generating analysis report\")\n",
    "    # TODO: Create comprehensive analysis_report and calculate confidence_score\n",
    "    # Also set processing_stage=\"report_generation\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build multi-stage document processing graph\n",
    "builder_doc = StateGraph(\n",
    "    DocumentProcessingState,\n",
    "    input_schema=DocumentInput,\n",
    "    output_schema=DocumentAnalysisOutput\n",
    ")\n",
    "\n",
    "# TODO: Add nodes and connect them in sequence\n",
    "# analyze_text -> extract_content -> generate_report\n",
    "\n",
    "graph_doc = builder_doc.compile()\n",
    "display(Image(graph_doc.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test document processing pipeline\n",
    "sample_document = {\n",
    "    \"document_text\": \"\"\"\n",
    "    LangGraph is a powerful framework for building stateful, multi-actor applications with LLMs. \n",
    "    It enables developers to create complex workflows that can maintain context and state across \n",
    "    multiple interactions. The framework provides excellent support for human-in-the-loop patterns \n",
    "    and sophisticated error handling. Overall, it represents a significant advancement in the field \n",
    "    of language model applications.\n",
    "    \"\"\",\n",
    "    \"document_type\": \"technical_article\"\n",
    "}\n",
    "\n",
    "result_doc_analysis = graph_doc.invoke(sample_document)\n",
    "print(\"Document analysis result:\", result_doc_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: API Gateway Pattern with Schema Transformation\n",
    "\n",
    "### Task\n",
    "Create an API gateway that accepts requests in one format, processes them through multiple internal services with different schemas, and returns a unified response format.\n",
    "\n",
    "### TODO: Define API gateway schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "# TODO: Define external API request schema\n",
    "class APIRequest(TypedDict):\n",
    "    # TODO: Add user_id, action, parameters\n",
    "    pass\n",
    "\n",
    "# TODO: Define external API response schema\n",
    "class APIResponse(TypedDict):\n",
    "    # TODO: Add success, data, message, request_id\n",
    "    pass\n",
    "\n",
    "# TODO: Define user service schema\n",
    "class UserServiceState(TypedDict):\n",
    "    # TODO: Add user_profile, permissions, last_login\n",
    "    pass\n",
    "\n",
    "# TODO: Define business logic schema\n",
    "class BusinessLogicState(TypedDict):\n",
    "    # TODO: Add business_rules, calculations, validations\n",
    "    pass\n",
    "\n",
    "# TODO: Define comprehensive internal schema\n",
    "class GatewayInternalState(TypedDict):\n",
    "    # TODO: Include all fields from above schemas plus:\n",
    "    # request_id, processing_start_time, service_calls, errors\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "\n",
    "# TODO: Implement API gateway nodes\n",
    "def process_request(state: APIRequest):\n",
    "    print(f\"Processing API request for user: {state['user_id']}\")\n",
    "    # TODO: Initialize internal processing state\n",
    "    # Generate request_id, set processing_start_time, initialize service_calls and errors\n",
    "    pass\n",
    "\n",
    "def call_user_service(state: GatewayInternalState) -> UserServiceState:\n",
    "    print(f\"Calling user service for request: {state['request_id']}\")\n",
    "    # TODO: Simulate user service call\n",
    "    # Return user_profile, permissions, last_login\n",
    "    # Update service_calls list\n",
    "    pass\n",
    "\n",
    "def execute_business_logic(state: UserServiceState) -> BusinessLogicState:\n",
    "    print(f\"Executing business logic\")\n",
    "    # TODO: Process based on action and user permissions\n",
    "    # Return business_rules, calculations, validations\n",
    "    pass\n",
    "\n",
    "def format_response(state: BusinessLogicState) -> APIResponse:\n",
    "    print(f\"Formatting API response\")\n",
    "    # TODO: Create unified response format\n",
    "    # Set success, data, message, request_id\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build API gateway graph\n",
    "builder_api = StateGraph(\n",
    "    GatewayInternalState,\n",
    "    input_schema=APIRequest,\n",
    "    output_schema=APIResponse\n",
    ")\n",
    "\n",
    "# TODO: Add nodes and edges\n",
    "# process_request -> call_user_service -> execute_business_logic -> format_response\n",
    "\n",
    "graph_api = builder_api.compile()\n",
    "display(Image(graph_api.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test API gateway\n",
    "api_request = {\n",
    "    \"user_id\": \"user_12345\",\n",
    "    \"action\": \"get_user_stats\",\n",
    "    \"parameters\": {\"include_history\": True, \"date_range\": \"30d\"}\n",
    "}\n",
    "\n",
    "api_response = graph_api.invoke(api_request)\n",
    "print(\"API Gateway Response:\", api_response)\n",
    "# Should only contain APIResponse fields, internal processing details filtered out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Conditional Schema Routing\n",
    "\n",
    "### Task\n",
    "Create a system where different paths through the graph use different schemas based on the input type or processing requirements.\n",
    "\n",
    "### TODO: Define conditional schema system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "# TODO: Define base input schema\n",
    "class ProcessingRequest(TypedDict):\n",
    "    # TODO: Add content, processing_type, priority\n",
    "    pass\n",
    "\n",
    "# TODO: Define text processing schema\n",
    "class TextProcessingState(TypedDict):\n",
    "    # TODO: Add text_content, language, nlp_results\n",
    "    pass\n",
    "\n",
    "# TODO: Define image processing schema\n",
    "class ImageProcessingState(TypedDict):\n",
    "    # TODO: Add image_metadata, vision_results, processed_image_path\n",
    "    pass\n",
    "\n",
    "# TODO: Define audio processing schema\n",
    "class AudioProcessingState(TypedDict):\n",
    "    # TODO: Add audio_metadata, transcription, audio_features\n",
    "    pass\n",
    "\n",
    "# TODO: Define unified output schema\n",
    "class ProcessingResult(TypedDict):\n",
    "    # TODO: Add processing_type, results, confidence, processing_time\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement routing logic\n",
    "def route_by_type(state: ProcessingRequest) -> Literal[\"process_text\", \"process_image\", \"process_audio\"]:\n",
    "    # TODO: Route based on processing_type\n",
    "    processing_type = state['processing_type']\n",
    "    # Return appropriate node name based on type\n",
    "    pass\n",
    "\n",
    "# TODO: Implement type-specific processing nodes\n",
    "def process_text(state: ProcessingRequest) -> TextProcessingState:\n",
    "    print(\"Processing text content\")\n",
    "    # TODO: Extract text from content and process it\n",
    "    pass\n",
    "\n",
    "def process_image(state: ProcessingRequest) -> ImageProcessingState:\n",
    "    print(\"Processing image content\")\n",
    "    # TODO: Extract image metadata and process it\n",
    "    pass\n",
    "\n",
    "def process_audio(state: ProcessingRequest) -> AudioProcessingState:\n",
    "    print(\"Processing audio content\")\n",
    "    # TODO: Extract audio features and process it\n",
    "    pass\n",
    "\n",
    "# TODO: Implement unified result formatter\n",
    "def format_results(state) -> ProcessingResult:\n",
    "    print(\"Formatting processing results\")\n",
    "    # TODO: Create unified result format regardless of processing type\n",
    "    # This node needs to handle different input schema types\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build conditional routing graph\n",
    "# Note: This is a more complex setup with conditional schemas\n",
    "builder_conditional = StateGraph(ProcessingRequest)  # Base state for routing\n",
    "\n",
    "# TODO: Add all processing nodes\n",
    "# TODO: Add conditional edges from START based on processing type\n",
    "# TODO: Connect all processing nodes to format_results\n",
    "\n",
    "graph_conditional = builder_conditional.compile()\n",
    "display(Image(graph_conditional.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test conditional routing with different content types\n",
    "test_requests = [\n",
    "    {\n",
    "        \"content\": \"This is a sample text for natural language processing.\",\n",
    "        \"processing_type\": \"text\",\n",
    "        \"priority\": \"high\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"path/to/image.jpg\",\n",
    "        \"processing_type\": \"image\", \n",
    "        \"priority\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"path/to/audio.wav\",\n",
    "        \"processing_type\": \"audio\",\n",
    "        \"priority\": \"low\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, request in enumerate(test_requests):\n",
    "    print(f\"\\n--- Test {i+1}: {request['processing_type']} processing ---\")\n",
    "    result = graph_conditional.invoke(request)\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise: Schema Migration System\n",
    "\n",
    "### Task\n",
    "Create a system that can handle different versions of schemas for backward compatibility, demonstrating how to manage schema evolution in production systems.\n",
    "\n",
    "### TODO: Implement schema versioning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define different schema versions\n",
    "class UserDataV1(TypedDict):\n",
    "    # TODO: Add basic fields: name, email\n",
    "    pass\n",
    "\n",
    "class UserDataV2(TypedDict):\n",
    "    # TODO: Add extended fields: name, email, phone, address\n",
    "    pass\n",
    "\n",
    "class UserDataV3(TypedDict):\n",
    "    # TODO: Add latest fields: name, email, phone, address, preferences, metadata\n",
    "    pass\n",
    "\n",
    "# TODO: Implement schema migration functions\n",
    "def migrate_v1_to_v2(v1_data: UserDataV1) -> UserDataV2:\n",
    "    # TODO: Migrate V1 to V2 by adding default values\n",
    "    pass\n",
    "\n",
    "def migrate_v2_to_v3(v2_data: UserDataV2) -> UserDataV3:\n",
    "    # TODO: Migrate V2 to V3 by adding default values\n",
    "    pass\n",
    "\n",
    "def detect_schema_version(data: dict) -> str:\n",
    "    # TODO: Detect schema version based on available fields\n",
    "    pass\n",
    "\n",
    "print(\"Schema migration system defined - implement the migration logic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In these exercises, you've practiced:\n",
    "- Using private state for internal processing that's not exposed in outputs\n",
    "- Defining separate input/output schemas to control API boundaries\n",
    "- Building multi-stage pipelines with different schemas at each stage\n",
    "- Creating API gateway patterns with schema transformation\n",
    "- Implementing conditional routing based on schema types\n",
    "- Managing schema versioning and migration\n",
    "\n",
    "Key takeaways:\n",
    "- **Private State**: Enables secure internal processing without exposing sensitive data\n",
    "- **Input/Output Schemas**: Provide clean API boundaries and control data exposure\n",
    "- **Multi-Schema Systems**: Allow complex workflows with different data requirements\n",
    "- **Schema Transformation**: Essential for building robust, maintainable systems\n",
    "- **Conditional Schemas**: Enable flexible processing based on input characteristics\n",
    "- **Version Management**: Critical for production systems that need to evolve\n",
    "\n",
    "These patterns are essential for building production-ready LangGraph applications that need to handle complex data flows while maintaining clean interfaces and security boundaries.\n",
    "\n",
    "Next, continue with the trim-filter-messages exercises to learn about message management in conversational systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}