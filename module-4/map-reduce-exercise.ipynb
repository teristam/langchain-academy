{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/map-reduce-exercise.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239947-lesson-3-map-reduce)\n",
    "\n",
    "# Map-Reduce Exercises\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing these exercises, you will be able to:\n",
    "\n",
    "1. **Understand Map-Reduce fundamentals**: Recognize when and why to use map-reduce patterns in LangGraph\n",
    "2. **Implement parallel processing**: Use the `Send` API to distribute work across multiple nodes\n",
    "3. **Design state management**: Create appropriate state schemas with reducers for accumulating results\n",
    "4. **Build complex workflows**: Combine map and reduce phases into cohesive applications\n",
    "5. **Handle dynamic parallelization**: Create systems that adapt to varying input sizes\n",
    "\n",
    "## Overview\n",
    "\n",
    "Map-reduce is a powerful pattern for breaking down complex tasks into smaller, parallelizable units. In this exercise notebook, you'll progressively build map-reduce systems of increasing complexity, starting with the joke generation example and advancing to more sophisticated applications.\n",
    "\n",
    "**Key Concepts to Master:**\n",
    "- **Map Phase**: Distribute work across parallel nodes using `Send`\n",
    "- **Reduce Phase**: Aggregate and synthesize results from parallel operations\n",
    "- **State Management**: Use reducers like `operator.add` to accumulate results\n",
    "- **Dynamic Routing**: Send varying numbers of tasks based on input data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LangSmith tracing\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy-exercises\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import operator\n",
    "from typing import Annotated, List, Dict\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.types import Send\n",
    "from IPython.display import Image\n",
    "\n",
    "# Initialize the LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 1: Understanding the Joke Generation System\n",
    "\n",
    "Let's start by implementing the complete joke generation system step by step. This will help you understand the core map-reduce concepts.\n",
    "\n",
    "## Step 1.1: Define the State Schema\n",
    "\n",
    "**Task**: Complete the state definition below. Pay attention to the `jokes` field - why does it use `Annotated[list, operator.add]`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subjects(BaseModel):\n",
    "    subjects: list[str]\n",
    "\n",
    "class BestJoke(BaseModel):\n",
    "    id: int\n",
    "    \n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    subjects: list\n",
    "    # TODO: Complete this line - what should the jokes field look like?\n",
    "    # Hint: It needs to accumulate jokes from multiple parallel operations\n",
    "    jokes: # YOUR CODE HERE\n",
    "    best_selected_joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Question**: Why do we use `operator.add` as the reducer for the `jokes` field? What would happen if we used a regular `list` instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Implement the Topic Generation Node\n",
    "\n",
    "**Task**: Complete the function that generates joke subjects from a main topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_prompt = \"\"\"Generate a list of 3 sub-topics that are all related to this overall topic: {topic}.\"\"\"\n",
    "\n",
    "def generate_topics(state: OverallState):\n",
    "    \"\"\"\n",
    "    Generate 3 sub-topics related to the main topic.\n",
    "    \n",
    "    Args:\n",
    "        state: OverallState containing the main topic\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated state with subjects list\n",
    "    \"\"\"\n",
    "    # TODO: Complete this function\n",
    "    # 1. Format the prompt with the topic from state\n",
    "    # 2. Use the model with structured output to get subjects\n",
    "    # 3. Return the subjects in the correct format\n",
    "    \n",
    "    prompt = # YOUR CODE HERE\n",
    "    response = # YOUR CODE HERE\n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3: Implement the Send Logic (Map Phase)\n",
    "\n",
    "**Task**: Complete the function that uses `Send` to distribute joke generation tasks. This is the core of the map phase!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_to_jokes(state: OverallState):\n",
    "    \"\"\"\n",
    "    Create Send objects to distribute joke generation across subjects.\n",
    "    \n",
    "    Args:\n",
    "        state: OverallState containing subjects list\n",
    "        \n",
    "    Returns:\n",
    "        list[Send]: List of Send objects for parallel processing\n",
    "    \"\"\"\n",
    "    # TODO: Complete this function\n",
    "    # Create a Send object for each subject that:\n",
    "    # - Targets the \"generate_joke\" node\n",
    "    # - Passes the subject as state\n",
    "    \n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Implement the Joke Generation Node\n",
    "\n",
    "**Task**: Complete the joke generation logic that will run in parallel for each subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_prompt = \"\"\"Generate a joke about {subject}\"\"\"\n",
    "\n",
    "class JokeState(TypedDict):\n",
    "    subject: str\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "def generate_joke(state: JokeState):\n",
    "    \"\"\"\n",
    "    Generate a single joke for the given subject.\n",
    "    \n",
    "    Args:\n",
    "        state: JokeState containing the subject\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated state with the generated joke\n",
    "    \"\"\"\n",
    "    # TODO: Complete this function\n",
    "    # 1. Format the prompt with the subject\n",
    "    # 2. Generate the joke using structured output\n",
    "    # 3. Return the joke in a list (why a list?)\n",
    "    \n",
    "    prompt = # YOUR CODE HERE\n",
    "    response = # YOUR CODE HERE\n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5: Implement the Best Joke Selection (Reduce Phase)\n",
    "\n",
    "**Task**: Complete the reduce phase that selects the best joke from all generated jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: \\n\\n  {jokes}\"\"\"\n",
    "\n",
    "def best_joke(state: OverallState):\n",
    "    \"\"\"\n",
    "    Select the best joke from all generated jokes.\n",
    "    \n",
    "    Args:\n",
    "        state: OverallState containing all jokes\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated state with the best selected joke\n",
    "    \"\"\"\n",
    "    # TODO: Complete this function\n",
    "    # 1. Join all jokes with double newlines\n",
    "    # 2. Format the prompt with topic and jokes\n",
    "    # 3. Get the best joke ID using structured output\n",
    "    # 4. Return the actual joke text using the ID\n",
    "    \n",
    "    jokes = # YOUR CODE HERE\n",
    "    prompt = # YOUR CODE HERE\n",
    "    response = # YOUR CODE HERE\n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6: Build and Test the Graph\n",
    "\n",
    "**Task**: Complete the graph construction and test it with different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the graph construction\n",
    "graph = StateGraph(OverallState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(# YOUR CODE HERE)\n",
    "graph.add_node(# YOUR CODE HERE)\n",
    "graph.add_node(# YOUR CODE HERE)\n",
    "\n",
    "# Add edges\n",
    "graph.add_edge(# YOUR CODE HERE)\n",
    "graph.add_conditional_edges(# YOUR CODE HERE)\n",
    "graph.add_edge(# YOUR CODE HERE)\n",
    "graph.add_edge(# YOUR CODE HERE)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "# Visualize the graph\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the graph with different topics\n",
    "test_topics = [\"technology\", \"cooking\", \"space exploration\"]\n",
    "\n",
    "for topic in test_topics:\n",
    "    print(f\"\\n=== Testing with topic: {topic} ===\")\n",
    "    for s in app.stream({\"topic\": topic}):\n",
    "        print(s)\n",
    "    print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Questions**:\n",
    "1. How does the parallelization work in this system?\n",
    "2. What happens if one of the joke generation tasks fails?\n",
    "3. How would you modify this to generate a different number of sub-topics?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Document Summarization System\n",
    "\n",
    "Now let's build a more complex map-reduce system for document summarization. This system will:\n",
    "1. **Map**: Split a document into chunks and summarize each chunk in parallel\n",
    "2. **Reduce**: Combine all chunk summaries into a final comprehensive summary\n",
    "\n",
    "## Step 2.1: Define the Document Processing State\n",
    "\n",
    "**Task**: Design a state schema for document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentChunks(BaseModel):\n",
    "    chunks: list[str]\n",
    "\n",
    "class ChunkSummary(BaseModel):\n",
    "    summary: str\n",
    "\n",
    "class FinalSummary(BaseModel):\n",
    "    summary: str\n",
    "\n",
    "class DocumentState(TypedDict):\n",
    "    # TODO: Complete the state definition\n",
    "    # Think about what fields you need:\n",
    "    # - Original document text\n",
    "    # - Document chunks\n",
    "    # - Individual chunk summaries (with reducer)\n",
    "    # - Final combined summary\n",
    "    \n",
    "    document: str\n",
    "    chunks: # YOUR CODE HERE\n",
    "    chunk_summaries: # YOUR CODE HERE\n",
    "    final_summary: # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Implement Document Chunking\n",
    "\n",
    "**Task**: Create a function that splits documents into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document(state: DocumentState):\n",
    "    \"\"\"\n",
    "    Split the document into chunks for parallel processing.\n",
    "    \n",
    "    Args:\n",
    "        state: DocumentState containing the original document\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated state with document chunks\n",
    "    \"\"\"\n",
    "    # TODO: Implement document chunking\n",
    "    # For simplicity, split by paragraphs (double newlines)\n",
    "    # Filter out empty chunks\n",
    "    \n",
    "    document = state[\"document\"]\n",
    "    \n",
    "    # Split by double newlines and filter empty chunks\n",
    "    chunks = # YOUR CODE HERE\n",
    "    \n",
    "    return {\"chunks\": chunks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3: Implement the Send Logic for Chunk Processing\n",
    "\n",
    "**Task**: Create the map phase that sends each chunk for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_to_summaries(state: DocumentState):\n",
    "    \"\"\"\n",
    "    Send each chunk to be summarized in parallel.\n",
    "    \n",
    "    Args:\n",
    "        state: DocumentState containing chunks\n",
    "        \n",
    "    Returns:\n",
    "        list[Send]: Send objects for parallel chunk summarization\n",
    "    \"\"\"\n",
    "    # TODO: Create Send objects for each chunk\n",
    "    # Each Send should target \"summarize_chunk\" and pass the chunk text\n",
    "    \n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.4: Implement Chunk Summarization\n",
    "\n",
    "**Task**: Create the node that summarizes individual chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_summary_prompt = \"\"\"Summarize the following text chunk in 2-3 sentences, capturing the key points:\n",
    "\n",
    "{chunk}\"\"\"\n",
    "\n",
    "class ChunkState(TypedDict):\n",
    "    chunk: str\n",
    "\n",
    "def summarize_chunk(state: ChunkState):\n",
    "    \"\"\"\n",
    "    Summarize a single chunk of text.\n",
    "    \n",
    "    Args:\n",
    "        state: ChunkState containing the chunk text\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated state with chunk summary\n",
    "    \"\"\"\n",
    "    # TODO: Implement chunk summarization\n",
    "    # 1. Format the prompt with the chunk\n",
    "    # 2. Generate summary using structured output\n",
    "    # 3. Return as a list for the reducer\n",
    "    \n",
    "    prompt = # YOUR CODE HERE\n",
    "    response = # YOUR CODE HERE\n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: Implement Final Summary Generation (Reduce Phase)\n",
    "\n",
    "**Task**: Create the reduce phase that combines all chunk summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary_prompt = \"\"\"Below are summaries of different sections of a document. \n",
    "Create a comprehensive final summary that captures the main themes and key points from all sections:\n",
    "\n",
    "{summaries}\"\"\"\n",
    "\n",
    "def create_final_summary(state: DocumentState):\n",
    "    \"\"\"\n",
    "    Combine all chunk summaries into a final comprehensive summary.\n",
    "    \n",
    "    Args:\n",
    "        state: DocumentState containing all chunk summaries\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated state with final summary\n",
    "    \"\"\"\n",
    "    # TODO: Implement final summary generation\n",
    "    # 1. Combine all chunk summaries\n",
    "    # 2. Generate comprehensive final summary\n",
    "    \n",
    "    summaries = # YOUR CODE HERE\n",
    "    prompt = # YOUR CODE HERE\n",
    "    response = # YOUR CODE HERE\n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.6: Build and Test the Document Summarization System\n",
    "\n",
    "**Task**: Construct the graph and test it with a sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build the document summarization graph\n",
    "doc_graph = StateGraph(DocumentState)\n",
    "\n",
    "# Add nodes\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Add edges\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Compile\n",
    "doc_app = doc_graph.compile()\n",
    "\n",
    "# Visualize\n",
    "Image(doc_app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample document\n",
    "sample_document = \"\"\"\n",
    "Artificial Intelligence has revolutionized numerous industries in recent years. From healthcare to finance, AI systems are transforming how we work and live.\n",
    "\n",
    "In healthcare, AI is being used for medical diagnosis, drug discovery, and personalized treatment plans. Machine learning algorithms can analyze medical images with incredible accuracy, often surpassing human doctors in detecting certain conditions.\n",
    "\n",
    "The finance industry has embraced AI for fraud detection, algorithmic trading, and risk assessment. Banks use AI to analyze spending patterns and detect suspicious activities in real-time.\n",
    "\n",
    "However, the rise of AI also brings challenges. Concerns about job displacement, privacy, and algorithmic bias are growing. Society must address these issues as AI continues to evolve.\n",
    "\n",
    "Looking forward, the future of AI holds both promise and uncertainty. Continued research and responsible development will be crucial for harnessing AI's benefits while minimizing potential risks.\n",
    "\"\"\"\n",
    "\n",
    "# Run the summarization\n",
    "result = doc_app.invoke({\"document\": sample_document})\n",
    "print(\"Original Document Length:\", len(sample_document))\n",
    "print(\"Number of Chunks:\", len(result[\"chunks\"]))\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(result[\"final_summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**: Modify the system to handle different chunk sizes or use more sophisticated chunking strategies (e.g., by sentence count, word count, or semantic similarity).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Research Paper Analysis System\n",
    "\n",
    "Let's build a sophisticated map-reduce system that analyzes research papers by:\n",
    "1. **Map**: Analyzing different aspects (methodology, results, conclusions) in parallel\n",
    "2. **Reduce**: Synthesizing insights into a comprehensive research report\n",
    "\n",
    "## Step 3.1: Design the Research Analysis State\n",
    "\n",
    "**Task**: Create a state schema for multi-aspect research paper analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAspects(BaseModel):\n",
    "    aspects: list[str]  # e.g., [\"methodology\", \"results\", \"conclusions\", \"novelty\"]\n",
    "\n",
    "class AspectAnalysis(BaseModel):\n",
    "    aspect: str\n",
    "    analysis: str\n",
    "    key_points: list[str]\n",
    "    strengths: list[str]\n",
    "    weaknesses: list[str]\n",
    "\n",
    "class ResearchReport(BaseModel):\n",
    "    overall_summary: str\n",
    "    key_contributions: list[str]\n",
    "    strengths: list[str]\n",
    "    limitations: list[str]\n",
    "    significance_score: int  # 1-10\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    # TODO: Define the complete state schema\n",
    "    # Consider what information you need to track:\n",
    "    \n",
    "    paper_text: str\n",
    "    analysis_aspects: # YOUR CODE HERE\n",
    "    aspect_analyses: # YOUR CODE HERE (hint: needs reducer)\n",
    "    final_report: # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Implement Aspect Generation\n",
    "\n",
    "**Task**: Create a function that determines what aspects of the paper to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects_prompt = \"\"\"Given this research paper excerpt, determine the most important aspects to analyze. \n",
    "Choose 4-5 aspects from: methodology, results, conclusions, novelty, experimental_design, \n",
    "literature_review, implications, limitations, reproducibility.\n",
    "\n",
    "Paper excerpt:\n",
    "{paper_text}\"\"\"\n",
    "\n",
    "def generate_analysis_aspects(state: ResearchState):\n",
    "    \"\"\"\n",
    "    Determine which aspects of the research paper to analyze.\n",
    "    \n",
    "    Args:\n",
    "        state: ResearchState containing the paper text\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated state with analysis aspects\n",
    "    \"\"\"\n",
    "    # TODO: Implement aspect generation\n",
    "    # Use the first 1000 characters of the paper as excerpt\n",
    "    \n",
    "    paper_excerpt = # YOUR CODE HERE\n",
    "    prompt = # YOUR CODE HERE\n",
    "    response = # YOUR CODE HERE\n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Implement Parallel Aspect Analysis Distribution\n",
    "\n",
    "**Task**: Create the Send logic for distributing aspect analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_to_aspect_analysis(state: ResearchState):\n",
    "    \"\"\"\n",
    "    Send each aspect for parallel analysis.\n",
    "    \n",
    "    Args:\n",
    "        state: ResearchState containing aspects to analyze\n",
    "        \n",
    "    Returns:\n",
    "        list[Send]: Send objects for parallel aspect analysis\n",
    "    \"\"\"\n",
    "    # TODO: Create Send objects for each aspect\n",
    "    # Each Send should include both the aspect and the full paper text\n",
    "    \n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4: Implement Individual Aspect Analysis\n",
    "\n",
    "**Task**: Create the node that analyzes a specific aspect of the research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_analysis_prompt = \"\"\"Analyze the {aspect} aspect of this research paper. Provide:\n",
    "1. A detailed analysis of this aspect\n",
    "2. Key points related to this aspect\n",
    "3. Strengths in this aspect\n",
    "4. Weaknesses or limitations in this aspect\n",
    "\n",
    "Research Paper:\n",
    "{paper_text}\"\"\"\n",
    "\n",
    "class AspectState(TypedDict):\n",
    "    aspect: str\n",
    "    paper_text: str\n",
    "\n",
    "def analyze_aspect(state: AspectState):\n",
    "    \"\"\"\n",
    "    Analyze a specific aspect of the research paper.\n",
    "    \n",
    "    Args:\n",
    "        state: AspectState containing aspect and paper text\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated state with aspect analysis\n",
    "    \"\"\"\n",
    "    # TODO: Implement aspect analysis\n",
    "    \n",
    "    prompt = # YOUR CODE HERE\n",
    "    response = # YOUR CODE HERE\n",
    "    \n",
    "    # Create the analysis object with the aspect name\n",
    "    analysis_with_aspect = AspectAnalysis(\n",
    "        aspect=state[\"aspect\"],\n",
    "        analysis=response.analysis,\n",
    "        key_points=response.key_points,\n",
    "        strengths=response.strengths,\n",
    "        weaknesses=response.weaknesses\n",
    "    )\n",
    "    \n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Implement Research Report Generation (Reduce Phase)\n",
    "\n",
    "**Task**: Create the reduce phase that synthesizes all aspect analyses into a comprehensive report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_generation_prompt = \"\"\"Based on the following aspect analyses of a research paper, \n",
    "create a comprehensive research report with:\n",
    "1. Overall summary\n",
    "2. Key contributions\n",
    "3. Overall strengths\n",
    "4. Overall limitations\n",
    "5. Significance score (1-10)\n",
    "\n",
    "Aspect Analyses:\n",
    "{analyses}\"\"\"\n",
    "\n",
    "def generate_research_report(state: ResearchState):\n",
    "    \"\"\"\n",
    "    Generate final research report from all aspect analyses.\n",
    "    \n",
    "    Args:\n",
    "        state: ResearchState containing all aspect analyses\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated state with final research report\n",
    "    \"\"\"\n",
    "    # TODO: Implement report generation\n",
    "    # 1. Combine all aspect analyses into a readable format\n",
    "    # 2. Generate comprehensive report\n",
    "    \n",
    "    # Format analyses for the prompt\n",
    "    analyses_text = \"\"\n",
    "    for analysis in state[\"aspect_analyses\"]:\n",
    "        # YOUR CODE HERE - format each analysis\n",
    "        pass\n",
    "    \n",
    "    prompt = # YOUR CODE HERE\n",
    "    response = # YOUR CODE HERE\n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.6: Build and Test the Research Analysis System\n",
    "\n",
    "**Task**: Construct and test the complete research analysis system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build the research analysis graph\n",
    "research_graph = StateGraph(ResearchState)\n",
    "\n",
    "# Add all nodes\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Add all edges\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Compile and visualize\n",
    "research_app = research_graph.compile()\n",
    "Image(research_app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample research paper abstract and introduction\n",
    "sample_paper = \"\"\"\n",
    "Title: Attention Is All You Need\n",
    "\n",
    "Abstract:\n",
    "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\n",
    "Introduction:\n",
    "Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n",
    "\n",
    "The Transformer model architecture eschews recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. This allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
    "\"\"\"\n",
    "\n",
    "# Run the research analysis\n",
    "research_result = research_app.invoke({\"paper_text\": sample_paper})\n",
    "\n",
    "print(\"Analysis Aspects:\", research_result[\"analysis_aspects\"])\n",
    "print(\"\\nNumber of Aspect Analyses:\", len(research_result[\"aspect_analyses\"]))\n",
    "print(\"\\nFinal Research Report:\")\n",
    "print(research_result[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Challenge**: Extend this system to:\n",
    "1. Handle different types of papers (experimental, theoretical, survey)\n",
    "2. Include citation analysis\n",
    "3. Compare against related work\n",
    "4. Generate peer review comments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Custom Map-Reduce System\n",
    "\n",
    "Now it's your turn to design a complete map-reduce system from scratch!\n",
    "\n",
    "## Challenge: Content Moderation System\n",
    "\n",
    "**Scenario**: You need to build a content moderation system for a social media platform. The system should:\n",
    "\n",
    "1. **Map Phase**: Analyze different aspects of content (toxicity, spam, misinformation, etc.) in parallel\n",
    "2. **Reduce Phase**: Combine all analyses to make a final moderation decision\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "1. **Content Analysis Aspects**:\n",
    "   - Toxicity detection\n",
    "   - Spam detection  \n",
    "   - Misinformation potential\n",
    "   - Hate speech detection\n",
    "   - Adult content detection\n",
    "\n",
    "2. **Final Decision**: \n",
    "   - Action (approve, flag, remove)\n",
    "   - Confidence score\n",
    "   - Reasoning\n",
    "   - Specific violations found\n",
    "\n",
    "### Your Task:\n",
    "\n",
    "Complete the implementation below, filling in all the TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1: Define your data models\n",
    "class ModerationAspects(BaseModel):\n",
    "    # TODO: Define the structure for moderation aspects\n",
    "    pass\n",
    "\n",
    "class AspectResult(BaseModel):\n",
    "    # TODO: Define the structure for individual aspect analysis results\n",
    "    pass\n",
    "\n",
    "class ModerationDecision(BaseModel):\n",
    "    # TODO: Define the structure for final moderation decision\n",
    "    pass\n",
    "\n",
    "class ModerationState(TypedDict):\n",
    "    # TODO: Define the complete state schema\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2: Implement aspect determination\n",
    "def determine_moderation_aspects(state: ModerationState):\n",
    "    \"\"\"\n",
    "    Determine which aspects of content to analyze based on content type/characteristics.\n",
    "    \"\"\"\n",
    "    # TODO: Implement logic to determine which aspects to analyze\n",
    "    # Consider content length, type, etc.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.3: Implement the Send logic\n",
    "def distribute_moderation_tasks(state: ModerationState):\n",
    "    \"\"\"\n",
    "    Distribute moderation analysis tasks across aspects.\n",
    "    \"\"\"\n",
    "    # TODO: Create Send objects for parallel processing\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.4: Implement individual aspect analysis\n",
    "class AspectModerationState(TypedDict):\n",
    "    # TODO: Define state for individual aspect analysis\n",
    "    pass\n",
    "\n",
    "def analyze_content_aspect(state: AspectModerationState):\n",
    "    \"\"\"\n",
    "    Analyze a specific aspect of the content for moderation.\n",
    "    \"\"\"\n",
    "    # TODO: Implement aspect-specific analysis\n",
    "    # Consider different prompts for different aspects\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.5: Implement final moderation decision\n",
    "def make_moderation_decision(state: ModerationState):\n",
    "    \"\"\"\n",
    "    Make final moderation decision based on all aspect analyses.\n",
    "    \"\"\"\n",
    "    # TODO: Implement decision logic\n",
    "    # Consider severity levels, confidence scores, etc.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.6: Build and test your system\n",
    "# TODO: Construct the graph\n",
    "moderation_graph = StateGraph(ModerationState)\n",
    "\n",
    "# Add nodes and edges\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Compile and visualize\n",
    "moderation_app = moderation_graph.compile()\n",
    "Image(moderation_app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your moderation system\n",
    "test_contents = [\n",
    "    \"This is a normal post about cooking recipes.\",\n",
    "    \"This content might be questionable and potentially harmful.\",\n",
    "    \"Check out this amazing product! Click here for 90% off! Limited time!\"\n",
    "]\n",
    "\n",
    "for content in test_contents:\n",
    "    print(f\"\\n=== Moderating: {content[:50]}... ===\")\n",
    "    # TODO: Test your system\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Reflection and Advanced Concepts\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "After completing these exercises, reflect on the following:\n",
    "\n",
    "### 1. When to Use Map-Reduce\n",
    "- **Parallelizable tasks**: When you can break work into independent subtasks\n",
    "- **Large-scale processing**: When dealing with data that benefits from parallel processing\n",
    "- **Multi-aspect analysis**: When you need to analyze different dimensions of the same data\n",
    "\n",
    "### 2. Design Patterns You've Learned\n",
    "- **Dynamic parallelization**: Using `Send` to create varying numbers of parallel tasks\n",
    "- **State accumulation**: Using reducers like `operator.add` to collect results\n",
    "- **Hierarchical processing**: Breaking complex tasks into manageable subtasks\n",
    "\n",
    "### 3. Performance Considerations\n",
    "- **Balancing granularity**: Too many small tasks vs. too few large tasks\n",
    "- **Error handling**: What happens when parallel tasks fail?\n",
    "- **Resource management**: LLM API rate limits and costs\n",
    "\n",
    "## Advanced Challenges\n",
    "\n",
    "### Challenge 1: Error Handling\n",
    "Modify one of your systems to handle cases where individual map tasks fail. How would you:\n",
    "- Retry failed tasks?\n",
    "- Proceed with partial results?\n",
    "- Provide graceful degradation?\n",
    "\n",
    "### Challenge 2: Dynamic Task Creation\n",
    "Create a system where the number and type of map tasks depend on the analysis of initial results. For example:\n",
    "- First, analyze content type\n",
    "- Then, based on content type, determine specific analysis tasks\n",
    "- Finally, combine results appropriately\n",
    "\n",
    "### Challenge 3: Nested Map-Reduce\n",
    "Design a system with multiple levels of map-reduce:\n",
    "- Map 1: Break document into sections\n",
    "- Map 2: For each section, analyze different aspects\n",
    "- Reduce 2: Combine aspect analyses per section\n",
    "- Reduce 1: Combine all section analyses\n",
    "\n",
    "### Challenge 4: Conditional Reduce\n",
    "Create a system where the reduce phase only triggers when certain conditions are met:\n",
    "- Minimum number of results\n",
    "- Quality threshold reached\n",
    "- Timeout exceeded\n",
    "\n",
    "## Questions for Further Exploration\n",
    "\n",
    "1. **How would you implement load balancing** if different map tasks take significantly different amounts of time?\n",
    "\n",
    "2. **What strategies would you use for caching** repeated analyses or intermediate results?\n",
    "\n",
    "3. **How would you handle streaming data** where new items arrive while processing is ongoing?\n",
    "\n",
    "4. **What metrics would you track** to monitor the performance and quality of your map-reduce systems?\n",
    "\n",
    "5. **How would you extend these patterns** to handle real-time collaborative filtering or recommendation systems?\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Congratulations! You've now mastered the fundamental concepts of map-reduce in LangGraph. These patterns are powerful tools for building scalable, efficient AI systems that can handle complex, multi-faceted problems.\n",
    "\n",
    "**Key skills you've developed:**\n",
    "- Designing parallel processing workflows\n",
    "- Managing complex state with reducers\n",
    "- Using the `Send` API for dynamic task distribution\n",
    "- Building systems that scale with input complexity\n",
    "- Combining multiple AI analyses into cohesive results\n",
    "\n",
    "These patterns will serve as building blocks for more complex AI systems, including multi-agent architectures and large-scale content processing pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}