{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/research-assistant-exercise.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-research-assistant)\n",
    "\n",
    "# Research Assistant - Exercise Notebook\n",
    "\n",
    "Welcome to the research assistant exercise notebook! In this interactive session, you'll build your own multi-agent research system using LangGraph.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of these exercises, you will be able to:\n",
    "\n",
    "1. **Design Multi-Agent Systems**: Create analyst personas with specific roles and expertise areas\n",
    "2. **Implement Human-in-the-Loop**: Build interactive workflows where humans can provide feedback and refine AI-generated content\n",
    "3. **Build Interview Workflows**: Create conversational agents that conduct structured interviews with experts\n",
    "4. **Use Parallel Processing**: Leverage LangGraph's `Send()` API for map-reduce patterns\n",
    "5. **Integrate Multiple Data Sources**: Combine web search, Wikipedia, and other sources for comprehensive research\n",
    "6. **Create Report Generation Pipelines**: Transform raw research into structured, professional reports\n",
    "\n",
    "## Exercise Structure\n",
    "\n",
    "This notebook contains **5 progressive exercises**:\n",
    "\n",
    "- **Exercise 1**: Basic Analyst Generation (Beginner)\n",
    "- **Exercise 2**: Human Feedback Loop (Intermediate) \n",
    "- **Exercise 3**: Expert Interview System (Intermediate)\n",
    "- **Exercise 4**: Multi-Source Research Integration (Advanced)\n",
    "- **Exercise 5**: Complete Research Assistant Pipeline (Advanced)\n",
    "\n",
    "Each exercise builds upon the previous one, gradually increasing in complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start by installing the required packages and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Set up tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import Image, display, Markdown\n",
    "import operator\n",
    "from typing import List, Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", reasoning={'effort':'minimal'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 1: Basic Analyst Generation (Beginner)\n",
    "\n",
    "## Objective\n",
    "Create a system that generates AI analyst personas for a given research topic.\n",
    "\n",
    "## Background\n",
    "The foundation of a good research assistant is having diverse perspectives. Different analysts bring unique viewpoints, expertise areas, and concerns to research topics.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "1. **Define Data Models**: Create Pydantic models for `Analyst` and `Perspectives`\n",
    "2. **Create State Schema**: Define a TypedDict for managing analyst generation state\n",
    "3. **Implement Analyst Generator**: Write a function that uses structured LLM output to create analysts\n",
    "4. **Test Your Implementation**: Generate analysts for a sample topic\n",
    "\n",
    "### Step 1: Define the Analyst Model\n",
    "\n",
    "Complete the `Analyst` class below. Each analyst should have:\n",
    "- `name`: The analyst's name\n",
    "- `affiliation`: Their organization or company\n",
    "- `role`: Their job title or role\n",
    "- `description`: A detailed description of their focus and expertise\n",
    "- `persona` property: A formatted string combining all analyst information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyst(BaseModel):\n",
    "    # TODO: Add fields for affiliation, name, role, and description\n",
    "    name: str = Field(description='analyst name')\n",
    "    affiliation: str = Field(description='organization of company')\n",
    "    role: str = Field(description='job title or role')\n",
    "    description: str = Field(description='a detailed description of their foucs and expertise')\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    # TODO: Add a field for a list of analysts\n",
    "    analysts: List[Analyst] = Field(description=\"Comprehensive list of analysts with their roles and affiliations.\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create State Schema\n",
    "\n",
    "Define a TypedDict for managing the analyst generation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalystGenerationState(TypedDict):\n",
    "    # TODO: Add fields for:\n",
    "    # - topic: str (the research topic)\n",
    "    # - max_analysts: int (maximum number of analysts to generate)\n",
    "    # - analysts: List[Analyst] (the generated analysts)\n",
    "    topic: str\n",
    "    max_analysts: int\n",
    "    analysts: List[Analyst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Analyst Generator\n",
    "\n",
    "Create a function that generates analysts based on a topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysts': [Analyst(name='Dr. Mira Patel', affiliation='AI Foundations Lab, University of Cambridge', role='Research Scientist — Machine Learning & Formal Methods', description='Focuses on theory and guarantees for AI systems, including formal verification, type systems, and model interpretability. Mira evaluates tooling for correctness, robust schema enforcement, and how libraries affect system reliability and provable properties. Concerned with how langgraph and pydantic surface invariants, type safety, and integration with formal verification workflows. Has published on typed DSLs for ML pipelines and on bridging probabilistic models with deterministic schema checks.', persona='Dr. Mira Patel — Research Scientist at AI Foundations Lab, University of Cambridge. Expert in formal methods, type systems, and ML pipeline correctness; evaluates langgraph and pydantic for guarantees, schema enforcement, and integration with verification workflows.'),\n",
       "  Analyst(name='Carlos Rivera', affiliation='EdgeScale Inc.', role='Senior Backend Engineer — Production ML Systems', description='Builds and maintains production-grade ML services at scale, with emphasis on latency, deployment, and operational reliability. Interested in practical trade-offs between langgraph (for orchestrating LLM calls and prompt flows) and pydantic (for input/output validation, data models, and runtime parsing). Evaluates performance, observability, ease of testing, and developer ergonomics in fast-moving product environments. Concerned about runtime overhead, backward compatibility, and operational complexity introduced by new abstractions.', persona='Carlos Rivera — Senior Backend Engineer at EdgeScale Inc. Focused on production ML systems, evaluating langgraph vs pydantic for orchestration, validation, performance, and operational concerns.'),\n",
       "  Analyst(name='Aisha Mohammed', affiliation='NovaLegal Consultancy', role='AI Policy Analyst & Product Compliance Lead', description='Works at the intersection of AI product development, compliance, and user safety for regulated industries (finance, healthcare). Examines how langgraph and pydantic affect traceability, auditability, data governance, and risk mitigation — e.g., structured outputs for auditing, schema-enforced logging, and reducing hallucinations. Concerned with regulatory requirements, explainability, consented data handling, and demonstrable safeguards in developer tooling.', persona='Aisha Mohammed — AI Policy Analyst & Product Compliance Lead at NovaLegal Consultancy. Focuses on compliance, traceability, and user-safety implications of using langgraph versus pydantic in regulated AI products.')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:\n",
    "\n",
    "1. First, review the research topic: {topic}\n",
    "        \n",
    "2. Determine the most interesting themes and perspectives for this topic.\n",
    "                    \n",
    "3. Pick the top {max_analysts} themes.\n",
    "\n",
    "4. Assign one analyst to each theme, ensuring diversity in:\n",
    "   - Professional backgrounds\n",
    "   - Industry affiliations\n",
    "   - Areas of expertise\n",
    "   - Potential concerns or interests\n",
    "\n",
    "5. Make each analyst realistic and specific to their domain.\"\"\"\n",
    "\n",
    "def create_analysts(state: AnalystGenerationState):\n",
    "   \"\"\" Create analysts based on topic and max count \"\"\"\n",
    "    \n",
    "   # TODO: Extract topic and max_analysts from state\n",
    "   topic = state['topic']\n",
    "   max_analysts = state['max_analysts']\n",
    "\n",
    "   \n",
    "   # TODO: Create a structured LLM that outputs Perspectives\n",
    "   # Use: llm.with_structured_output(Perspectives)\n",
    "   structured_llm = llm.with_structured_output(Perspectives)\n",
    "    \n",
    "   # TODO: Format the system message with topic and max_analysts\n",
    "   system_prompt = analyst_instructions.format(topic=topic, max_analysts=max_analysts)\n",
    "\n",
    "   # TODO: Invoke the structured LLM with system message and human message\n",
    "   analysts = structured_llm.invoke([SystemMessage(content=system_prompt)]+[HumanMessage(content='Generate a list of analysts')])\n",
    "   \n",
    "   \n",
    "   # TODO: Return the analysts in the correct state format\n",
    "   return {'analysts': analysts.analysts}\n",
    "\n",
    "create_analysts({'topic':'Compare langgraph and pydantic AI', 'max_analysts':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Test Your Implementation\n",
    "\n",
    "Create a simple graph and test your analyst generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Aisha Rahman\n",
      "Affiliation: Massachusetts General Hospital / Harvard Medical School\n",
      "Role: Clinical AI Research Lead, Diagnostic Radiology\n",
      "Description: Radiologist and physician-scientist focused on integrating machine learning into medical imaging workflows. Leads clinical trials evaluating AI tools for chest X-ray, CT, and MRI interpretation, with expertise in model validation, prospective trial design, and regulatory requirements for diagnostic devices. Interests include improving diagnostic sensitivity and reducing time-to-diagnosis while maintaining clinician oversight. Concerns center on algorithmic bias across demographic groups, clinical workflow disruption, and liability/credentialing for AI-augmented decisions.\n",
      "--------------------------------------------------\n",
      "Name: Miguel Santos\n",
      "Affiliation: MedTech Start-up: VeridX Health\n",
      "Role: Chief Product Officer, AI Diagnostics\n",
      "Description: Product leader with a background in biomedical engineering and startup scaling. Oversees development and commercialization of AI-first diagnostic products (e.g., point-of-care triage, automated ECG and pathology screening). Expertise in data engineering, human-centered product design, market access, and reimbursement strategy. Prioritizes rapid iteration, clinician usability, and integration with EHRs. Potential concerns include data quality and representativeness from partner health systems, achieving regulatory clearance in multiple markets, and balancing speed-to-market with robust clinical validation.\n",
      "--------------------------------------------------\n",
      "Name: Professor Elena Kovács\n",
      "Affiliation: Central European University; Member, National Bioethics Council\n",
      "Role: Health Policy & Bioethics Scholar\n",
      "Description: Academic specializing in ethics, law, and policy for digital health and AI. Research areas include informed consent for AI-informed diagnoses, data governance, fairness and accountability, and the societal impacts of diagnostic automation. Advises governments and NGOs on regulatory frameworks and equitable deployment strategies. Key concerns include patient privacy, consent transparency when AI is used in diagnostics, widening health disparities from uneven access to AI tools, and governance mechanisms to ensure accountability and redress.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a StateGraph with AnalystGenerationState\n",
    "# Add the create_analysts node\n",
    "# Connect START -> create_analysts -> END\n",
    "# Compile the graph\n",
    "\n",
    "builder = StateGraph(AnalystGenerationState)\n",
    "builder.add_node('create_analysts', create_analysts)\n",
    "builder.add_edge(START, 'create_analysts')\n",
    "builder.add_edge('create_analysts', END)\n",
    "graph = builder.compile()\n",
    "# Test with this topic:\n",
    "test_topic = \"The impact of artificial intelligence on healthcare diagnostics\"\n",
    "max_analysts = 3\n",
    "\n",
    "# Run the graph and print results\n",
    "result = graph.invoke({\"topic\": test_topic, \"max_analysts\": max_analysts})\n",
    "for analyst in result['analysts']:\n",
    "    print(f\"Name: {analyst.name}\")\n",
    "    print(f\"Affiliation: {analyst.affiliation}\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    print(f\"Description: {analyst.description}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 **Checkpoint 1**: Verify Your Solution\n",
    "\n",
    "Your analyst generator should:\n",
    "- ✅ Create exactly the requested number of analysts\n",
    "- ✅ Generate diverse perspectives relevant to the topic\n",
    "- ✅ Include realistic names, affiliations, and roles\n",
    "- ✅ Provide detailed descriptions of each analyst's focus\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Human Feedback Loop (Intermediate)\n",
    "\n",
    "## Objective\n",
    "Extend your analyst generation system to include human-in-the-loop feedback for refining analyst selection.\n",
    "\n",
    "## Background\n",
    "Real-world research assistants need human oversight. Users should be able to review generated analysts and provide feedback to improve the selection before proceeding with research.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "1. **Extend State Schema**: Add human feedback capability\n",
    "2. **Create Feedback Node**: Implement a human feedback interruption point\n",
    "3. **Add Conditional Logic**: Route based on feedback presence\n",
    "4. **Test Interactive Flow**: Experience the human-in-the-loop workflow\n",
    "\n",
    "### Step 1: Extend State Schema\n",
    "\n",
    "Update your state to include human feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedAnalystState(TypedDict):\n",
    "    topic: str\n",
    "    max_analysts: int\n",
    "    analysts: List[Analyst]\n",
    "    # TODO: Add human_analyst_feedback: str field\n",
    "    human_analyst_feedback: str  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Update Analyst Creation Function\n",
    "\n",
    "Modify your analyst creation to consider human feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:\n",
    "\n",
    "1. First, review the research topic: {topic}\n",
    "        \n",
    "2. Examine any editorial feedback that has been provided to guide creation of the analysts: \n",
    "{human_analyst_feedback}\n",
    "    \n",
    "3. Determine the most interesting themes based upon the topic and feedback above.\n",
    "                    \n",
    "4. Pick the top {max_analysts} themes.\n",
    "\n",
    "5. Assign one analyst to each theme, incorporating any specific feedback provided.\"\"\"\n",
    "\n",
    "def create_analysts_with_feedback(state: EnhancedAnalystState):\n",
    "    \"\"\" Create analysts, considering human feedback if provided \"\"\"\n",
    "    \n",
    "    topic = state['topic']\n",
    "    max_analysts = state['max_analysts']\n",
    "    # TODO: Get human_analyst_feedback from state, default to empty string if not present\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback','')\n",
    "    \n",
    "    # TODO: Create structured LLM\n",
    "    llm_with_struct = llm.with_structured_output(Perspectives)\n",
    "    \n",
    "    # TODO: Format system message with topic, feedback, and max_analysts\n",
    "    \n",
    "    system_prompt = enhanced_analyst_instructions.format(topic=topic, human_analyst_feedback=human_analyst_feedback,\n",
    "                                                         max_analysts=max_analysts)\n",
    "    # print(system_prompt)\n",
    "    \n",
    "    # TODO: Generate analysts and return in state format\n",
    "    result = llm_with_struct.invoke([SystemMessage(content=system_prompt)]+[HumanMessage(content='Generate a list of analysts')])\n",
    "\n",
    "    return {'analysts':result.analysts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Human Feedback Components\n",
    "\n",
    "Implement the feedback node and routing logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback(state: EnhancedAnalystState):\n",
    "    \"\"\" No-op node that should be interrupted on \"\"\"\n",
    "    # TODO: This should be a pass-through node\n",
    "    # The interruption happens in the graph compilation\n",
    "    pass\n",
    "\n",
    "def should_continue(state: EnhancedAnalystState) -> Literal['create_analysts', END]:\n",
    "    \"\"\" Determine next node based on feedback presence \"\"\"\n",
    "    \n",
    "    # TODO: Check if human_analyst_feedback exists and is not None/empty\n",
    "    # If feedback exists, return \"create_analysts\" to regenerate\n",
    "    # Otherwise, return END\n",
    "    # if there is human feedback, then recreate the analysts\n",
    "    if state.get('human_analyst_feedback',None):\n",
    "        return 'create_analysts'\n",
    "    else:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build Interactive Graph\n",
    "\n",
    "Create a graph with human-in-the-loop functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Create StateGraph with EnhancedAnalystState\n",
    "# Add nodes: create_analysts_with_feedback, human_feedback\n",
    "# Add edges: START -> create_analysts_with_feedback -> human_feedback\n",
    "# Add conditional edge: human_feedback -> should_continue -> [\"create_analysts_with_feedback\", END]\n",
    "# Compile with interrupt_before=['human_feedback'] and MemorySaver checkpointer\n",
    "\n",
    "builder = StateGraph(EnhancedAnalystState)\n",
    "builder.add_node('create_analysts', create_analysts_with_feedback)\n",
    "builder.add_node('human_feedback', human_feedback)\n",
    "builder.add_edge(START, 'create_analysts')\n",
    "\n",
    "builder.add_edge('create_analysts', 'human_feedback')\n",
    "builder.add_conditional_edges('human_feedback', should_continue)\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "\n",
    "# Display the graph\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test Interactive Flow\n",
    "\n",
    "Experience the human feedback workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Analysts:\n",
      "- Dr. Aisha Raman, Global Renewable Futures Institute, Senior Energy Storage Researcher\n",
      "- Miguel Torres, Urban Clean Grid Labs, Distributed Energy Systems Architect\n",
      "- Prof. Elena Kovács, European Centre for Materials & Energy, Materials Scientist & Storage Innovation Lead\n",
      "--------------------------------------------------\n",
      "Initial analysts generated. Waiting for human feedback...\n"
     ]
    }
   ],
   "source": [
    "# TODO: Set up initial parameters\n",
    "topic = \"The future of renewable energy storage technologies\"\n",
    "max_analysts = 3\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# TODO: Run until first interruption\n",
    "# Use graph.stream() with stream_mode=\"values\"\n",
    "# Print analysts when they appear in events\n",
    "# steam will return the graph state\n",
    "# thread is a required argument for .stream()\n",
    "\n",
    "for cur_state in graph.stream({'topic': topic, 'max_analysts':max_analysts}, thread, stream_mode='values'):\n",
    "    analysts = cur_state.get('analysts','')\n",
    "    if analysts:\n",
    "        print(\"Generated Analysts:\")\n",
    "        for analyst in analysts:\n",
    "            print(f\"- {analyst.name}, {analyst.affiliation}, {analyst.role}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        \n",
    "    \n",
    "print(\"Initial analysts generated. Waiting for human feedback...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback provided: Add a policy analyst focused on government regulations and incentives\n"
     ]
    }
   ],
   "source": [
    "# TODO: Provide feedback using graph.update_state()\n",
    "# Example feedback: \"Add a policy analyst focused on government regulations and incentives\"\n",
    "# Use as_node=\"human_feedback\"\n",
    "\n",
    "feedback = \"Add a policy analyst focused on government regulations and incentives\"\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": feedback}, as_node=\"human_feedback\")\n",
    "\n",
    "print(f\"Feedback provided: {feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'create_analysts': {'analysts': [Analyst(name='Dr. Maya R. Singh', affiliation='Global Energy Systems Lab', role='Storage Technology Research Lead', description='Focuses on advanced chemical and electrochemical storage solutions, including next-generation batteries (solid-state, flow, metal-air) and hydrogen carriers. Research spans materials discovery, cell architecture, lifecycle performance, cost projections, and grid integration challenges. Advises on scaling lab discoveries to manufacturable products and assesses long-term durability, supply chain constraints for critical minerals, and recycling pathways.', persona='Dr. Maya R. Singh — Storage Technology Research Lead at Global Energy Systems Lab. An expert on advanced batteries, hydrogen carriers, materials innovation, and the path from lab-scale breakthroughs to scalable, durable, and recyclable commercial storage systems.'), Analyst(name=\"Liam O'Connor\", affiliation='Institute for Grid Innovation', role='Systems Integration and Grid Architect', description='Specializes in integrating distributed and utility-scale storage into electricity systems. Focus areas include power electronics, controls and software for aggregated storage fleets, demand-response pairing, transmission-deferral use cases, and techno-economic modeling of storage value streams. Evaluates operational strategies (stacking revenue streams), resilience benefits, and how storage interacts with variable renewables at multiple time scales.', persona=\"Liam O'Connor — Systems Integration and Grid Architect at the Institute for Grid Innovation. Focused on how storage assets—across scales—are controlled, optimized, and monetized within modern power systems to maximize reliability and renewable utilization.\"), Analyst(name='Dr. Elena Márquez', affiliation='Centre for Energy Policy & Climate', role='Policy Analyst — Regulations & Incentives', description='Examines government policy instruments that shape deployment of storage technologies, including incentives, procurement frameworks, interconnection rules, market mechanisms for capacity and ancillary services, and environmental/safety standards. Analyzes how policy design affects innovation, equity, domestic manufacturing, and lifecycle environmental outcomes. Advises on regulatory reforms to accelerate storage adoption while managing grid impacts and public interest objectives.', persona='Dr. Elena Márquez — Policy Analyst at the Centre for Energy Policy & Climate. Specializes in regulations, incentives, and market design to enable equitable, safe, and cost-effective deployment of renewable energy storage technologies.')]}}\n",
      "{'__interrupt__': ()}\n",
      "Updated analysts after feedback:\n",
      "Generated Analysts:\n",
      "- Dr. Maya R. Singh, Global Energy Systems Lab, Storage Technology Research Lead\n",
      "- Liam O'Connor, Institute for Grid Innovation, Systems Integration and Grid Architect\n",
      "- Dr. Elena Márquez, Centre for Energy Policy & Climate, Policy Analyst — Regulations & Incentives\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: Continue execution after feedback\n",
    "# Use graph.stream(None, thread, stream_mode=\"values\")\n",
    "# Print the updated analysts\n",
    "# values will return the full state\n",
    "# update will return a dict of node name and output\n",
    "\n",
    "for cur_state in graph.stream(None, thread, stream_mode='updates'):\n",
    "    print(cur_state)\n",
    "\n",
    "print(\"Updated analysts after feedback:\")\n",
    "\n",
    "# get state will return the latest statesnapshot\n",
    "state = graph.get_state(thread) \n",
    "analysts = state.values.get('analysts','')\n",
    "if analysts:\n",
    "    print(\"Generated Analysts:\")\n",
    "    for analyst in analysts:\n",
    "        print(f\"- {analyst.name}, {analyst.affiliation}, {analyst.role}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "graph.update_state(thread, {'human_analyst_feedback':None}, as_node='human_feedback')\n",
    "\n",
    "# run till the end\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(event.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 **Checkpoint 2**: Verify Your Solution\n",
    "\n",
    "Your interactive system should:\n",
    "- ✅ Generate initial analysts and pause for feedback\n",
    "- ✅ Accept human feedback and regenerate analysts accordingly\n",
    "- ✅ Allow multiple rounds of feedback if needed\n",
    "- ✅ Proceed to completion when no feedback is provided\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Expert Interview System (Intermediate)\n",
    "\n",
    "## Objective\n",
    "Build an interview system where AI analysts conduct multi-turn conversations with AI experts to gather insights.\n",
    "\n",
    "## Background\n",
    "The heart of the research assistant is conducting thorough interviews. Analysts ask questions, experts search for information and provide answers, and the conversation continues until sufficient insights are gathered.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "1. **Design Interview State**: Create state schema for managing conversations\n",
    "2. **Build Question Generator**: Implement analyst question generation\n",
    "3. **Create Search Functions**: Add web and Wikipedia search capabilities\n",
    "4. **Implement Answer Generator**: Create expert response system\n",
    "5. **Add Conversation Control**: Manage interview flow and termination\n",
    "\n",
    "### Step 1: Define Interview State\n",
    "\n",
    "Create a state schema for managing interviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewState(MessagesState):\n",
    "    # TODO: Add the following fields:\n",
    "    # - max_num_turns: int (maximum conversation turns)\n",
    "    # - context: Annotated[list, operator.add] (source documents)\n",
    "    # - analyst: Analyst (the analyst conducting interview)\n",
    "    # - interview: str (final interview transcript)\n",
    "    # pass\n",
    "    max_num_turns: int \n",
    "    context: Annotated[list, operator.add]\n",
    "    analyst: Analyst\n",
    "    interview: str\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    # TODO: Add search_query field with appropriate description\n",
    "    search_query: str = Field(description='Search query for gathering more information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Question Generation\n",
    "\n",
    "Create a function for analysts to generate interview questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
    "\n",
    "Your goal is to gather interesting and specific insights related to your area of expertise.\n",
    "\n",
    "1. Interesting: Insights that people will find surprising or non-obvious.\n",
    "2. Specific: Insights that avoid generalities and include specific examples.\n",
    "\n",
    "Here is your area of focus: {goals}\n",
    "        \n",
    "Begin by introducing yourself using your name, and then ask your question.\n",
    "\n",
    "Continue to ask follow-up questions to drill down and refine your understanding.\n",
    "        \n",
    "When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "\n",
    "Stay in character throughout your response.\"\"\"\n",
    "\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\" Generate analyst question \"\"\"\n",
    "    \n",
    "    # TODO: Extract analyst and messages from state\n",
    "    analyst = state['analyst']\n",
    "    messages = state['messages']\n",
    "    \n",
    "    # TODO: Create system message using question_instructions template\n",
    "    # Format with analyst.persona as goals\n",
    "    system_prompt = question_instructions.format(goals=analyst.persona)\n",
    "    \n",
    "    # TODO: Invoke LLM with system message + existing messages\n",
    "    questions = llm.invoke([SystemMessage(content=system_prompt)]+messages)\n",
    "    \n",
    "    # TODO: Return new message in messages list\n",
    "    return {'messages': [questions]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Search Functions\n",
    "\n",
    "Implement web and Wikipedia search capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up search tools\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "\n",
    "# Search query instructions\n",
    "search_instructions = SystemMessage(content=\"\"\"You will be given a conversation between an analyst and an expert. \n",
    "\n",
    "Your goal is to generate a well-structured query for retrieval and web search.\n",
    "        \n",
    "Analyze the full conversation and pay particular attention to the final question posed by the analyst.\n",
    "\n",
    "Convert this final question into a well-structured web search query.\"\"\")\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\" Search web for relevant information \"\"\"\n",
    "    \n",
    "    # TODO: Create structured LLM for SearchQuery\n",
    "    llm_search = llm.with_structured_output(SearchQuery)\n",
    "    \n",
    "    # TODO: Generate search query from conversation messages\n",
    "    queries = llm_search.invoke([search_instructions]+state['messages'])\n",
    "    \n",
    "    # TODO: Use tavily_search.invoke() to get search results\n",
    "    search_doc = tavily_search.invoke(queries.search_query)\n",
    "    # TODO: Format search results as documents with <Document> tags\n",
    "    # Format: '<Document href=\"{url}\"/>\\n{content}\\n</Document>'\n",
    "\n",
    "    formatted_search_docs = '\\n\\n---\\n\\n'.join([\n",
    "        f'<Document href=\"{doc['url']}\"/>\\n{doc['content']}\\n</Document>' for doc in search_doc\n",
    "    ])\n",
    "    \n",
    "    # TODO: Return formatted docs in context list\n",
    "    return {'context': [formatted_search_docs]}\n",
    "\n",
    "def search_wikipedia(state: InterviewState):\n",
    "    \"\"\" Search Wikipedia for relevant information \"\"\"\n",
    "    \n",
    "    # TODO: Similar to search_web but use WikipediaLoader\n",
    "    # Load max 2 documents\n",
    "    # Format with source and page metadata\n",
    "    llm_search = llm.with_structured_output(SearchQuery)\n",
    "    queries = llm_search.invoke([search_instructions]+state['messages'])\n",
    "\n",
    "    search_docs = WikipediaLoader(queries.search_query, load_max_docs=2).load()\n",
    "\n",
    "    # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {'context': [formatted_search_docs]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Answer Generation\n",
    "\n",
    "Create the expert response system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
    "\n",
    "Here is the analyst's area of focus: {goals}\n",
    "        \n",
    "Your goal is to answer the question posed by the interviewer using this context:\n",
    "        \n",
    "{context}\n",
    "\n",
    "Guidelines:\n",
    "        \n",
    "1. Use only information provided in the context.\n",
    "2. Do not introduce external information beyond what is in the context.\n",
    "3. Include source citations using [1], [2], etc.\n",
    "4. List sources at the end of your answer.\n",
    "5. Be specific and provide concrete examples when available.\"\"\"\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\" Generate expert answer using search context \"\"\"\n",
    "    \n",
    "    # TODO: Extract analyst, messages, and context from state\n",
    "    analyst = state['analyst']\n",
    "    messages = state['messages']\n",
    "    context = state['context']\n",
    "    \n",
    "    # TODO: Format system message with analyst goals and context\n",
    "    system_prompt = answer_instructions.format(goals=analyst.persona, context=context)\n",
    "    \n",
    "    # TODO: Generate answer using LLM\n",
    "    answer = llm.invoke([SystemMessage(content=system_prompt)]+[HumanMessage(content='Answer the question.')])\n",
    "    \n",
    "    # TODO: Set answer.name = \"expert\"\n",
    "    answer.name = 'expert'\n",
    "    \n",
    "    # TODO: Return answer in messages list\n",
    "    return {'messages': [answer]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Add Interview Control\n",
    "\n",
    "Implement conversation flow management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "def route_conversation(state: InterviewState) -> Literal['end_interview', 'continue_questions']:\n",
    "    \"\"\" Route between continuing questions or ending interview \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get('max_num_turns', 3)\n",
    "\n",
    "    # TODO: Count expert responses (messages with name=\"expert\")\n",
    "    num_expert_response = len([m for m in messages if m.name =='expert'])\n",
    "    \n",
    "    # TODO: If expert responses >= max_num_turns, return 'end_interview'\n",
    "    if num_expert_response >= max_num_turns:\n",
    "        return 'end_interview'\n",
    "    \n",
    "    # TODO: Check if last question contains \"Thank you so much for your help\"\n",
    "    # If yes, return 'end_interview'\n",
    "    if 'Thank you so much for your help' in messages[-1].content:\n",
    "        return 'end_interview'\n",
    "    \n",
    "    # TODO: Otherwise return 'continue_questions'\n",
    "    return 'continue_questions'\n",
    "\n",
    "def save_interview(state: InterviewState):\n",
    "    \"\"\" Convert interview to string format \"\"\"\n",
    "    \n",
    "    # TODO: Get messages from state\n",
    "    messages = state['messages']\n",
    "    \n",
    "    # TODO: Use get_buffer_string() to convert to text\n",
    "    interview = get_buffer_string(messages)\n",
    "    \n",
    "    # TODO: Return as interview field\n",
    "    return {'interview': interview}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build Interview Graph\n",
    "\n",
    "Assemble your interview system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create StateGraph with InterviewState\n",
    "# Add nodes: generate_question, search_web, search_wikipedia, generate_answer, save_interview\n",
    "builder = StateGraph(InterviewState)\n",
    "builder.add_node('generate_question', generate_question)\n",
    "builder.add_node('search_web', search_web)\n",
    "builder.add_node('search_wikipedia', search_wikipedia)\n",
    "builder.add_node('generate_answer', generate_answer)\n",
    "builder.add_node('save_interview', save_interview)\n",
    "\n",
    "# TODO: Add edges:\n",
    "# START -> generate_question\n",
    "# generate_question -> search_web (parallel)\n",
    "# generate_question -> search_wikipedia (parallel)\n",
    "# Both searches -> generate_answer\n",
    "# generate_answer -> route_conversation (conditional)\n",
    "# route: continue_questions -> generate_question, end_interview -> save_interview\n",
    "# save_interview -> END\n",
    "builder.add_edge(START, 'generate_question')\n",
    "builder.add_edge('generate_question', 'search_wikipedia')\n",
    "builder.add_edge('generate_question', 'search_web')\n",
    "builder.add_edge('search_web', 'generate_answer')\n",
    "builder.add_edge('search_wikipedia', 'generate_answer')\n",
    "builder.add_conditional_edges('generate_answer', route_conversation, \n",
    "                              {'continue_questions': 'generate_question', 'end_interview':'save_interview'})\n",
    "\n",
    "\n",
    "# TODO: Compile with MemorySaver\n",
    "memory = MemorySaver()\n",
    "interview_graph = builder.compile(memory)\n",
    "\n",
    "# Display graph\n",
    "\n",
    "# display(Image(interview_graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "\n",
    "# interview_graph.get_graph()\n",
    "\n",
    "# display(Image(interview_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Test Interview System\n",
    "\n",
    "Run a sample interview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m thread = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33minterview_test\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# TODO: Run interview and display results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m result = \u001b[43minterview_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manalyst\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_analyst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43minitial_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_num_turns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# print(\"Interview completed!\")\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# print(\"\\nInterview transcript:\")\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.get(\u001b[33m'\u001b[39m\u001b[33minterview\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNo interview saved\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/langchain-academy/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3024\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3032\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/langchain-academy/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/langchain-academy/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:253\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/langchain-academy/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:511\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    509\u001b[39m                 interrupts.append(exc)\n\u001b[32m    510\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/langchain-academy/.venv/lib/python3.13/site-packages/langgraph/pregel/_executor.py:81\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/langchain-academy/.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/langchain-academy/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/langchain-academy/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36msearch_web\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     26\u001b[39m search_doc = tavily_search.invoke(queries.search_query)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# TODO: Format search results as documents with <Document> tags\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Format: '<Document href=\"{url}\"/>\\n{content}\\n</Document>'\u001b[39;00m\n\u001b[32m     30\u001b[39m formatted_search_docs = \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m.join([\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m<Document href=\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m/>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m</Document>\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m search_doc\n\u001b[32m     32\u001b[39m ])\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# TODO: Return formatted docs in context list\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m'\u001b[39m: [formatted_search_docs]}\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'",
      "During task with name 'search_web' and id '6b334fbf-2a0c-e3be-7472-f27e27d9d844'"
     ]
    }
   ],
   "source": [
    "# Create a test analyst\n",
    "test_analyst = Analyst(\n",
    "    name=\"Dr. Sarah Chen\",\n",
    "    affiliation=\"Climate Technology Institute\",\n",
    "    role=\"Sustainability Researcher\",\n",
    "    description=\"Focuses on renewable energy adoption barriers and policy solutions\"\n",
    ")\n",
    "\n",
    "# TODO: Set up initial interview state\n",
    "topic = \"barriers to renewable energy adoption\"\n",
    "initial_message = HumanMessage(content=f\"I understand you're researching {topic}?\")\n",
    "thread = {\"configurable\": {\"thread_id\": \"interview_test\"}}\n",
    "\n",
    "# TODO: Run interview and display results\n",
    "result = interview_graph.invoke({\n",
    "    \"analyst\": test_analyst,\n",
    "    \"messages\": [initial_message],\n",
    "    \"max_num_turns\": 2,\n",
    "    \"context\": [],\n",
    "    \"interview\": \"\"\n",
    "}, thread)\n",
    "\n",
    "# print(\"Interview completed!\")\n",
    "# print(\"\\nInterview transcript:\")\n",
    "print(result.get('interview', 'No interview saved'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 **Checkpoint 3**: Verify Your Solution\n",
    "\n",
    "Your interview system should:\n",
    "- ✅ Generate contextual questions based on analyst persona\n",
    "- ✅ Search multiple sources (web + Wikipedia) in parallel\n",
    "- ✅ Provide well-sourced expert answers\n",
    "- ✅ Control conversation flow based on turn limits or completion phrases\n",
    "- ✅ Save complete interview transcripts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Multi-Source Research Integration (Advanced)\n",
    "\n",
    "## Objective\n",
    "Enhance the interview system with section writing capabilities and integrate multiple research sources.\n",
    "\n",
    "## Background\n",
    "Raw interview transcripts need to be transformed into polished research sections. This exercise focuses on processing interview results and generating structured reports with proper citations.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "1. **Extend Interview State**: Add section writing capability\n",
    "2. **Create Section Writer**: Transform interviews into report sections\n",
    "3. **Add Custom Data Sources**: Integrate additional research sources\n",
    "4. **Test Enhanced Pipeline**: Run complete interview-to-section workflow\n",
    "\n",
    "### Step 1: Enhanced Interview State\n",
    "\n",
    "Extend the interview state to support section generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedInterviewState(MessagesState):\n",
    "    max_num_turns: int\n",
    "    context: Annotated[list, operator.add]\n",
    "    analyst: Analyst\n",
    "    interview: str\n",
    "    # TODO: Add sections field as list for report sections\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Section Writer\n",
    "\n",
    "Create a function to transform interviews into structured sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
    "            \n",
    "Your task is to create a short, digestible section of a report based on source documents.\n",
    "\n",
    "1. Analyze the content of the source documents:\n",
    "- Document names are at the start with <Document> tags\n",
    "        \n",
    "2. Create a report structure using markdown:\n",
    "- Use ## for section title\n",
    "- Use ### for sub-section headers\n",
    "        \n",
    "3. Write the report with this structure:\n",
    "a. Title (## header)\n",
    "b. Summary (### header)  \n",
    "c. Sources (### header)\n",
    "\n",
    "4. Make your title engaging based on the analyst focus: {focus}\n",
    "\n",
    "5. For the summary section:\n",
    "- Provide background context related to the analyst's focus area\n",
    "- Emphasize novel, interesting, or surprising insights\n",
    "- Use numbered citations [1], [2], etc.\n",
    "- Target approximately 400 words maximum\n",
    "- Do not mention interviewer or expert names\n",
    "        \n",
    "6. In the Sources section:\n",
    "- List all sources used with full links/paths\n",
    "- Use newlines between sources\n",
    "- Combine duplicate sources\n",
    "\n",
    "### Sources\n",
    "[1] Source name or link\n",
    "[2] Source name or link\n",
    "\"\"\"\n",
    "\n",
    "def write_section(state: EnhancedInterviewState):\n",
    "    \"\"\" Transform interview and context into structured section \"\"\"\n",
    "    \n",
    "    # TODO: Extract interview, context, and analyst from state\n",
    "    \n",
    "    # TODO: Format system message with analyst description as focus\n",
    "    \n",
    "    # TODO: Create human message with context as source material\n",
    "    \n",
    "    # TODO: Invoke LLM to generate section\n",
    "    \n",
    "    # TODO: Return section content in sections list\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add Custom Data Sources\n",
    "\n",
    "Create additional search capabilities for more diverse sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_academic_papers(state: EnhancedInterviewState):\n",
    "    \"\"\" Simulate academic paper search (placeholder implementation) \"\"\"\n",
    "    \n",
    "    # TODO: For this exercise, create a mock academic source\n",
    "    # In a real implementation, you might use arXiv API, Semantic Scholar, etc.\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
    "    \n",
    "    # Mock academic content based on search query\n",
    "    mock_academic_content = f\"\"\"<Document source=\"academic_db\" paper=\"{search_query.search_query.replace(' ', '_')}_2024.pdf\"/>\n",
    "Academic research findings related to {search_query.search_query}. This paper presents evidence-based insights \n",
    "and methodological approaches relevant to the research question. Key findings include statistical analysis, \n",
    "experimental results, and peer-reviewed conclusions that support the theoretical framework.\n",
    "</Document>\"\"\"\n",
    "    \n",
    "    return {\"context\": [mock_academic_content]}\n",
    "\n",
    "def search_industry_reports(state: EnhancedInterviewState):\n",
    "    \"\"\" Simulate industry report search \"\"\"\n",
    "    \n",
    "    # TODO: Create mock industry report content\n",
    "    # Similar structure to academic search but with industry focus\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build Enhanced Interview Graph\n",
    "\n",
    "Create an interview system with section writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create StateGraph with EnhancedInterviewState\n",
    "# Add all previous nodes plus:\n",
    "# - search_academic_papers\n",
    "# - search_industry_reports  \n",
    "# - write_section\n",
    "\n",
    "# TODO: Update edges to include new search sources in parallel\n",
    "# Add: save_interview -> write_section -> END\n",
    "\n",
    "# TODO: Compile and display\n",
    "# enhanced_interview_graph = ...\n",
    "# display(Image(enhanced_interview_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test Enhanced System\n",
    "\n",
    "Run a complete interview with section generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test analyst focused on a specific domain\n",
    "tech_analyst = Analyst(\n",
    "    name=\"Alex Rodriguez\",\n",
    "    affiliation=\"Future Tech Ventures\",\n",
    "    role=\"Technology Investment Analyst\",\n",
    "    description=\"Evaluates emerging technologies for investment potential, focusing on market viability, technical feasibility, and competitive advantages\"\n",
    ")\n",
    "\n",
    "# TODO: Run enhanced interview system\n",
    "topic = \"quantum computing commercialization prospects\"\n",
    "# result = enhanced_interview_graph.invoke({\n",
    "#     \"analyst\": tech_analyst,\n",
    "#     \"messages\": [HumanMessage(content=f\"I'd like to discuss {topic}\")],\n",
    "#     \"max_num_turns\": 2\n",
    "# })\n",
    "\n",
    "# TODO: Display the generated section\n",
    "# if 'sections' in result and result['sections']:\n",
    "#     display(Markdown(result['sections'][0]))\n",
    "# else:\n",
    "#     print(\"No section generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 **Checkpoint 4**: Verify Your Solution\n",
    "\n",
    "Your enhanced interview system should:\n",
    "- ✅ Generate structured report sections with proper markdown formatting\n",
    "- ✅ Include citations and source references\n",
    "- ✅ Integrate multiple data sources (web, Wikipedia, academic, industry)\n",
    "- ✅ Transform conversational interviews into professional report content\n",
    "- ✅ Handle diverse analyst perspectives and focus areas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Complete Research Assistant Pipeline (Advanced)\n",
    "\n",
    "## Objective\n",
    "Build the full research assistant system that generates analysts, conducts parallel interviews, and produces comprehensive research reports.\n",
    "\n",
    "## Background\n",
    "This is the capstone exercise where you'll combine all previous components into a sophisticated research assistant using LangGraph's `Send()` API for parallel processing and map-reduce patterns.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "1. **Design Complete State Schema**: Create comprehensive state for the full pipeline\n",
    "2. **Implement Map-Reduce Pattern**: Use Send() API for parallel interviews\n",
    "3. **Create Report Generation**: Build introduction, content, and conclusion writers\n",
    "4. **Assemble Final Pipeline**: Connect all components with proper flow control\n",
    "5. **Test Complete System**: Run end-to-end research assistant workflow\n",
    "\n",
    "### Step 1: Complete State Schema\n",
    "\n",
    "Design the master state for the research assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "class ResearchAssistantState(TypedDict):\n",
    "    # TODO: Add fields for complete research pipeline:\n",
    "    # - topic: str (research topic)\n",
    "    # - max_analysts: int (number of analysts)\n",
    "    # - human_analyst_feedback: str (human feedback)\n",
    "    # - analysts: List[Analyst] (generated analysts)\n",
    "    # - sections: Annotated[list, operator.add] (report sections from interviews)\n",
    "    # - introduction: str (report introduction)\n",
    "    # - content: str (main report content)\n",
    "    # - conclusion: str (report conclusion)\n",
    "    # - final_report: str (complete assembled report)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Interview Orchestration\n",
    "\n",
    "Create the map function that launches parallel interviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_all_interviews(state: ResearchAssistantState):\n",
    "    \"\"\" Launch parallel interviews using Send() API \"\"\"\n",
    "    \n",
    "    # TODO: Check if human feedback exists\n",
    "    # If yes, return \"create_analysts\" to regenerate\n",
    "    \n",
    "    # TODO: Otherwise, create Send() messages for each analyst\n",
    "    # Each Send should target \"conduct_interview\" with:\n",
    "    # - analyst: the specific analyst\n",
    "    # - messages: initial HumanMessage about the topic\n",
    "    \n",
    "    # Example structure:\n",
    "    # return [Send(\"conduct_interview\", {\n",
    "    #     \"analyst\": analyst,\n",
    "    #     \"messages\": [HumanMessage(content=f\"I'd like to discuss {topic}\")],\n",
    "    #     \"max_num_turns\": 2\n",
    "    # }) for analyst in state[\"analysts\"]]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Report Writers\n",
    "\n",
    "Implement functions to generate different parts of the final report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_writer_instructions = \"\"\"You are a technical writer creating a report on: {topic}\n",
    "    \n",
    "You have a team of analysts who each:\n",
    "1. Conducted expert interviews on specific sub-topics\n",
    "2. Wrote findings into memos\n",
    "\n",
    "Your task:\n",
    "1. Analyze the collection of memos from your analysts\n",
    "2. Consolidate insights into a cohesive narrative\n",
    "3. Tie together central ideas from all memos\n",
    "\n",
    "Format requirements:\n",
    "1. Use markdown formatting\n",
    "2. No preamble\n",
    "3. No sub-headings\n",
    "4. Start with: ## Insights\n",
    "5. Preserve citations [1], [2], etc.\n",
    "6. Create consolidated Sources section: ## Sources\n",
    "7. Don't mention analyst names\n",
    "\n",
    "Here are the analyst memos: {context}\"\"\"\n",
    "\n",
    "def write_report(state: ResearchAssistantState):\n",
    "    \"\"\" Consolidate all sections into main report content \"\"\"\n",
    "    \n",
    "    # TODO: Extract sections and topic from state\n",
    "    \n",
    "    # TODO: Join all sections with newlines\n",
    "    \n",
    "    # TODO: Format system message with topic and combined sections\n",
    "    \n",
    "    # TODO: Generate consolidated report content\n",
    "    \n",
    "    # TODO: Return in content field\n",
    "    pass\n",
    "\n",
    "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
    "\n",
    "You will be given all sections of the report.\n",
    "\n",
    "Write a crisp and compelling {section_type} section.\n",
    "\n",
    "Guidelines:\n",
    "- No preamble\n",
    "- Target ~100 words\n",
    "- Use markdown formatting\n",
    "- For introduction: compelling title with # header, then ## Introduction\n",
    "- For conclusion: ## Conclusion header\n",
    "- Preview (intro) or recap (conclusion) all report sections\n",
    "\n",
    "Report sections: {formatted_str_sections}\"\"\"\n",
    "\n",
    "def write_introduction(state: ResearchAssistantState):\n",
    "    \"\"\" Generate report introduction \"\"\"\n",
    "    \n",
    "    # TODO: Extract sections and topic\n",
    "    # TODO: Join sections and format instructions\n",
    "    # TODO: Generate introduction\n",
    "    # TODO: Return in introduction field\n",
    "    pass\n",
    "\n",
    "def write_conclusion(state: ResearchAssistantState):\n",
    "    \"\"\" Generate report conclusion \"\"\"\n",
    "    \n",
    "    # TODO: Similar to write_introduction but for conclusion\n",
    "    pass\n",
    "\n",
    "def finalize_report(state: ResearchAssistantState):\n",
    "    \"\"\" Assemble complete final report \"\"\"\n",
    "    \n",
    "    # TODO: Extract introduction, content, and conclusion\n",
    "    \n",
    "    # TODO: Clean up content format (remove \"## Insights\" prefix if present)\n",
    "    \n",
    "    # TODO: Handle sources section separation if needed\n",
    "    \n",
    "    # TODO: Assemble: introduction + \"\\n\\n---\\n\\n\" + content + \"\\n\\n---\\n\\n\" + conclusion\n",
    "    \n",
    "    # TODO: Add sources section if extracted\n",
    "    \n",
    "    # TODO: Return in final_report field\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Assemble Master Graph\n",
    "\n",
    "Connect all components into the complete research assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create StateGraph with ResearchAssistantState\n",
    "# Add nodes:\n",
    "# - create_analysts_with_feedback (from Exercise 2)\n",
    "# - human_feedback\n",
    "# - conduct_interview (use enhanced_interview_graph from Exercise 4)\n",
    "# - write_report\n",
    "# - write_introduction  \n",
    "# - write_conclusion\n",
    "# - finalize_report\n",
    "\n",
    "# TODO: Add edges:\n",
    "# START -> create_analysts_with_feedback\n",
    "# create_analysts_with_feedback -> human_feedback\n",
    "# human_feedback -> initiate_all_interviews (conditional)\n",
    "# conduct_interview -> write_report (parallel)\n",
    "# conduct_interview -> write_introduction (parallel) \n",
    "# conduct_interview -> write_conclusion (parallel)\n",
    "# [write_report, write_introduction, write_conclusion] -> finalize_report\n",
    "# finalize_report -> END\n",
    "\n",
    "# TODO: Compile with interrupt_before=['human_feedback']\n",
    "\n",
    "# research_assistant = builder.compile(interrupt_before=['human_feedback'], checkpointer=MemorySaver())\n",
    "# display(Image(research_assistant.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test Complete Research Assistant\n",
    "\n",
    "Run the full end-to-end workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up research parameters\n",
    "research_topic = \"The impact of edge computing on IoT device performance and security\"\n",
    "max_analysts = 3\n",
    "thread = {\"configurable\": {\"thread_id\": \"final_research\"}}\n",
    "\n",
    "print(f\"Starting research on: {research_topic}\")\n",
    "print(f\"Generating {max_analysts} analysts...\")\n",
    "\n",
    "# TODO: Run until first interruption (human feedback)\n",
    "# for event in research_assistant.stream({\"topic\": research_topic, \"max_analysts\": max_analysts}, thread):\n",
    "#     analysts = event.get('analysts', '')\n",
    "#     if analysts:\n",
    "#         print(f\"\\nGenerated {len(analysts)} analysts:\")\n",
    "#         for analyst in analysts:\n",
    "#             print(f\"- {analyst.name} ({analyst.role})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Provide feedback or continue without feedback\n",
    "# Option 1: Provide feedback\n",
    "# feedback = \"Add a cybersecurity specialist focused on IoT vulnerabilities\"\n",
    "# research_assistant.update_state(thread, {\"human_analyst_feedback\": feedback}, as_node=\"human_feedback\")\n",
    "\n",
    "# Option 2: Continue without feedback\n",
    "research_assistant.update_state(thread, {\"human_analyst_feedback\": None}, as_node=\"human_feedback\")\n",
    "\n",
    "print(\"Proceeding with research...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Continue execution and monitor progress\n",
    "# for event in research_assistant.stream(None, thread, stream_mode=\"updates\"):\n",
    "#     node_name = next(iter(event.keys()))\n",
    "#     print(f\"Executing: {node_name}\")\n",
    "    \n",
    "#     if node_name == \"conduct_interview\":\n",
    "#         print(\"  -> Conducting interviews in parallel...\")\n",
    "#     elif node_name in [\"write_report\", \"write_introduction\", \"write_conclusion\"]:\n",
    "#         print(f\"  -> Generating {node_name.replace('write_', '')}...\")\n",
    "#     elif node_name == \"finalize_report\":\n",
    "#         print(\"  -> Assembling final report...\")\n",
    "\n",
    "print(\"Research completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display final research report\n",
    "# final_state = research_assistant.get_state(thread)\n",
    "# final_report = final_state.values.get('final_report')\n",
    "\n",
    "# if final_report:\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"FINAL RESEARCH REPORT\")\n",
    "#     print(\"=\"*80)\n",
    "#     display(Markdown(final_report))\n",
    "# else:\n",
    "#     print(\"No final report generated\")\n",
    "\n",
    "# TODO: Display summary statistics\n",
    "# sections = final_state.values.get('sections', [])\n",
    "# print(f\"\\nReport generated from {len(sections)} analyst interviews\")\n",
    "# print(f\"Total report length: ~{len(final_report.split()) if final_report else 0} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 **Final Checkpoint**: Verify Complete System\n",
    "\n",
    "Your complete research assistant should:\n",
    "- ✅ Generate diverse analysts with human feedback capability\n",
    "- ✅ Conduct multiple interviews in parallel using Send() API\n",
    "- ✅ Integrate multiple data sources for comprehensive research\n",
    "- ✅ Transform interviews into structured report sections\n",
    "- ✅ Generate cohesive introduction, main content, and conclusion\n",
    "- ✅ Assemble professional final report with proper citations\n",
    "- ✅ Handle error cases and provide meaningful feedback\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎉 Congratulations!\n",
    "\n",
    "You've successfully built a sophisticated multi-agent research assistant using LangGraph! \n",
    "\n",
    "## What You've Accomplished\n",
    "\n",
    "Through these exercises, you've mastered:\n",
    "\n",
    "1. **Multi-Agent System Design**: Created specialized analyst personas with distinct expertise\n",
    "2. **Human-in-the-Loop Workflows**: Built interactive systems that incorporate human feedback\n",
    "3. **Conversational AI Systems**: Implemented multi-turn interview workflows with AI experts\n",
    "4. **Parallel Processing**: Used LangGraph's Send() API for concurrent operations\n",
    "5. **Data Integration**: Combined multiple information sources into cohesive research\n",
    "6. **Report Generation**: Transformed raw conversations into professional research reports\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Consider extending your research assistant with:\n",
    "\n",
    "- **Custom Data Sources**: Integrate domain-specific APIs or databases\n",
    "- **Advanced Citations**: Add more sophisticated reference management\n",
    "- **Quality Control**: Implement fact-checking and source verification\n",
    "- **User Interface**: Create a web interface for easier interaction\n",
    "- **Report Templates**: Support different output formats (executive summary, technical report, etc.)\n",
    "- **Collaborative Features**: Allow multiple users to provide feedback and guidance\n",
    "\n",
    "## Key Patterns You've Learned\n",
    "\n",
    "- **Map-Reduce with LangGraph**: Using Send() API for parallel processing\n",
    "- **State Management**: Designing complex state schemas for multi-step workflows\n",
    "- **Conditional Logic**: Routing based on state conditions and user input\n",
    "- **Human-AI Collaboration**: Building systems that enhance human decision-making\n",
    "- **Structured Output**: Using Pydantic models for reliable AI responses\n",
    "\n",
    "These patterns are fundamental to building production-grade AI applications!\n",
    "\n",
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. How might you adapt this research assistant for your specific domain or use case?\n",
    "2. What additional quality controls would be important for production deployment?\n",
    "3. How could you measure and improve the quality of generated reports?\n",
    "4. What ethical considerations should guide the development of AI research assistants?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [STORM Paper](https://arxiv.org/abs/2402.14207) - Academic foundation for this approach\n",
    "- [LangSmith Tracing](https://docs.smith.langchain.com/) - Monitor and debug your graphs\n",
    "- [Research Assistant Examples](https://github.com/langchain-ai/langchain-academy) - More advanced implementations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-academy (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
