{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/research-assistant-exercise.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-research-assistant)\n",
    "\n",
    "# Research Assistant - Exercise Notebook\n",
    "\n",
    "Welcome to the research assistant exercise notebook! In this interactive session, you'll build your own multi-agent research system using LangGraph.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of these exercises, you will be able to:\n",
    "\n",
    "1. **Design Multi-Agent Systems**: Create analyst personas with specific roles and expertise areas\n",
    "2. **Implement Human-in-the-Loop**: Build interactive workflows where humans can provide feedback and refine AI-generated content\n",
    "3. **Build Interview Workflows**: Create conversational agents that conduct structured interviews with experts\n",
    "4. **Use Parallel Processing**: Leverage LangGraph's `Send()` API for map-reduce patterns\n",
    "5. **Integrate Multiple Data Sources**: Combine web search, Wikipedia, and other sources for comprehensive research\n",
    "6. **Create Report Generation Pipelines**: Transform raw research into structured, professional reports\n",
    "\n",
    "## Exercise Structure\n",
    "\n",
    "This notebook contains **5 progressive exercises**:\n",
    "\n",
    "- **Exercise 1**: Basic Analyst Generation (Beginner)\n",
    "- **Exercise 2**: Human Feedback Loop (Intermediate) \n",
    "- **Exercise 3**: Expert Interview System (Intermediate)\n",
    "- **Exercise 4**: Multi-Source Research Integration (Advanced)\n",
    "- **Exercise 5**: Complete Research Assistant Pipeline (Advanced)\n",
    "\n",
    "Each exercise builds upon the previous one, gradually increasing in complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start by installing the required packages and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Set up tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import Image, display, Markdown\n",
    "import operator\n",
    "from typing import List, Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", reasoning={'effort':'minimal'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 1: Basic Analyst Generation (Beginner)\n",
    "\n",
    "## Objective\n",
    "Create a system that generates AI analyst personas for a given research topic.\n",
    "\n",
    "## Background\n",
    "The foundation of a good research assistant is having diverse perspectives. Different analysts bring unique viewpoints, expertise areas, and concerns to research topics.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "1. **Define Data Models**: Create Pydantic models for `Analyst` and `Perspectives`\n",
    "2. **Create State Schema**: Define a TypedDict for managing analyst generation state\n",
    "3. **Implement Analyst Generator**: Write a function that uses structured LLM output to create analysts\n",
    "4. **Test Your Implementation**: Generate analysts for a sample topic\n",
    "\n",
    "### Step 1: Define the Analyst Model\n",
    "\n",
    "Complete the `Analyst` class below. Each analyst should have:\n",
    "- `name`: The analyst's name\n",
    "- `affiliation`: Their organization or company\n",
    "- `role`: Their job title or role\n",
    "- `description`: A detailed description of their focus and expertise\n",
    "- `persona` property: A formatted string combining all analyst information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyst(BaseModel):\n",
    "    # TODO: Add fields for affiliation, name, role, and description\n",
    "    name: str = Field(description='analyst name')\n",
    "    affiliation: str = Field(description='organization of company')\n",
    "    role: str = Field(description='job title or role')\n",
    "    description: str = Field(description='a detailed description of their foucs and expertise')\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    # TODO: Add a field for a list of analysts\n",
    "    analysts: List[Analyst] = Field(description=\"Comprehensive list of analysts with their roles and affiliations.\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create State Schema\n",
    "\n",
    "Define a TypedDict for managing the analyst generation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalystGenerationState(TypedDict):\n",
    "    # TODO: Add fields for:\n",
    "    # - topic: str (the research topic)\n",
    "    # - max_analysts: int (maximum number of analysts to generate)\n",
    "    # - analysts: List[Analyst] (the generated analysts)\n",
    "    topic: str\n",
    "    max_analysts: int\n",
    "    analysts: List[Analyst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Analyst Generator\n",
    "\n",
    "Create a function that generates analysts based on a topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysts': [Analyst(name='Dr. Aisha Rahman', affiliation='AI Engineering Research Lab, University of Toronto', role='Research Scientist & Lecturer — Machine Learning Infrastructure', description='Focuses on tooling and frameworks for reliable ML/AI model deployment. Expertise in ML systems, data validation, schema design, and model serving. Interested in how libraries affect model lifecycle (development, testing, observability) and reproducibility. Concerned with type safety, validation semantics, runtime performance, and how developer ergonomics influence engineering debt in production ML pipelines.'),\n",
       "  Analyst(name='Miguel Santos', affiliation='Open Standards Group for Python Data Tools', role='Senior Software Engineer & Standards Advocate', description='Background in Python library design, API ergonomics, and open-source community governance. Expert in typing, pydantic, JSON schema, and interoperability across tooling. Focused on compatibility, extensibility, and developer experience for library adoption. Interested in ecosystem integration (editors, linters, CI) and practical migration concerns. Worries about API stability, community support, and fragmentation between competing libs.'),\n",
       "  Analyst(name='Linnea Bergström', affiliation='Nordic Fintech — Machine Learning Platform Team', role='Platform Engineer — Responsible for Compliance & Production ML', description='Works at the intersection of regulated production systems and ML-driven features. Expertise includes data governance, compliance, auditability, and operational risk. Evaluates libraries for security, deterministic behavior, and certification readiness. Interested in traceability of inputs/outputs, schema enforcement, and how using langgraph vs pydantic affects audit trails, privacy controls, and regulatory compliance. Concerned about non-deterministic schema behavior, dependency supply chain risk, and maintainability under strict governance.')]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:\n",
    "\n",
    "1. First, review the research topic: {topic}\n",
    "        \n",
    "2. Determine the most interesting themes and perspectives for this topic.\n",
    "                    \n",
    "3. Pick the top {max_analysts} themes.\n",
    "\n",
    "4. Assign one analyst to each theme, ensuring diversity in:\n",
    "   - Professional backgrounds\n",
    "   - Industry affiliations\n",
    "   - Areas of expertise\n",
    "   - Potential concerns or interests\n",
    "\n",
    "5. Make each analyst realistic and specific to their domain.\"\"\"\n",
    "\n",
    "def create_analysts(state: AnalystGenerationState):\n",
    "   \"\"\" Create analysts based on topic and max count \"\"\"\n",
    "    \n",
    "   # TODO: Extract topic and max_analysts from state\n",
    "   topic = state['topic']\n",
    "   max_analysts = state['max_analysts']\n",
    "\n",
    "   \n",
    "   # TODO: Create a structured LLM that outputs Perspectives\n",
    "   # Use: llm.with_structured_output(Perspectives)\n",
    "   structured_llm = llm.with_structured_output(Perspectives)\n",
    "    \n",
    "   # TODO: Format the system message with topic and max_analysts\n",
    "   system_prompt = analyst_instructions.format(topic=topic, max_analysts=max_analysts)\n",
    "\n",
    "   # TODO: Invoke the structured LLM with system message and human message\n",
    "   analysts = structured_llm.invoke([SystemMessage(content=system_prompt)]+[HumanMessage(content='Generate a list of analysts')])\n",
    "   \n",
    "   \n",
    "   # TODO: Return the analysts in the correct state format\n",
    "   return {'analysts': analysts.analysts}\n",
    "\n",
    "create_analysts({'topic':'Compare langgraph and pydantic AI', 'max_analysts':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Test Your Implementation\n",
    "\n",
    "Create a simple graph and test your analyst generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Aisha Rahman\n",
      "Affiliation: Massachusetts General Hospital / Broad Institute\n",
      "Role: Clinical AI Research Lead\n",
      "Description: Board-certified radiologist and clinical researcher focused on integrating machine learning into diagnostic workflows. Expertise includes medical imaging AI (CT, MRI, X‑ray), clinical trial design for diagnostic tools, and regulatory pathways (FDA 510(k), De Novo). Interested in improving diagnostic accuracy, reducing time-to-diagnosis, and designing clinician-AI interfaces. Concerned about algorithmic bias across demographic groups, validation in diverse clinical settings, interpretability of models for clinical decision-making, and ensuring prospective clinical trials rather than retrospective validation alone.\n",
      "--------------------------------------------------\n",
      "Name: Marcus Li\n",
      "Affiliation: HealTech Ventures\n",
      "Role: Healthtech Strategy Partner & Data Privacy Advisor\n",
      "Description: Investor and former healthcare data engineer specializing in commercialization of AI diagnostics and data governance. Expertise spans product-market fit for diagnostics startups, health data interoperability (FHIR), secure multi-party computation, and HIPAA/GDPR compliance. Focused on scalability, reimbursement pathways, and ROI for hospitals adopting AI diagnostics. Potential concerns include data privacy risks, liability allocation between vendors and providers, dataset representativeness for training, and sustainable business models that align incentives for clinicians, patients, and payers.\n",
      "--------------------------------------------------\n",
      "Name: Prof. Elena García\n",
      "Affiliation: Universidad Autónoma de Madrid, Department of Biomedical Engineering\n",
      "Role: Academic Ethicist & Biomedical Engineer\n",
      "Description: Professor investigating socio-ethical implications of AI in medicine, combining signal processing expertise with qualitative research on patient-clinician relationships. Research areas include algorithmic transparency, informed consent for AI-assisted diagnosis, and effects of automation on clinician skill retention. Interested in equitable access to AI diagnostic tools across resource-limited settings and in policy frameworks that protect patient autonomy. Concerned about deskilling of clinicians, opaque proprietary models limiting auditability, and widening health disparities if AI tools prioritize profitable populations.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a StateGraph with AnalystGenerationState\n",
    "# Add the create_analysts node\n",
    "# Connect START -> create_analysts -> END\n",
    "# Compile the graph\n",
    "\n",
    "builder = StateGraph(AnalystGenerationState)\n",
    "builder.add_node('create_analysts', create_analysts)\n",
    "builder.add_edge(START, 'create_analysts')\n",
    "builder.add_edge('create_analysts', END)\n",
    "graph = builder.compile()\n",
    "# Test with this topic:\n",
    "test_topic = \"The impact of artificial intelligence on healthcare diagnostics\"\n",
    "max_analysts = 3\n",
    "\n",
    "# Run the graph and print results\n",
    "result = graph.invoke({\"topic\": test_topic, \"max_analysts\": max_analysts})\n",
    "for analyst in result['analysts']:\n",
    "    print(f\"Name: {analyst.name}\")\n",
    "    print(f\"Affiliation: {analyst.affiliation}\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    print(f\"Description: {analyst.description}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 **Checkpoint 1**: Verify Your Solution\n",
    "\n",
    "Your analyst generator should:\n",
    "- ✅ Create exactly the requested number of analysts\n",
    "- ✅ Generate diverse perspectives relevant to the topic\n",
    "- ✅ Include realistic names, affiliations, and roles\n",
    "- ✅ Provide detailed descriptions of each analyst's focus\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Human Feedback Loop (Intermediate)\n",
    "\n",
    "## Objective\n",
    "Extend your analyst generation system to include human-in-the-loop feedback for refining analyst selection.\n",
    "\n",
    "## Background\n",
    "Real-world research assistants need human oversight. Users should be able to review generated analysts and provide feedback to improve the selection before proceeding with research.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "1. **Extend State Schema**: Add human feedback capability\n",
    "2. **Create Feedback Node**: Implement a human feedback interruption point\n",
    "3. **Add Conditional Logic**: Route based on feedback presence\n",
    "4. **Test Interactive Flow**: Experience the human-in-the-loop workflow\n",
    "\n",
    "### Step 1: Extend State Schema\n",
    "\n",
    "Update your state to include human feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedAnalystState(TypedDict):\n",
    "    topic: str\n",
    "    max_analysts: int\n",
    "    analysts: List[Analyst]\n",
    "    # TODO: Add human_analyst_feedback: str field\n",
    "    human_analyst_feedback: str  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Update Analyst Creation Function\n",
    "\n",
    "Modify your analyst creation to consider human feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:\n",
    "\n",
    "1. First, review the research topic: {topic}\n",
    "        \n",
    "2. Examine any editorial feedback that has been provided to guide creation of the analysts: \n",
    "{human_analyst_feedback}\n",
    "    \n",
    "3. Determine the most interesting themes based upon the topic and feedback above.\n",
    "                    \n",
    "4. Pick the top {max_analysts} themes.\n",
    "\n",
    "5. Assign one analyst to each theme, incorporating any specific feedback provided.\"\"\"\n",
    "\n",
    "def create_analysts_with_feedback(state: EnhancedAnalystState):\n",
    "    \"\"\" Create analysts, considering human feedback if provided \"\"\"\n",
    "    \n",
    "    topic = state['topic']\n",
    "    max_analysts = state['max_analysts']\n",
    "    # TODO: Get human_analyst_feedback from state, default to empty string if not present\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback','')\n",
    "    \n",
    "    # TODO: Create structured LLM\n",
    "    llm_with_struct = llm.with_structured_output(Perspectives)\n",
    "    \n",
    "    # TODO: Format system message with topic, feedback, and max_analysts\n",
    "    \n",
    "    system_prompt = enhanced_analyst_instructions.format(topic=topic, human_analyst_feedback=human_analyst_feedback,\n",
    "                                                         max_analysts=max_analysts)\n",
    "    # print(system_prompt)\n",
    "    \n",
    "    # TODO: Generate analysts and return in state format\n",
    "    result = llm_with_struct.invoke([SystemMessage(content=system_prompt)]+[HumanMessage(content='Generate a list of analysts')])\n",
    "\n",
    "    return {'analysts':result.analysts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Human Feedback Components\n",
    "\n",
    "Implement the feedback node and routing logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback(state: EnhancedAnalystState):\n",
    "    \"\"\" No-op node that should be interrupted on \"\"\"\n",
    "    # TODO: This should be a pass-through node\n",
    "    # The interruption happens in the graph compilation\n",
    "    pass\n",
    "\n",
    "def should_continue(state: EnhancedAnalystState) -> Literal['create_analysts', END]:\n",
    "    \"\"\" Determine next node based on feedback presence \"\"\"\n",
    "    \n",
    "    # TODO: Check if human_analyst_feedback exists and is not None/empty\n",
    "    # If feedback exists, return \"create_analysts\" to regenerate\n",
    "    # Otherwise, return END\n",
    "    # if there is human feedback, then recreate the analysts\n",
    "    if state.get('human_analyst_feedback',None):\n",
    "        return 'create_analysts'\n",
    "    else:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build Interactive Graph\n",
    "\n",
    "Create a graph with human-in-the-loop functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Create StateGraph with EnhancedAnalystState\n",
    "# Add nodes: create_analysts_with_feedback, human_feedback\n",
    "# Add edges: START -> create_analysts_with_feedback -> human_feedback\n",
    "# Add conditional edge: human_feedback -> should_continue -> [\"create_analysts_with_feedback\", END]\n",
    "# Compile with interrupt_before=['human_feedback'] and MemorySaver checkpointer\n",
    "\n",
    "builder = StateGraph(EnhancedAnalystState)\n",
    "builder.add_node('create_analysts', create_analysts_with_feedback)\n",
    "builder.add_node('human_feedback', human_feedback)\n",
    "builder.add_edge(START, 'create_analysts')\n",
    "\n",
    "builder.add_edge('create_analysts', 'human_feedback')\n",
    "builder.add_conditional_edges('human_feedback', should_continue)\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "\n",
    "# Display the graph\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test Interactive Flow\n",
    "\n",
    "Experience the human feedback workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Analysts:\n",
      "- Dr. Amina Rahman, International Renewable Energy Institute, Grid-scale Storage Strategist\n",
      "- Prof. Lucas Moreno, Center for Advanced Materials and Energy, Materials & Battery Technology Specialist\n",
      "- Sofia Petrov, Distributed Energy Futures Lab, Distributed Storage & Systems Integration Analyst\n",
      "--------------------------------------------------\n",
      "Initial analysts generated. Waiting for human feedback...\n"
     ]
    }
   ],
   "source": [
    "# TODO: Set up initial parameters\n",
    "topic = \"The future of renewable energy storage technologies\"\n",
    "max_analysts = 3\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# TODO: Run until first interruption\n",
    "# Use graph.stream() with stream_mode=\"values\"\n",
    "# Print analysts when they appear in events\n",
    "# steam will return the graph state\n",
    "# thread is a required argument for .stream()\n",
    "\n",
    "for cur_state in graph.stream({'topic': topic, 'max_analysts':max_analysts}, thread, stream_mode='values'):\n",
    "    analysts = cur_state.get('analysts','')\n",
    "    if analysts:\n",
    "        print(\"Generated Analysts:\")\n",
    "        for analyst in analysts:\n",
    "            print(f\"- {analyst.name}, {analyst.affiliation}, {analyst.role}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        \n",
    "    \n",
    "print(\"Initial analysts generated. Waiting for human feedback...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback provided: Add a policy analyst focused on government regulations and incentives\n"
     ]
    }
   ],
   "source": [
    "# TODO: Provide feedback using graph.update_state()\n",
    "# Example feedback: \"Add a policy analyst focused on government regulations and incentives\"\n",
    "# Use as_node=\"human_feedback\"\n",
    "\n",
    "feedback = \"Add a policy analyst focused on government regulations and incentives\"\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": feedback}, as_node=\"human_feedback\")\n",
    "\n",
    "print(f\"Feedback provided: {feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'create_analysts': {'analysts': [Analyst(name='Dr. Maya Alvarez', affiliation='RenewGrid Institute', role='Technology Roadmapping Lead', description=\"Focuses on advanced battery chemistries and long-duration storage solutions. Maya's expertise spans grid-scale lithium-ion improvements, flow batteries, solid-state systems, and novel chemistries (sodium-ion, magnesium, metal-air). She models technology readiness, cost trajectories, and supply-chain constraints to identify likely commercialization pathways over the next 5–20 years.\"), Analyst(name='Prof. Thomas Okoye', affiliation='Center for Energy Systems Research, University of Lagos', role='Systems Integration and Grid Architect', description='Specializes in integrating diverse storage technologies into power systems. Thomas analyzes operational strategies (frequency regulation, peak shifting, capacity firming), hybridization of storage with renewables, and distribution vs. transmission-level deployment. He focuses on interoperability, control algorithms, and resiliency benefits for grids with high variable renewable penetration.'), Analyst(name='Aisha Bennett', affiliation='Energy Policy Futures', role='Policy Analyst — Regulations & Incentives', description='Concentrates on government policy, regulatory frameworks, and incentive design to accelerate storage deployment. Aisha evaluates subsidy programs, market rules for energy and capacity markets, permitting reforms, and public procurement strategies. She assesses how policy can lower deployment barriers, de-risk investment, and ensure equitable access to storage benefits.')]}}\n",
      "{'__interrupt__': ()}\n",
      "Updated analysts after feedback:\n",
      "Generated Analysts:\n",
      "- Dr. Maya Alvarez, RenewGrid Institute, Technology Roadmapping Lead\n",
      "- Prof. Thomas Okoye, Center for Energy Systems Research, University of Lagos, Systems Integration and Grid Architect\n",
      "- Aisha Bennett, Energy Policy Futures, Policy Analyst — Regulations & Incentives\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: Continue execution after feedback\n",
    "# Use graph.stream(None, thread, stream_mode=\"values\")\n",
    "# Print the updated analysts\n",
    "# values will return the full state\n",
    "# update will return a dict of node name and output\n",
    "\n",
    "for cur_state in graph.stream(None, thread, stream_mode='updates'):\n",
    "    print(cur_state)\n",
    "\n",
    "print(\"Updated analysts after feedback:\")\n",
    "\n",
    "# get state will return the latest statesnapshot\n",
    "state = graph.get_state(thread) \n",
    "analysts = state.values.get('analysts','')\n",
    "if analysts:\n",
    "    print(\"Generated Analysts:\")\n",
    "    for analyst in analysts:\n",
    "        print(f\"- {analyst.name}, {analyst.affiliation}, {analyst.role}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "graph.update_state(thread, {'human_analyst_feedback':None}, as_node='human_feedback')\n",
    "\n",
    "# run till the end\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(event.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 **Checkpoint 2**: Verify Your Solution\n",
    "\n",
    "Your interactive system should:\n",
    "- ✅ Generate initial analysts and pause for feedback\n",
    "- ✅ Accept human feedback and regenerate analysts accordingly\n",
    "- ✅ Allow multiple rounds of feedback if needed\n",
    "- ✅ Proceed to completion when no feedback is provided\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Expert Interview System (Intermediate)\n",
    "\n",
    "## Objective\n",
    "Build an interview system where AI analysts conduct multi-turn conversations with AI experts to gather insights.\n",
    "\n",
    "## Background\n",
    "The heart of the research assistant is conducting thorough interviews. Analysts ask questions, experts search for information and provide answers, and the conversation continues until sufficient insights are gathered.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "1. **Design Interview State**: Create state schema for managing conversations\n",
    "2. **Build Question Generator**: Implement analyst question generation\n",
    "3. **Create Search Functions**: Add web and Wikipedia search capabilities\n",
    "4. **Implement Answer Generator**: Create expert response system\n",
    "5. **Add Conversation Control**: Manage interview flow and termination\n",
    "\n",
    "### Step 1: Define Interview State\n",
    "\n",
    "Create a state schema for managing interviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewState(MessagesState):\n",
    "    # TODO: Add the following fields:\n",
    "    # - max_num_turns: int (maximum conversation turns)\n",
    "    # - context: Annotated[list, operator.add] (source documents)\n",
    "    # - analyst: Analyst (the analyst conducting interview)\n",
    "    # - interview: str (final interview transcript)\n",
    "    # pass\n",
    "    max_num_turns: int \n",
    "    context: Annotated[list, operator.add]\n",
    "    analyst: Analyst\n",
    "    interview: str\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    # TODO: Add search_query field with appropriate description\n",
    "    search_query: str = Field(description='Search query for gathering more information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Question Generation\n",
    "\n",
    "Create a function for analysts to generate interview questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
    "\n",
    "Your goal is boil down to interesting and specific insights related to your topic.\n",
    "\n",
    "1. Interesting: Insights that people will find surprising or non-obvious.\n",
    "        \n",
    "2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
    "\n",
    "Here is your topic of focus and set of goals: {goals}\n",
    "        \n",
    "Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
    "\n",
    "Continue to ask questions to drill down and refine your understanding of the topic.\n",
    "        \n",
    "When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "\n",
    "Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"\n",
    "\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\" Generate analyst question \"\"\"\n",
    "    \n",
    "    # TODO: Extract analyst and messages from state\n",
    "    analyst = state['analyst']\n",
    "    messages = state['messages']\n",
    "    \n",
    "    # TODO: Create system message using question_instructions template\n",
    "    # Format with analyst.persona as goals\n",
    "    system_prompt = question_instructions.format(goals=analyst.persona)\n",
    "    \n",
    "    # TODO: Invoke LLM with system message + existing messages\n",
    "    questions = llm.invoke([SystemMessage(content=system_prompt)]+messages)\n",
    "    \n",
    "    # TODO: Return new message in messages list\n",
    "    return {'messages': [questions]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Search Functions\n",
    "\n",
    "Implement web and Wikipedia search capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up search tools\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "\n",
    "# Search query instructions\n",
    "search_instructions = SystemMessage(content=\"\"\"You will be given a conversation between an analyst and an expert. \n",
    "\n",
    "Your goal is to generate a well-structured query for retrieval and web search.\n",
    "        \n",
    "Analyze the full conversation and pay particular attention to the final question posed by the analyst.\n",
    "\n",
    "Convert this final question into a well-structured web search query.\"\"\")\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\" Search web for relevant information \"\"\"\n",
    "    \n",
    "    # TODO: Create structured LLM for SearchQuery\n",
    "    llm_search = llm.with_structured_output(SearchQuery)\n",
    "    \n",
    "    # TODO: Generate search query from conversation messages\n",
    "    queries = llm_search.invoke([search_instructions]+state['messages'])\n",
    "    \n",
    "    # TODO: Use tavily_search.invoke() to get search results\n",
    "    search_doc = tavily_search.invoke(queries.search_query)\n",
    "    print('Tavily results:', search_doc)\n",
    "    # TODO: Format search results as documents with <Document> tags\n",
    "    # Format: '<Document href=\"{url}\"/>\\n{content}\\n</Document>'\n",
    "    if isinstance(search_doc, list):\n",
    "        formatted_search_docs = '\\n\\n---\\n\\n'.join([\n",
    "            f'<Document href=\"{doc['url']}\"/>\\n{doc['content']}\\n</Document>' for doc in search_doc\n",
    "        ])\n",
    "    else:\n",
    "        formatted_search_docs = ''\n",
    "    \n",
    "    # TODO: Return formatted docs in context list\n",
    "    return {'context': [formatted_search_docs]}\n",
    "\n",
    "def search_wikipedia(state: InterviewState):\n",
    "    \"\"\" Search Wikipedia for relevant information \"\"\"\n",
    "    \n",
    "    # TODO: Similar to search_web but use WikipediaLoader\n",
    "    # Load max 2 documents\n",
    "    # Format with source and page metadata\n",
    "    llm_search = llm.with_structured_output(SearchQuery)\n",
    "    queries = llm_search.invoke([search_instructions]+state['messages'])\n",
    "\n",
    "    search_docs = WikipediaLoader(queries.search_query, load_max_docs=2).load()\n",
    "    # print(queries)\n",
    "    print('wikipedia results', search_docs)\n",
    "\n",
    "    # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {'context': [formatted_search_docs]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Answer Generation\n",
    "\n",
    "Create the expert response system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
    "\n",
    "Here is the analyst's area of focus: {goals}\n",
    "        \n",
    "Your goal is to answer the question posed by the interviewer using this context:\n",
    "        \n",
    "{context}\n",
    "\n",
    "Guidelines:\n",
    "        \n",
    "1. Use only information provided in the context.\n",
    "2. Do not introduce external information beyond what is in the context.\n",
    "3. Include source citations using [1], [2], etc.\n",
    "4. List sources at the end of your answer.\n",
    "5. Be specific and provide concrete examples when available.\"\"\"\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\" Generate expert answer using search context \"\"\"\n",
    "    \n",
    "    # TODO: Extract analyst, messages, and context from state\n",
    "    analyst = state['analyst']\n",
    "    messages = state['messages']\n",
    "    context = state['context']\n",
    "    \n",
    "    # TODO: Format system message with analyst goals and context\n",
    "    system_prompt = answer_instructions.format(goals=analyst.persona, context=context)\n",
    "    \n",
    "    # TODO: Generate answer using LLM\n",
    "    answer = llm.invoke([SystemMessage(content=system_prompt)]+messages)\n",
    "    \n",
    "    # TODO: Set answer.name = \"expert\"\n",
    "    answer.name = 'expert'\n",
    "    \n",
    "    # TODO: Return answer in messages list\n",
    "    return {'messages': [answer]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Add Interview Control\n",
    "\n",
    "Implement conversation flow management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "def route_conversation(state: InterviewState) -> Literal['end_interview', 'continue_questions']:\n",
    "    \"\"\" Route between continuing questions or ending interview \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get('max_num_turns', 3)\n",
    "\n",
    "    # TODO: Count expert responses (messages with name=\"expert\")\n",
    "    num_expert_response = len([m for m in messages if m.name =='expert'])\n",
    "    \n",
    "    # TODO: If expert responses >= max_num_turns, return 'end_interview'\n",
    "    if num_expert_response >= max_num_turns:\n",
    "        return 'end_interview'\n",
    "    \n",
    "    # TODO: Check if last question contains \"Thank you so much for your help\"\n",
    "    # If yes, return 'end_interview'\n",
    "    if 'Thank you so much for your help' in messages[-1].content:\n",
    "        return 'end_interview'\n",
    "    \n",
    "    # TODO: Otherwise return 'continue_questions'\n",
    "    return 'continue_questions'\n",
    "\n",
    "def save_interview(state: InterviewState):\n",
    "    \"\"\" Convert interview to string format \"\"\"\n",
    "    \n",
    "    # TODO: Get messages from state\n",
    "    messages = state['messages']\n",
    "    \n",
    "    # TODO: Use get_buffer_string() to convert to text\n",
    "    interview = get_buffer_string(messages)\n",
    "    \n",
    "    # TODO: Return as interview field, be careful for the return type\n",
    "    return {'interview': interview}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build Interview Graph\n",
    "\n",
    "Assemble your interview system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create StateGraph with InterviewState\n",
    "# Add nodes: generate_question, search_web, search_wikipedia, generate_answer, save_interview\n",
    "builder = StateGraph(InterviewState)\n",
    "builder.add_node('generate_question', generate_question)\n",
    "builder.add_node('search_web', search_web)\n",
    "builder.add_node('search_wikipedia', search_wikipedia)\n",
    "builder.add_node('generate_answer', generate_answer)\n",
    "builder.add_node('save_interview', save_interview)\n",
    "\n",
    "# TODO: Add edges:\n",
    "# START -> generate_question\n",
    "# generate_question -> search_web (parallel)\n",
    "# generate_question -> search_wikipedia (parallel)\n",
    "# Both searches -> generate_answer\n",
    "# generate_answer -> route_conversation (conditional)\n",
    "# route: continue_questions -> generate_question, end_interview -> save_interview\n",
    "# save_interview -> END\n",
    "builder.add_edge(START, 'generate_question')\n",
    "builder.add_edge('generate_question', 'search_wikipedia')\n",
    "builder.add_edge('generate_question', 'search_web')\n",
    "builder.add_edge('search_web', 'generate_answer')\n",
    "builder.add_edge('search_wikipedia', 'generate_answer')\n",
    "builder.add_conditional_edges('generate_answer', route_conversation, \n",
    "                              {'continue_questions': 'generate_question', 'end_interview':'save_interview'})\n",
    "\n",
    "\n",
    "# TODO: Compile with MemorySaver\n",
    "memory = MemorySaver()\n",
    "interview_graph = builder.compile(memory)\n",
    "\n",
    "# Display graph\n",
    "\n",
    "# display(Image(interview_graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "\n",
    "# interview_graph.get_graph()\n",
    "\n",
    "# display(Image(interview_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Test Interview System\n",
    "\n",
    "Run a sample interview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia results []\n",
      "Tavily results: [{'title': '[PDF] A Systematic Review of Barriers to Renewable Energy Integration ...', 'url': 'https://ejournals.lib.hkbu.edu.hk/index.php/jaes/article/download/2881/2385', 'content': 'Policy fragmentation remains a pervasive barrier to renewable energy deployment.  high-lighted India’s struggles with poor coordination between central and state-level policies, which hampers the realization of its renewable energy targets. Similarly,  described how the global governance of renewable energy is fragmented, with nations prioritizing domestic policies over international cooperation. Inconsistent regulatory frameworks discourage investment by creating uncertainty for stakeholders. [...] One of the most significant economic barriers to renewable energy adoption is the high upfront cost of infrastructure and technology.  noted that these costs deter private sector investment, especially in developing regions where economic resources are limited. Similarly,  emphasized the financial burden on traditional oil and gas companies transitioning to renewable energy, which requires substantial investments and involves complex risk management. [...] While public engagement has proven effective in increasing acceptance, it remains underutilized in many regions. For instance,  noted that in the Canary Islands, psychosocial factors such as perceived utility, emotional responses, and risk perception significantly influence acceptance of both renewable and non-renewable energy projects. This highlights the importance of tailored communication strategies that address local concerns and foster a sense of ownership among communities.', 'score': 0.5142789}, {'title': '6 Major Barriers to Renewable Energy Adoption and How ... - Wildtribe', 'url': 'https://www.wildtribe.agency/post/6-major-barriers-to-renewable-energy-adoption-and-how-marketings-role-is-vital', 'content': \"Concerns about insufficient infrastructure, such as charging stations for electric vehicles or renewable energy storage facilities, can discourage adoption. Marketing can highlight ongoing infrastructure development projects, showcasing progress - such as the number of publicly accessible chargers in Europe increasing by 30% in 2021 for both slow and fast EV charging stations - and the expanding accessibility of renewable energy solutions.\\n\\n### 6. Perception of Inconvenience / Viability [...] ### 4. Policy and Regulations\\n\\nInadequate or unclear policies and regulations hinder renewable energy growth. A strong majority of energy professionals (88%) say accelerating permitting and licensing is critical to meeting climate goals.\\n\\nMarketing can advocate for supportive policies and actively encourage public engagement and participation in pressuring governance to shape regulations that promote renewable energy and will help us meet our climate goals.\\n\\n### 5. Infrastructure [...] Several barriers are inhibiting our readiness or willingness to embrace green energy as a trusted energy source. Among them are investments in grid infrastructure, supply chain issues, finance/investment, and governmental policies and actions - or lack of them.\\n\\nAlongside these, myths and misconceptions about renewables' viability are actively pushed into the public arena, slowing communities' willingness to adopt them.\", 'score': 0.50490767}, {'title': 'Evaluating Barriers to Renewable Energy Adoption', 'url': 'https://www.cyis.org/post/evaluating-barriers-to-renewable-energy-adoption', 'content': 'barriers, we can create a more favorable environment for the adoption of renewable energy,\\n\\nensuring a sustainable and resilient energy future.\\n\\nEnvironmental and Geographical Barriers\\n\\nOne of the most significant barriers to renewable energy adoption is environmental and\\n\\ngeographical constraints. The availability of renewable resources is geographically variable, making it necessary to develop tailored energy strategies for different regions. Some areas have [...] transition to renewables. This resistance can be rooted in a fear of economic disruption, job\\n\\nlosses in traditional energy sectors, and a general reluctance to change established ways of living\\n\\nand working. Addressing this barrier requires comprehensive transition plans that include\\n\\nretraining programs, economic diversification strategies, and inclusive policy-making that\\n\\nconsiders the needs and concerns of affected communities. By tackling these social and cultural [...] environmental and economic benefits. Addressing these political and regulatory barriers through\\n\\nconsistent policy frameworks, streamlined regulations, and reducing fossil fuel lobbying is\\n\\ncritical for fostering the growth of renewable energy.\\n\\nSocial and Cultural Barriers\\n\\nPublic perception and awareness of renewable energy are equally critical for its acceptance and\\n\\nadoption. Misconceptions about the reliability, cost, and environmental impact of renewable', 'score': 0.49699774}]\n",
      "wikipedia results []\n",
      "Tavily results: [{'title': 'Navigating the Energy Transition in India: Challenges and ...', 'url': 'https://www.sciencedirect.com/science/article/pii/S2588912525000190', 'content': 'Financial barriers, investment uncertainty, and policy inconsistency across regions have been found to be significant market barriers that', 'score': 0.9815}, {'title': 'RE Policy Options for India - Enablers and Barriers', 'url': 'https://www.abacademies.org/articles/re-policy-options-for-india-enablers-and-barriers-16560.html', 'content': 'The cost of renewable energy (RE) technology is one of the major barrier to the acceptance of non-conventional power production. Further financial and socio-', 'score': 0.98049}, {'title': '[PDF] Case study - World Bank Documents & Reports', 'url': 'https://documents.worldbank.org/curated/en/838051468026936715/pdf/425290PUB0ISBN11OFFICIAL0USE0ONLY10.pdf', 'content': 'promoting self-sustaining investment in energy generation from Renewable Sources,. Energy Efficiency and Environmental Technologies for sustainable', 'score': 0.98025}]\n",
      "Human: I understand you're researching barriers to renewable energy adoption?\n",
      "AI: Hi Dr. Chen — I’m Alex Morales, a policy analyst working on a briefing about accelerating renewable energy adoption in mid-sized U.S. regions. Thanks for taking the time.\n",
      "\n",
      "To start: In your work at the Climate Technology Institute, what have you found to be the single most underappreciated barrier to renewable energy adoption that policymakers and practitioners often miss? Can you give a concrete example from a project or case study you’ve worked on?\n",
      "AI: Thanks — here’s my answer focused on your question about the single most underappreciated barrier, with a concrete example.\n",
      "\n",
      "Most underappreciated barrier: policy fragmentation and poor multi-level coordination\n",
      "- Why it’s underappreciated: stakeholders often focus on visible problems (cost, technology, grid upgrades) while assuming policy is solvable through more funding or single-level reforms. But fragmentation across national/central and subnational/state levels creates persistent procedural uncertainty that deters investment, slows permitting, and prevents coherent deployment strategies. It can’t be fixed simply by money; it needs institutional coordination and sustained governance reforms to align incentives, timelines, and standards across levels of government [1].\n",
      "- How it operates concretely: inconsistent regulations and conflicting priorities mean project developers face different permitting rules, varied incentive designs, and shifting targets depending on jurisdiction. That regulatory uncertainty increases perceived investment risk and raises transaction costs, reducing private-sector appetite—especially in regions with limited public funds [1][2].\n",
      "\n",
      "Concrete example (from documented case)\n",
      "- India’s experience: analyses highlight how weak coordination between central and state-level policies has hampered India’s ability to meet its renewable targets. States and the center have mismatched policies and timelines, creating barriers to scaling projects and deterring investors who need predictable, harmonized regulatory environments to commit capital [1].\n",
      "- Impacts observed: delayed project approvals, duplicated or conflicting permitting requirements, and investment caution by private developers—particularly problematic where upfront capital costs are already high and financing is scarce. This combination slows deployment even when the technical resource (solar/wind potential) is available [1][2].\n",
      "\n",
      "Why this matters for mid-sized regions (policy implications)\n",
      "- Even if technology costs fall, fragmented governance will continue to produce uneven deployment and missed targets. Addressing fragmentation can unlock private capital more effectively than additional subsidies alone because it reduces uncertainty and administrative costs for developers [1][2].\n",
      "- Practical policy moves (illustrative, drawn from the documented problems): create clear, streamlined permitting across levels; harmonize incentive structures; establish joint central–state implementation units or memoranda of understanding; and prioritize accelerating licensing/permitting as a near-term objective (88% of energy professionals rate faster permitting/licensing as critical) [2].\n",
      "\n",
      "Supporting points from related barriers (to show interconnections)\n",
      "- High upfront costs and financing constraints amplify the effects of fragmentation: when capital is scarce, investors become especially sensitive to regulatory risk, so coordination failures disproportionately deter projects in developing regions [1].\n",
      "- Public engagement and perceptions: while governance fixes are essential, tailored public communication and community engagement are also needed to increase acceptance and reduce delays from local opposition; psychosocial factors (perceived utility, emotions, risk perception) shaped outcomes in places like the Canary Islands, showing the need for local-level involvement alongside higher-level coordination [1][3].\n",
      "- Infrastructure concerns (EV chargers, storage, grid upgrades) are often cited; marketing and advocacy can help create public pressure for coordinated policy reforms and infrastructure investment, but these efforts must be backed by aligned policy to be effective [2].\n",
      "\n",
      "Sources\n",
      "[1] Policy fragmentation, investment barriers, psychosocial/community acceptance examples (including India and Canary Islands). Source: ejournals.lib.hkbu.edu.hk article (document provided).\n",
      "[2] Industry/professional view on permitting/licensing criticality and infrastructure/marketing role (including 88% figure and EV charger growth). Source: wildtribe.agency post (document provided).\n",
      "[3] Social acceptance and environmental/geographical barriers, need for retraining and inclusive policymaking. Source: cyis.org post (document provided).\n",
      "\n",
      "If you’d like, I can: (a) draft a one-page checklist of institutional reforms to reduce fragmentation for mid-sized regions; or (b) map how fragmentation raises the cost of capital for developers with a simple causal diagram. Which would be most useful?\n",
      "AI: Thank you so much for your help!\n",
      "AI: You’re welcome — glad it helped. If you want next steps, I can prepare that checklist of institutional reforms or the causal diagram I mentioned, or tailor recommendations specifically to a region you’re studying. Which would be most useful?\n"
     ]
    }
   ],
   "source": [
    "# Create a test analyst\n",
    "test_analyst = Analyst(\n",
    "    name=\"Dr. Sarah Chen\",\n",
    "    affiliation=\"Climate Technology Institute\",\n",
    "    role=\"Sustainability Researcher\",\n",
    "    description=\"Focuses on renewable energy adoption barriers and policy solutions\"\n",
    ")\n",
    "\n",
    "# TODO: Set up initial interview state\n",
    "topic = \"barriers to renewable energy adoption\"\n",
    "initial_message = HumanMessage(content=f\"I understand you're researching {topic}?\")\n",
    "thread = {\"configurable\": {\"thread_id\": \"interview_test\"}}\n",
    "\n",
    "# TODO: Run interview and display results\n",
    "result = interview_graph.invoke({\n",
    "    \"analyst\": test_analyst,\n",
    "    \"messages\": [initial_message],\n",
    "    \"max_num_turns\": 2,\n",
    "    \"context\": [],\n",
    "    \"interview\": ''\n",
    "}, thread)\n",
    "\n",
    "# print(\"Interview completed!\")\n",
    "# print(\"\\nInterview transcript:\")\n",
    "print(result.get('interview', 'No interview saved'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 **Checkpoint 3**: Verify Your Solution\n",
    "\n",
    "Your interview system should:\n",
    "- ✅ Generate contextual questions based on analyst persona\n",
    "- ✅ Search multiple sources (web + Wikipedia) in parallel\n",
    "- ✅ Provide well-sourced expert answers\n",
    "- ✅ Control conversation flow based on turn limits or completion phrases\n",
    "- ✅ Save complete interview transcripts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Multi-Source Research Integration (Advanced)\n",
    "\n",
    "## Objective\n",
    "Enhance the interview system with section writing capabilities and integrate multiple research sources.\n",
    "\n",
    "## Background\n",
    "Raw interview transcripts need to be transformed into polished research sections. This exercise focuses on processing interview results and generating structured reports with proper citations.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "1. **Extend Interview State**: Add section writing capability\n",
    "2. **Create Section Writer**: Transform interviews into report sections\n",
    "3. **Add Custom Data Sources**: Integrate additional research sources\n",
    "4. **Test Enhanced Pipeline**: Run complete interview-to-section workflow\n",
    "\n",
    "### Step 1: Enhanced Interview State\n",
    "\n",
    "Extend the interview state to support section generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedInterviewState(MessagesState):\n",
    "    max_num_turns: int\n",
    "    context: Annotated[list, operator.add]\n",
    "    analyst: Analyst\n",
    "    interview: str\n",
    "    # TODO: Add sections field as list for report sections\n",
    "    sections: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Section Writer\n",
    "\n",
    "Create a function to transform interviews into structured sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
    "            \n",
    "Your task is to create a short, digestible section of a report based on source documents.\n",
    "\n",
    "1. Analyze the content of the source documents:\n",
    "- Document names are at the start with <Document> tags\n",
    "        \n",
    "2. Create a report structure using markdown:\n",
    "- Use ## for section title\n",
    "- Use ### for sub-section headers\n",
    "        \n",
    "3. Write the report with this structure:\n",
    "a. Title (## header)\n",
    "b. Summary (### header)  \n",
    "c. Sources (### header)\n",
    "\n",
    "4. Make your title engaging based on the analyst focus: {focus}\n",
    "\n",
    "5. For the summary section:\n",
    "- Provide background context related to the analyst's focus area\n",
    "- Emphasize novel, interesting, or surprising insights\n",
    "- Use numbered citations [1], [2], etc.\n",
    "- Target approximately 400 words maximum\n",
    "- Do not mention interviewer or expert names\n",
    "        \n",
    "6. In the Sources section:\n",
    "- List all sources used with full links/paths\n",
    "- Use newlines between sources\n",
    "- Combine duplicate sources\n",
    "\n",
    "### Sources\n",
    "[1] Source name or link\n",
    "[2] Source name or link\n",
    "\"\"\"\n",
    "\n",
    "def write_section(state: EnhancedInterviewState):\n",
    "    \"\"\" Transform interview and context into structured section \"\"\"\n",
    "    \n",
    "    # TODO: Extract interview, context, and analyst from state\n",
    "    interview = state['interview']\n",
    "    context = state['context']\n",
    "    analyst = state['analyst']\n",
    "\n",
    "    \n",
    "    # TODO: Format system message with analyst description as focus\n",
    "    system_prompt = SystemMessage(content=section_writer_instructions.format(focus=analyst.persona))\n",
    "    \n",
    "    # TODO: Create human message with context as source material\n",
    "    human_msg = HumanMessage(content=f'Use this to write your report: {context}') # the content sould be a string, not a list\n",
    "    \n",
    "    # TODO: Invoke LLM to generate section\n",
    "    report = llm.invoke([system_prompt]+[human_msg])\n",
    "    print('write section: ', report)\n",
    "    \n",
    "    # TODO: Return section content in sections list\n",
    "    return {'sections': [report]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add Custom Data Sources\n",
    "\n",
    "Create additional search capabilities for more diverse sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_academic_papers(state: EnhancedInterviewState):\n",
    "    \"\"\" Simulate academic paper search (placeholder implementation) \"\"\"\n",
    "    \n",
    "    # TODO: For this exercise, create a mock academic source\n",
    "    # In a real implementation, you might use arXiv API, Semantic Scholar, etc.\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
    "    \n",
    "    # Mock academic content based on search query\n",
    "    mock_academic_content = f\"\"\"<Document source=\"academic_db\" paper=\"{search_query.search_query.replace(' ', '_')}_2024.pdf\"/>\n",
    "Academic research findings related to {search_query.search_query}. This paper presents evidence-based insights \n",
    "and methodological approaches relevant to the research question. Key findings include statistical analysis, \n",
    "experimental results, and peer-reviewed conclusions that support the theoretical framework.\n",
    "</Document>\"\"\"\n",
    "    \n",
    "    return {\"context\": [mock_academic_content]}\n",
    "\n",
    "def search_industry_reports(state: EnhancedInterviewState):\n",
    "    \"\"\" Simulate industry report search \"\"\"\n",
    "    \n",
    "    # TODO: Create mock industry report content\n",
    "    # Similar structure to academic search but with industry focus\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
    "\n",
    "    mock_industrial_content = f\"\"\"<Document source=\"industrial_db\" paper=\"{search_query.search_query.replace(' ', '_')}_2024.pdf\"/>\n",
    "    Industrial research findings related to {search_query.search_query}. This paper presents evidence-based insights \n",
    "    and methodological approaches relevant to the research question. Key findings include marketing analysis, industral trend and future outlook.\n",
    "    </Document>\"\"\"\n",
    "\n",
    "    return {'context': [mock_industrial_content.content]} # the return type is AIMessage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build Enhanced Interview Graph\n",
    "\n",
    "Create an interview system with section writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              +-----------+                       \n",
      "                              | __start__ |                       \n",
      "                              +-----------+                       \n",
      "                                    *                             \n",
      "                                    *                             \n",
      "                                    *                             \n",
      "                          +-------------------+                   \n",
      "                          | generate_question |                   \n",
      "                        **+-------------------+..                 \n",
      "                    ****            *            .....            \n",
      "               *****               *                  ....        \n",
      "            ***                    *                      .....   \n",
      "+------------+           +------------------+                  ...\n",
      "| search_web |*          | search_wikipedia |             .....   \n",
      "+------------+ *****     +------------------+         ....        \n",
      "                    ****           *             .....            \n",
      "                        *****       *       .....                 \n",
      "                             ***    *    ...                      \n",
      "                          +-----------------+                     \n",
      "                          | generate_answer |                     \n",
      "                          +-----------------+                     \n",
      "                                    .                             \n",
      "                                    .                             \n",
      "                                    .                             \n",
      "                           +----------------+                     \n",
      "                           | save_interview |                     \n",
      "                           +----------------+                     \n",
      "                                    *                             \n",
      "                                    *                             \n",
      "                                    *                             \n",
      "                            +---------------+                     \n",
      "                            | write_section |                     \n",
      "                            +---------------+                     \n",
      "                                    *                             \n",
      "                                    *                             \n",
      "                                    *                             \n",
      "                              +---------+                         \n",
      "                              | __end__ |                         \n",
      "                              +---------+                         \n"
     ]
    }
   ],
   "source": [
    "# TODO: Create StateGraph with EnhancedInterviewState\n",
    "# Add all previous nodes plus:\n",
    "# - search_academic_papers\n",
    "# - search_industry_reports  \n",
    "# - write_section\n",
    "\n",
    "builder = StateGraph(EnhancedInterviewState)\n",
    "builder.add_node('generate_question', generate_question)\n",
    "builder.add_node('search_web', search_web)\n",
    "builder.add_node('search_wikipedia', search_wikipedia)\n",
    "builder.add_node('generate_answer', generate_answer)\n",
    "builder.add_node('save_interview', save_interview)\n",
    "builder.add_node('search_academic_papers', search_academic_papers)\n",
    "builder.add_node('search_industry_reports', search_industry_reports)\n",
    "builder.add_node('write_section', write_section)\n",
    "\n",
    "\n",
    "# TODO: Update edges to include new search sources in parallel\n",
    "# Add: save_interview -> write_section -> END\n",
    "builder.add_edge(START, 'generate_question')\n",
    "builder.add_edge('generate_question', 'search_wikipedia')\n",
    "builder.add_edge('generate_question', 'search_web')\n",
    "builder.add_edge('search_web', 'generate_answer')\n",
    "builder.add_edge('search_wikipedia', 'generate_answer')\n",
    "builder.add_conditional_edges('generate_answer', route_conversation, \n",
    "                              {'continue_questions': 'generate_question', 'end_interview':'save_interview'})\n",
    "\n",
    "builder.add_edge('save_interview', 'write_section')\n",
    "builder.add_edge('write_section', END)\n",
    "\n",
    "# TODO: Compile and display\n",
    "enhanced_interview_graph = builder.compile()\n",
    "\n",
    "# display(Image(enhanced_interview_graph.get_graph().draw_mermaid_png()))\n",
    "enhanced_interview_graph.get_graph().print_ascii()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test Enhanced System\n",
    "\n",
    "Run a complete interview with section generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia results []\n",
      "Tavily results: [{'title': '[PDF] Quantum Computing Use Case Compendium', 'url': 'https://www.nqcc.ac.uk/wp-content/uploads/2025/06/NQCC-Quantum-Computing-Use-Case-Compendium-web.pdf', 'content': '2 3 Quantum Computing Use Case Compendium Quantum Computing Use Case Compendium Executive Summary This Quantum Computing Use case Compendium brings together a record of industry-led proof-of-concept (PoC) and feasibility studies conducted between 2023 and 2025. These projects, facilitated by the National Quantum Computing Centre (NQCC) through its SparQ programme, explore practical applications of quantum computing across various sectors. Programme overview The SparQ initiative is central to [...] Projects are expected to appraise hardware and software requirements, verify current quantum computing capabilities, and develop models using quantum algorithms on real or emulated quantum processors. The 2024/2025 initiative successfully supported the exploration of 12 use cases, contributing to the UK’s efforts in advancing quantum readiness and fostering industry engagement in quantum technologies. [...] Dr Rob Whiteman Quantum Readiness Lead, NQCC  Projects provided 50% match funding or over  Match funding levels depending on organisation type Call NQCC funding Matched private investment Projects Organisations STFC Cross\\u202fCluster PoC (2024/25) £726k ≥£726k 12 28 Innovate\\u202fUK Industrial Applications Feasibility (2023 25) £6.2m >£2.8m 19 (NQCC core partner on 7) 61 Total £6.9m >£3.5m 31 80+ 4 5 Quantum Computing Use Case Compendium Quantum Computing Use Case Compendium Contents Foreword Dr Michael', 'score': 0.6844544}, {'title': 'Quantum Computing: How Close to Commercial Value? - IDTechEx', 'url': 'https://www.idtechex.com/en/research-article/quantum-computing-how-close-to-commercial-value/33552', 'content': \"US dollars already invested. Development roadmaps have accelerated, with several market leaders now predicting that commercially relevant quantum computing will arrive within the next 5 years. [...] 4, representing the first application-specific commercial use cases, and reach versatile deployment by approximately 2034. The QCRL scale, along with a historical database created from IDTechEx research in the industry, forms the basis of this report's 20-year market forecasts, which break down both the volume and revenue generated by each of the leading quantum computer hardware types.\", 'score': 0.6399392}, {'title': 'Quantum computing futures | Deloitte Insights', 'url': 'https://www.deloitte.com/us/en/insights/topics/emerging-technologies/quantum-computing-futures.html', 'content': 'Recent chip advancements, networking breakthroughs,2 and error correction3 reflect steady progress. Quantum computing vendors are projecting tangible business benefits by 2030 and accelerating their expected timelines to commercial scale over the next five to seven years.4 [...] last five years.15 Rafal Janik, chief operating officer at Xanadu, notes that, in the next three to five years, among chemistry, materials, pharmaceuticals, and energy companies, “There will be some really big winners that have already captured IP and partnerships in order to get access to those (quantum) devices.”16 [...] Overall investment in quantum computing is growing.It is estimated that the quantum computing market could grow nearly 35% annually from 2024 to 2032.26 Yuval Boger, chief commercial officer at QuEra Computing, explained that organizations are buying quantum computers “because they believe that in two or three years, they will be valuable.”27', 'score': 0.6175132}]\n"
     ]
    }
   ],
   "source": [
    "# Create a test analyst focused on a specific domain\n",
    "tech_analyst = Analyst(\n",
    "    name=\"Alex Rodriguez\",\n",
    "    affiliation=\"Future Tech Ventures\",\n",
    "    role=\"Technology Investment Analyst\",\n",
    "    description=\"Evaluates emerging technologies for investment potential, focusing on market viability, technical feasibility, and competitive advantages\"\n",
    ")\n",
    "# This will write the section based on the expertise of the provided analyst only\n",
    "\n",
    "# TODO: Run enhanced interview system\n",
    "topic = \"quantum computing commercialization prospects\"\n",
    "result = enhanced_interview_graph.invoke({\n",
    "    \"analyst\": tech_analyst,\n",
    "    \"messages\": [HumanMessage(content=f\"I'd like to discuss {topic}\")],\n",
    "    \"max_num_turns\": 2,\n",
    "    \"context\": [],\n",
    "    \"interview\": \"\"\n",
    "})\n",
    "\n",
    "# TODO: Display the generated section\n",
    "# TODO: Display the generated section\n",
    "if 'sections' in result and result['sections']:\n",
    "    display(Markdown(result['sections'][0].content[0]['text']))\n",
    "else:\n",
    "    print(\"No section generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\nconfig:\\n  flowchart:\\n    curve: linear\\n---\\ngraph TD;\\n\\t__start__(<p>__start__</p>)\\n\\tgenerate_question(generate_question)\\n\\tsearch_web(search_web)\\n\\tsearch_wikipedia(search_wikipedia)\\n\\tgenerate_answer(generate_answer)\\n\\tsave_interview(save_interview)\\n\\tsearch_academic_papers(search_academic_papers)\\n\\tsearch_industry_reports(search_industry_reports)\\n\\twrite_section(write_section)\\n\\t__end__(<p>__end__</p>)\\n\\t__start__ --> generate_question;\\n\\tgenerate_answer -. &nbsp;continue_questions&nbsp; .-> generate_question;\\n\\tgenerate_answer -. &nbsp;end_interview&nbsp; .-> save_interview;\\n\\tgenerate_question --> search_web;\\n\\tgenerate_question --> search_wikipedia;\\n\\tsave_interview --> write_section;\\n\\tsearch_web --> generate_answer;\\n\\tsearch_wikipedia --> generate_answer;\\n\\twrite_section --> __end__;\\n\\tclassDef default fill:#f2f0ff,line-height:1.2\\n\\tclassDef first fill-opacity:0\\n\\tclassDef last fill:#bfb6fc\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enhanced_interview_graph.get_graph().draw_mermaid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Investment Brief: Emerging Tech Opportunities — Tactical Insights for Future Tech Ventures\\n\\n### Summary\\nBackground: As a Technology Investment Analyst at Future Tech Ventures, your mandate is to identify emerging technologies with strong market fit, defensible technical advantages, and scalable business models. This brief summarizes available source material and highlights actionable gaps and next steps to support investment decision-making.\\n\\nKey findings and insights\\n1. Source availability and quality issues: The provided source set contained no substantive content, preventing direct extraction of market, technical, or competitive intelligence. This absence is itself informative: either the dataset was not supplied, or primary materials are missing, which increases due diligence risk and suggests an immediate need to source reliable inputs before any investment recommendations can be made. [1]\\n\\n2. Due diligence priorities: Given the content gap, prioritize acquiring the following datasets to evaluate opportunities effectively:\\n   - Technology whitepapers and patents to assess technical feasibility and novelty.\\n   - Market research reports and TAM/SAM estimates to evaluate market viability and growth trajectories.\\n   - Competitive landscape analyses, including incumbent capabilities and potential disruptors.\\n   - Customer validation evidence (pilot results, LOIs, or early revenue) to gauge product-market fit.\\n\\n3. Rapid discovery framework: To move from no data to investable insights quickly, implement a staged research plan:\\n   - Phase 1 — Data acquisition: Obtain primary sources (technical docs, IP filings), secondary market analyses, and customer references.\\n   - Phase 2 — Technical screening: Rapid technical review (feasibility, maturity, integration risk) and IP freedom-to-operate checks.\\n   - Phase 3 — Commercial validation: Customer interviews, pricing sensitivity, and go-to-market channel assessment.\\n   - Phase 4 — Competitive moat analysis: Identify proprietary tech, network effects, regulatory barriers, or capital intensity that create defensibility.\\n\\n4. Investment risk signals: In the absence of content, treat any opportunity as high risk until documentary evidence is provided. Key red flags to watch for once documents are supplied include lack of reproducible technical results, vague go-to-market strategies, absent IP or defensible trade secrets, and minimal customer traction.\\n\\n5. Recommended next steps for the analyst:\\n   - Request complete source documents, including any missing attachments or links.\\n   - If immediate screening is required, commission a focused data-gathering sprint (3–5 business days) to collect the items listed under Due diligence priorities.\\n   - Prepare a template due-diligence checklist aligned with your firm’s investment criteria to standardize evaluation once documents arrive.\\n\\n### Sources\\n[1] No substantive source content provided — input consisted of four empty strings.'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['sections'][0].content[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Investment Brief: Emerging Tech Opportunities — Tactical Insights for Future Tech Ventures\n",
       "\n",
       "### Summary\n",
       "Background: As a Technology Investment Analyst at Future Tech Ventures, your mandate is to identify emerging technologies with strong market fit, defensible technical advantages, and scalable business models. This brief summarizes available source material and highlights actionable gaps and next steps to support investment decision-making.\n",
       "\n",
       "Key findings and insights\n",
       "1. Source availability and quality issues: The provided source set contained no substantive content, preventing direct extraction of market, technical, or competitive intelligence. This absence is itself informative: either the dataset was not supplied, or primary materials are missing, which increases due diligence risk and suggests an immediate need to source reliable inputs before any investment recommendations can be made. [1]\n",
       "\n",
       "2. Due diligence priorities: Given the content gap, prioritize acquiring the following datasets to evaluate opportunities effectively:\n",
       "   - Technology whitepapers and patents to assess technical feasibility and novelty.\n",
       "   - Market research reports and TAM/SAM estimates to evaluate market viability and growth trajectories.\n",
       "   - Competitive landscape analyses, including incumbent capabilities and potential disruptors.\n",
       "   - Customer validation evidence (pilot results, LOIs, or early revenue) to gauge product-market fit.\n",
       "\n",
       "3. Rapid discovery framework: To move from no data to investable insights quickly, implement a staged research plan:\n",
       "   - Phase 1 — Data acquisition: Obtain primary sources (technical docs, IP filings), secondary market analyses, and customer references.\n",
       "   - Phase 2 — Technical screening: Rapid technical review (feasibility, maturity, integration risk) and IP freedom-to-operate checks.\n",
       "   - Phase 3 — Commercial validation: Customer interviews, pricing sensitivity, and go-to-market channel assessment.\n",
       "   - Phase 4 — Competitive moat analysis: Identify proprietary tech, network effects, regulatory barriers, or capital intensity that create defensibility.\n",
       "\n",
       "4. Investment risk signals: In the absence of content, treat any opportunity as high risk until documentary evidence is provided. Key red flags to watch for once documents are supplied include lack of reproducible technical results, vague go-to-market strategies, absent IP or defensible trade secrets, and minimal customer traction.\n",
       "\n",
       "5. Recommended next steps for the analyst:\n",
       "   - Request complete source documents, including any missing attachments or links.\n",
       "   - If immediate screening is required, commission a focused data-gathering sprint (3–5 business days) to collect the items listed under Due diligence priorities.\n",
       "   - Prepare a template due-diligence checklist aligned with your firm’s investment criteria to standardize evaluation once documents arrive.\n",
       "\n",
       "### Sources\n",
       "[1] No substantive source content provided — input consisted of four empty strings."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 **Checkpoint 4**: Verify Your Solution\n",
    "\n",
    "Your enhanced interview system should:\n",
    "- ✅ Generate structured report sections with proper markdown formatting\n",
    "- ✅ Include citations and source references\n",
    "- ✅ Integrate multiple data sources (web, Wikipedia, academic, industry)\n",
    "- ✅ Transform conversational interviews into professional report content\n",
    "- ✅ Handle diverse analyst perspectives and focus areas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Complete Research Assistant Pipeline (Advanced)\n",
    "\n",
    "## Objective\n",
    "Build the full research assistant system that generates analysts, conducts parallel interviews, and produces comprehensive research reports.\n",
    "\n",
    "## Background\n",
    "This is the capstone exercise where you'll combine all previous components into a sophisticated research assistant using LangGraph's `Send()` API for parallel processing and map-reduce patterns.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "1. **Design Complete State Schema**: Create comprehensive state for the full pipeline\n",
    "2. **Implement Map-Reduce Pattern**: Use Send() API for parallel interviews\n",
    "3. **Create Report Generation**: Build introduction, content, and conclusion writers\n",
    "4. **Assemble Final Pipeline**: Connect all components with proper flow control\n",
    "5. **Test Complete System**: Run end-to-end research assistant workflow\n",
    "\n",
    "### Step 1: Complete State Schema\n",
    "\n",
    "Design the master state for the research assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "class ResearchAssistantState(TypedDict):\n",
    "    # TODO: Add fields for complete research pipeline:\n",
    "    # - topic: str (research topic)\n",
    "    # - max_analysts: int (number of analysts)\n",
    "    # - human_analyst_feedback: str (human feedback)\n",
    "    # - analysts: List[Analyst] (generated analysts)\n",
    "    # - sections: Annotated[list, operator.add] (report sections from interviews)\n",
    "    # - introduction: str (report introduction)\n",
    "    # - content: str (main report content)\n",
    "    # - conclusion: str (report conclusion)\n",
    "    # - final_report: str (complete assembled report)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Interview Orchestration\n",
    "\n",
    "Create the map function that launches parallel interviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_all_interviews(state: ResearchAssistantState):\n",
    "    \"\"\" Launch parallel interviews using Send() API \"\"\"\n",
    "    \n",
    "    # TODO: Check if human feedback exists\n",
    "    # If yes, return \"create_analysts\" to regenerate\n",
    "    \n",
    "    # TODO: Otherwise, create Send() messages for each analyst\n",
    "    # Each Send should target \"conduct_interview\" with:\n",
    "    # - analyst: the specific analyst\n",
    "    # - messages: initial HumanMessage about the topic\n",
    "    \n",
    "    # Example structure:\n",
    "    # return [Send(\"conduct_interview\", {\n",
    "    #     \"analyst\": analyst,\n",
    "    #     \"messages\": [HumanMessage(content=f\"I'd like to discuss {topic}\")],\n",
    "    #     \"max_num_turns\": 2\n",
    "    # }) for analyst in state[\"analysts\"]]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Report Writers\n",
    "\n",
    "Implement functions to generate different parts of the final report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_writer_instructions = \"\"\"You are a technical writer creating a report on: {topic}\n",
    "    \n",
    "You have a team of analysts who each:\n",
    "1. Conducted expert interviews on specific sub-topics\n",
    "2. Wrote findings into memos\n",
    "\n",
    "Your task:\n",
    "1. Analyze the collection of memos from your analysts\n",
    "2. Consolidate insights into a cohesive narrative\n",
    "3. Tie together central ideas from all memos\n",
    "\n",
    "Format requirements:\n",
    "1. Use markdown formatting\n",
    "2. No preamble\n",
    "3. No sub-headings\n",
    "4. Start with: ## Insights\n",
    "5. Preserve citations [1], [2], etc.\n",
    "6. Create consolidated Sources section: ## Sources\n",
    "7. Don't mention analyst names\n",
    "\n",
    "Here are the analyst memos: {context}\"\"\"\n",
    "\n",
    "def write_report(state: ResearchAssistantState):\n",
    "    \"\"\" Consolidate all sections into main report content \"\"\"\n",
    "    \n",
    "    # TODO: Extract sections and topic from state\n",
    "    \n",
    "    # TODO: Join all sections with newlines\n",
    "    \n",
    "    # TODO: Format system message with topic and combined sections\n",
    "    \n",
    "    # TODO: Generate consolidated report content\n",
    "    \n",
    "    # TODO: Return in content field\n",
    "    pass\n",
    "\n",
    "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
    "\n",
    "You will be given all sections of the report.\n",
    "\n",
    "Write a crisp and compelling {section_type} section.\n",
    "\n",
    "Guidelines:\n",
    "- No preamble\n",
    "- Target ~100 words\n",
    "- Use markdown formatting\n",
    "- For introduction: compelling title with # header, then ## Introduction\n",
    "- For conclusion: ## Conclusion header\n",
    "- Preview (intro) or recap (conclusion) all report sections\n",
    "\n",
    "Report sections: {formatted_str_sections}\"\"\"\n",
    "\n",
    "def write_introduction(state: ResearchAssistantState):\n",
    "    \"\"\" Generate report introduction \"\"\"\n",
    "    \n",
    "    # TODO: Extract sections and topic\n",
    "    # TODO: Join sections and format instructions\n",
    "    # TODO: Generate introduction\n",
    "    # TODO: Return in introduction field\n",
    "    pass\n",
    "\n",
    "def write_conclusion(state: ResearchAssistantState):\n",
    "    \"\"\" Generate report conclusion \"\"\"\n",
    "    \n",
    "    # TODO: Similar to write_introduction but for conclusion\n",
    "    pass\n",
    "\n",
    "def finalize_report(state: ResearchAssistantState):\n",
    "    \"\"\" Assemble complete final report \"\"\"\n",
    "    \n",
    "    # TODO: Extract introduction, content, and conclusion\n",
    "    \n",
    "    # TODO: Clean up content format (remove \"## Insights\" prefix if present)\n",
    "    \n",
    "    # TODO: Handle sources section separation if needed\n",
    "    \n",
    "    # TODO: Assemble: introduction + \"\\n\\n---\\n\\n\" + content + \"\\n\\n---\\n\\n\" + conclusion\n",
    "    \n",
    "    # TODO: Add sources section if extracted\n",
    "    \n",
    "    # TODO: Return in final_report field\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Assemble Master Graph\n",
    "\n",
    "Connect all components into the complete research assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create StateGraph with ResearchAssistantState\n",
    "# Add nodes:\n",
    "# - create_analysts_with_feedback (from Exercise 2)\n",
    "# - human_feedback\n",
    "# - conduct_interview (use enhanced_interview_graph from Exercise 4)\n",
    "# - write_report\n",
    "# - write_introduction  \n",
    "# - write_conclusion\n",
    "# - finalize_report\n",
    "\n",
    "# TODO: Add edges:\n",
    "# START -> create_analysts_with_feedback\n",
    "# create_analysts_with_feedback -> human_feedback\n",
    "# human_feedback -> initiate_all_interviews (conditional)\n",
    "# conduct_interview -> write_report (parallel)\n",
    "# conduct_interview -> write_introduction (parallel) \n",
    "# conduct_interview -> write_conclusion (parallel)\n",
    "# [write_report, write_introduction, write_conclusion] -> finalize_report\n",
    "# finalize_report -> END\n",
    "\n",
    "# TODO: Compile with interrupt_before=['human_feedback']\n",
    "\n",
    "# research_assistant = builder.compile(interrupt_before=['human_feedback'], checkpointer=MemorySaver())\n",
    "# display(Image(research_assistant.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test Complete Research Assistant\n",
    "\n",
    "Run the full end-to-end workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up research parameters\n",
    "research_topic = \"The impact of edge computing on IoT device performance and security\"\n",
    "max_analysts = 3\n",
    "thread = {\"configurable\": {\"thread_id\": \"final_research\"}}\n",
    "\n",
    "print(f\"Starting research on: {research_topic}\")\n",
    "print(f\"Generating {max_analysts} analysts...\")\n",
    "\n",
    "# TODO: Run until first interruption (human feedback)\n",
    "# for event in research_assistant.stream({\"topic\": research_topic, \"max_analysts\": max_analysts}, thread):\n",
    "#     analysts = event.get('analysts', '')\n",
    "#     if analysts:\n",
    "#         print(f\"\\nGenerated {len(analysts)} analysts:\")\n",
    "#         for analyst in analysts:\n",
    "#             print(f\"- {analyst.name} ({analyst.role})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Provide feedback or continue without feedback\n",
    "# Option 1: Provide feedback\n",
    "# feedback = \"Add a cybersecurity specialist focused on IoT vulnerabilities\"\n",
    "# research_assistant.update_state(thread, {\"human_analyst_feedback\": feedback}, as_node=\"human_feedback\")\n",
    "\n",
    "# Option 2: Continue without feedback\n",
    "research_assistant.update_state(thread, {\"human_analyst_feedback\": None}, as_node=\"human_feedback\")\n",
    "\n",
    "print(\"Proceeding with research...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Continue execution and monitor progress\n",
    "# for event in research_assistant.stream(None, thread, stream_mode=\"updates\"):\n",
    "#     node_name = next(iter(event.keys()))\n",
    "#     print(f\"Executing: {node_name}\")\n",
    "    \n",
    "#     if node_name == \"conduct_interview\":\n",
    "#         print(\"  -> Conducting interviews in parallel...\")\n",
    "#     elif node_name in [\"write_report\", \"write_introduction\", \"write_conclusion\"]:\n",
    "#         print(f\"  -> Generating {node_name.replace('write_', '')}...\")\n",
    "#     elif node_name == \"finalize_report\":\n",
    "#         print(\"  -> Assembling final report...\")\n",
    "\n",
    "print(\"Research completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display final research report\n",
    "# final_state = research_assistant.get_state(thread)\n",
    "# final_report = final_state.values.get('final_report')\n",
    "\n",
    "# if final_report:\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"FINAL RESEARCH REPORT\")\n",
    "#     print(\"=\"*80)\n",
    "#     display(Markdown(final_report))\n",
    "# else:\n",
    "#     print(\"No final report generated\")\n",
    "\n",
    "# TODO: Display summary statistics\n",
    "# sections = final_state.values.get('sections', [])\n",
    "# print(f\"\\nReport generated from {len(sections)} analyst interviews\")\n",
    "# print(f\"Total report length: ~{len(final_report.split()) if final_report else 0} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 **Final Checkpoint**: Verify Complete System\n",
    "\n",
    "Your complete research assistant should:\n",
    "- ✅ Generate diverse analysts with human feedback capability\n",
    "- ✅ Conduct multiple interviews in parallel using Send() API\n",
    "- ✅ Integrate multiple data sources for comprehensive research\n",
    "- ✅ Transform interviews into structured report sections\n",
    "- ✅ Generate cohesive introduction, main content, and conclusion\n",
    "- ✅ Assemble professional final report with proper citations\n",
    "- ✅ Handle error cases and provide meaningful feedback\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎉 Congratulations!\n",
    "\n",
    "You've successfully built a sophisticated multi-agent research assistant using LangGraph! \n",
    "\n",
    "## What You've Accomplished\n",
    "\n",
    "Through these exercises, you've mastered:\n",
    "\n",
    "1. **Multi-Agent System Design**: Created specialized analyst personas with distinct expertise\n",
    "2. **Human-in-the-Loop Workflows**: Built interactive systems that incorporate human feedback\n",
    "3. **Conversational AI Systems**: Implemented multi-turn interview workflows with AI experts\n",
    "4. **Parallel Processing**: Used LangGraph's Send() API for concurrent operations\n",
    "5. **Data Integration**: Combined multiple information sources into cohesive research\n",
    "6. **Report Generation**: Transformed raw conversations into professional research reports\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Consider extending your research assistant with:\n",
    "\n",
    "- **Custom Data Sources**: Integrate domain-specific APIs or databases\n",
    "- **Advanced Citations**: Add more sophisticated reference management\n",
    "- **Quality Control**: Implement fact-checking and source verification\n",
    "- **User Interface**: Create a web interface for easier interaction\n",
    "- **Report Templates**: Support different output formats (executive summary, technical report, etc.)\n",
    "- **Collaborative Features**: Allow multiple users to provide feedback and guidance\n",
    "\n",
    "## Key Patterns You've Learned\n",
    "\n",
    "- **Map-Reduce with LangGraph**: Using Send() API for parallel processing\n",
    "- **State Management**: Designing complex state schemas for multi-step workflows\n",
    "- **Conditional Logic**: Routing based on state conditions and user input\n",
    "- **Human-AI Collaboration**: Building systems that enhance human decision-making\n",
    "- **Structured Output**: Using Pydantic models for reliable AI responses\n",
    "\n",
    "These patterns are fundamental to building production-grade AI applications!\n",
    "\n",
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. How might you adapt this research assistant for your specific domain or use case?\n",
    "2. What additional quality controls would be important for production deployment?\n",
    "3. How could you measure and improve the quality of generated reports?\n",
    "4. What ethical considerations should guide the development of AI research assistants?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [STORM Paper](https://arxiv.org/abs/2402.14207) - Academic foundation for this approach\n",
    "- [LangSmith Tracing](https://docs.smith.langchain.com/) - Monitor and debug your graphs\n",
    "- [Research Assistant Examples](https://github.com/langchain-ai/langchain-academy) - More advanced implementations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-academy (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
