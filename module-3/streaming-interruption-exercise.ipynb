{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming and Interruption - Exercise Notebook\n",
    "\n",
    "## Objectives\n",
    "Test your understanding of LangGraph streaming capabilities and how they integrate with interruption patterns by completing the exercises below.\n",
    "\n",
    "## What You Should Know\n",
    "- Different streaming modes: `values`, `updates`, and `messages`\n",
    "- How to stream full state vs. state updates vs. individual messages\n",
    "- Token-level streaming using `astream_events`\n",
    "- How streaming works with breakpoints and human-in-the-loop workflows\n",
    "- Using streaming for real-time feedback and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "import asyncio\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Streaming Modes\n",
    "\n",
    "Create a simple chatbot and explore different streaming modes.\n",
    "\n",
    "**Your Task:**\n",
    "1. Create a basic chatbot with conversation summarization\n",
    "2. Test streaming with `values` mode\n",
    "3. Test streaming with `updates` mode\n",
    "4. Compare the outputs and understand the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up the chatbot from the lesson\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n# Define the extended state class\nclass State(MessagesState):\n    summary: str\n\n# Define call_model function\ndef call_model(state: State, config: RunnableConfig):\n    \n    # Get summary if it exists\n    summary = state.get(\"summary\", \"\")\n\n    # If there is summary, then we add it\n    if summary:\n        \n        # Add summary to system message\n        system_message = f\"Summary of conversation earlier: {summary}\"\n\n        # Append summary to any newer messages\n        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n    \n    else:\n        messages = state[\"messages\"]\n    \n    response = model.invoke(messages, config)\n    return {\"messages\": response}\n\n# Define summarize_conversation function\ndef summarize_conversation(state: State):\n    \n    # First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n    # Create our summarization prompt \n    if summary:\n        \n        # A summary already exists\n        summary_message = (\n            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n        \n    else:\n        summary_message = \"Create a summary of the conversation above:\"\n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n    \n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n\n# Define should_continue function  \ndef should_continue(state: State):\n    \"\"\"Return the next node to execute.\"\"\"\n    \n    messages = state[\"messages\"]\n    \n    # If there are more than six messages, then we summarize the conversation\n    if len(messages) > 6:\n        return \"summarize_conversation\"\n    \n    # Otherwise we can just end\n    return END\n\n# Build the graph\nworkflow = StateGraph(State)\nworkflow.add_node(\"conversation\", call_model)\nworkflow.add_node(summarize_conversation)\n\n# Set the entrypoint as conversation\nworkflow.add_edge(START, \"conversation\")\nworkflow.add_conditional_edges(\"conversation\", should_continue)\nworkflow.add_edge(\"summarize_conversation\", END)\n\nmemory = MemorySaver()\ngraph = workflow.compile(checkpointer=memory)\n\nprint(\"Chatbot created successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Compare Streaming Modes\n",
    "\n",
    "Test the same conversation with different streaming modes and analyze the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test streaming with 'updates' mode\ndef test_updates_mode():\n    print(\"=== Testing 'updates' mode ===\")\n    config = {\"configurable\": {\"thread_id\": \"updates_test\"}}\n    \n    # Stream with updates mode - shows only state updates after each node\n    for chunk in graph.stream({\"messages\": [HumanMessage(content=\"Hello! Tell me about Python programming.\")]}, config, stream_mode=\"updates\"):\n        # Each chunk contains the node name as key and updated state as value\n        for node_name, state_update in chunk.items():\n            print(f\"Node '{node_name}' updated:\")\n            if \"messages\" in state_update:\n                # Print the new message that was added\n                state_update[\"messages\"].pretty_print()\n            if \"summary\" in state_update:\n                print(f\"Summary updated: {state_update['summary']}\")\n            print(\"-\" * 40)\n\n# Test streaming with 'values' mode  \ndef test_values_mode():\n    print(\"\\n=== Testing 'values' mode ===\")\n    config = {\"configurable\": {\"thread_id\": \"values_test\"}}\n    \n    # Stream with values mode - shows full state after each node\n    for state in graph.stream({\"messages\": [HumanMessage(content=\"Hello! Tell me about Python programming.\")]}, config, stream_mode=\"values\"):\n        print(f\"Full state contains {len(state['messages'])} messages:\")\n        # Print last message to show progress\n        if state[\"messages\"]:\n            state[\"messages\"][-1].pretty_print()\n        if state.get(\"summary\"):\n            print(f\"Current summary: {state['summary'][:100]}...\")\n        print(\"=\" * 60)\n\n# Test both modes\ntest_input = {\"messages\": [HumanMessage(content=\"Hello! Tell me about Python programming.\")]}\n\n# Run both tests and compare the outputs\nprint(\"COMPARISON: 'updates' mode shows only what changed in each step\")\nprint(\"'values' mode shows the complete state after each step\\n\")\n\ntest_updates_mode()\ntest_values_mode()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Token-Level Streaming\n",
    "\n",
    "Implement token-level streaming to see AI responses as they're generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Implement token streaming function\nasync def stream_tokens(graph, input_message, config, node_to_stream='conversation'):\n    \"\"\"\n    Stream tokens from a specific node in the graph\n    \"\"\"\n    print(f\"Streaming tokens from node: {node_to_stream}\")\n    print(\"Response: \", end=\"\", flush=True)\n    \n    # Use astream_events to get token-level streaming\n    # Filter for 'on_chat_model_stream' events from the specified node\n    async for event in graph.astream_events(input_message, config, version=\"v2\"):\n        # Check if this is a chat model stream event from our target node\n        if (event[\"event\"] == \"on_chat_model_stream\" and \n            event['metadata'].get('langgraph_node', '') == node_to_stream):\n            # Extract and print the token content\n            token_content = event[\"data\"][\"chunk\"].content\n            print(token_content, end=\"\", flush=True)\n    \n    print()  # New line after streaming\n\n# Test token streaming\nasync def test_token_streaming():\n    config = {\"configurable\": {\"thread_id\": \"token_test\"}}\n    input_message = {\"messages\": [HumanMessage(content=\"Explain machine learning in simple terms\")]}\n    \n    # Call your streaming function\n    await stream_tokens(graph, input_message, config)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"Token streaming complete! Notice how you saw each token as it was generated.\")\n\n# Run the async test\nawait test_token_streaming()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Streaming with Breakpoints\n",
    "\n",
    "Combine streaming with breakpoints to create an interactive experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langgraph.prebuilt import tools_condition, ToolNode\n\n# Define some tools for the agent\ndef get_weather(location: str) -> str:\n    \"\"\"Get weather information for a location.\"\"\"\n    return f\"The weather in {location} is sunny and 75Â°F.\"\n\ndef calculate(expression: str) -> str:\n    \"\"\"Calculate a mathematical expression.\"\"\"\n    try:\n        result = eval(expression)  # Note: eval is unsafe in production!\n        return f\"The result of {expression} is {result}\"\n    except:\n        return f\"Could not calculate {expression}\"\n\n# Create an agent with breakpoints and streaming\ntools = [get_weather, calculate]\nllm_with_tools = model.bind_tools(tools)\n\ndef agent_with_tools(state: MessagesState):\n    # Call the LLM with tools\n    response = llm_with_tools.invoke(state[\"messages\"])\n    return {\"messages\": [response]}\n\n# Build agent graph with breakpoint before tools\nagent_builder = StateGraph(MessagesState)\nagent_builder.add_node(\"assistant\", agent_with_tools)\nagent_builder.add_node(\"tools\", ToolNode(tools))\n\n# Add edges\nagent_builder.add_edge(START, \"assistant\")\nagent_builder.add_conditional_edges(\"assistant\", tools_condition)\nagent_builder.add_edge(\"tools\", \"assistant\")\n\n# Compile with breakpoint before tools node\nagent_memory = MemorySaver()\nagent_graph = agent_builder.compile(checkpointer=agent_memory, interrupt_before=[\"tools\"])\n\n# Function to stream until breakpoint and handle approval\ndef stream_with_approval(graph, input_msg, thread_config):\n    \"\"\"\n    Stream execution until breakpoint, show tool calls, get approval, continue\n    \"\"\"\n    print(\"Starting agent with streaming and approval...\")\n    \n    # Stream until breakpoint\n    print(\"\\n--- Streaming until breakpoint ---\")\n    for chunk in graph.stream(input_msg, thread_config, stream_mode=\"updates\"):\n        for node_name, state_update in chunk.items():\n            print(f\"Node '{node_name}' executed:\")\n            if \"messages\" in state_update:\n                last_message = state_update[\"messages\"]\n                print(f\"Message type: {type(last_message).__name__}\")\n                if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n                    print(\"Tool calls to be executed:\")\n                    for tool_call in last_message.tool_calls:\n                        print(f\"  - {tool_call['name']}: {tool_call['args']}\")\n                else:\n                    last_message.pretty_print()\n    \n    # Check what tools will be called\n    state = graph.get_state(thread_config)\n    print(f\"\\nGraph is interrupted. Next node: {state.next}\")\n    \n    # Simulate user approval (in real app, you'd get user input)\n    print(\"\\nSimulating user approval... â\")\n    time.sleep(1)\n    \n    # Continue execution\n    print(\"\\n--- Continuing after approval ---\")\n    for chunk in graph.stream(None, thread_config, stream_mode=\"updates\"):\n        for node_name, state_update in chunk.items():\n            print(f\"Node '{node_name}' executed:\")\n            if \"messages\" in state_update:\n                state_update[\"messages\"].pretty_print()\n            print(\"-\" * 40)\n\n# Test the streaming + breakpoint combination\ntest_config = {\"configurable\": {\"thread_id\": \"agent_test\"}}\ntest_message = {\"messages\": [HumanMessage(content=\"What's the weather in San Francisco and calculate 25 * 4?\")]}\n\nstream_with_approval(agent_graph, test_message, test_config)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Real-time Progress Monitoring\n",
    "\n",
    "Create a system that provides real-time progress updates during long-running operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom typing_extensions import TypedDict\n\nclass ProcessingState(TypedDict):\n    task: str\n    progress: float  # 0.0 to 1.0\n    status: str\n    result: str\n    steps_completed: list\n\n# Define processing steps that take time\ndef data_collection(state: ProcessingState) -> ProcessingState:\n    \"\"\"\n    Simulate data collection phase\n    \"\"\"\n    print(\"Starting data collection...\")\n    \n    # Simulate time-consuming task with progress updates\n    for i in range(3):\n        time.sleep(0.5)  # Simulate work\n        progress = (i + 1) / 3 * 0.3  # First 30% of total progress\n        print(f\"  Collecting data... {progress*100:.0f}% complete\")\n    \n    return {\n        \"task\": state[\"task\"],\n        \"progress\": 0.3,\n        \"status\": \"Data collection complete\",\n        \"result\": state.get(\"result\", \"\"),\n        \"steps_completed\": state.get(\"steps_completed\", []) + [\"data_collection\"]\n    }\n\ndef data_processing(state: ProcessingState) -> ProcessingState:\n    \"\"\"\n    Simulate data processing phase\n    \"\"\"\n    print(\"Starting data processing...\")\n    \n    # Simulate processing with multiple steps\n    steps = [\"Cleaning data\", \"Analyzing patterns\", \"Applying algorithms\"]\n    \n    for i, step in enumerate(steps):\n        time.sleep(0.3)  # Simulate work\n        progress = 0.3 + (i + 1) / len(steps) * 0.5  # 30% + next 50% of total progress\n        print(f\"  {step}... {progress*100:.0f}% complete\")\n    \n    return {\n        \"task\": state[\"task\"],\n        \"progress\": 0.8,\n        \"status\": \"Data processing complete\", \n        \"result\": \"Processed 10,000 records with 95% accuracy\",\n        \"steps_completed\": state[\"steps_completed\"] + [\"data_processing\"]\n    }\n\ndef result_generation(state: ProcessingState) -> ProcessingState:\n    \"\"\"\n    Generate final results\n    \"\"\"\n    print(\"Generating final results...\")\n    time.sleep(0.5)  # Simulate final processing\n    \n    final_result = f\"Task '{state['task']}' completed successfully. {state['result']}. Generated comprehensive report.\"\n    \n    return {\n        \"task\": state[\"task\"],\n        \"progress\": 1.0,\n        \"status\": \"Complete\",\n        \"result\": final_result,\n        \"steps_completed\": state[\"steps_completed\"] + [\"result_generation\"]\n    }\n\n# Build processing pipeline graph\nprocessing_builder = StateGraph(ProcessingState)\nprocessing_builder.add_node(\"data_collection\", data_collection)\nprocessing_builder.add_node(\"data_processing\", data_processing)\nprocessing_builder.add_node(\"result_generation\", result_generation)\n\n# Add edges to create pipeline\nprocessing_builder.add_edge(START, \"data_collection\")\nprocessing_builder.add_edge(\"data_collection\", \"data_processing\")\nprocessing_builder.add_edge(\"data_processing\", \"result_generation\")\nprocessing_builder.add_edge(\"result_generation\", END)\n\nprocessing_graph = processing_builder.compile()\n\n# Function to stream progress updates\ndef stream_progress(graph, initial_state, thread_config):\n    \"\"\"\n    Stream progress updates in real-time\n    \"\"\"\n    print(\"Starting long-running process...\")\n    print(f\"Task: {initial_state['task']}\")\n    print(\"=\"*60)\n    \n    # Stream with progress updates\n    for chunk in graph.stream(initial_state, thread_config, stream_mode=\"updates\"):\n        for node_name, state_update in chunk.items():\n            print(f\"\\n[{node_name.upper()}] {state_update['status']}\")\n            \n            # Show progress bar\n            progress = state_update['progress']\n            bar_length = 30\n            filled_length = int(bar_length * progress)\n            bar = 'â' * filled_length + 'â' * (bar_length - filled_length)\n            print(f\"Progress: [{bar}] {progress*100:.1f}%\")\n            \n            # Show completed steps\n            if state_update['steps_completed']:\n                print(f\"Completed steps: {', '.join(state_update['steps_completed'])}\")\n            \n            # Show result if available\n            if state_update.get('result') and state_update['result']:\n                print(f\"Current result: {state_update['result']}\")\n            \n            print(\"-\" * 40)\n\n# Test progress monitoring\ninitial_state = {\n    \"task\": \"Machine Learning Model Training\",\n    \"progress\": 0.0,\n    \"status\": \"Starting\",\n    \"result\": \"\",\n    \"steps_completed\": []\n}\n\nthread_config = {\"configurable\": {\"thread_id\": \"progress_test\"}}\nstream_progress(processing_graph, initial_state, thread_config)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Custom Streaming Event Handler\n",
    "\n",
    "Create a custom event handler that processes different types of streaming events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class StreamEventHandler:\n    def __init__(self):\n        self.message_count = 0\n        self.token_count = 0\n        self.event_log = []\n        self.nodes_executed = set()\n        self.total_execution_time = 0\n        self.start_time = None\n    \n    def handle_event(self, event):\n        \"\"\"\n        Handle different types of streaming events\n        \"\"\"\n        event_type = event.get('event', 'unknown')\n        timestamp = time.time()\n        \n        # Initialize start time on first event\n        if self.start_time is None:\n            self.start_time = timestamp\n        \n        # Handle different event types\n        if event_type == 'on_chat_model_stream':\n            # Handle token streaming\n            token = event['data'].get('chunk', {})\n            if hasattr(token, 'content') and token.content:\n                self.token_count += 1\n                print(token.content, end='', flush=True)\n                \n        elif event_type == 'on_chain_start':\n            # Handle chain start events\n            node_name = event['metadata'].get('langgraph_node', 'unknown')\n            if node_name != 'unknown':\n                self.nodes_executed.add(node_name)\n                print(f\"\\nð Starting node: {node_name}\")\n                \n        elif event_type == 'on_chain_end':\n            # Handle chain end events\n            node_name = event['metadata'].get('langgraph_node', 'unknown')\n            if node_name != 'unknown':\n                print(f\"\\nâ Completed node: {node_name}\")\n                \n        elif event_type == 'on_chat_model_start':\n            # Handle LLM call start\n            print(f\"\\nð¤ Starting LLM call...\")\n            \n        elif event_type == 'on_chat_model_end':\n            # Handle LLM call end\n            print(f\"\\nâ¨ LLM call complete\")\n            self.message_count += 1\n            \n        # Log all events for analysis\n        self.event_log.append({\n            'event': event_type,\n            'timestamp': timestamp,\n            'node': event['metadata'].get('langgraph_node', 'unknown'),\n            'data_keys': list(event.get('data', {}).keys())\n        })\n    \n    def get_statistics(self):\n        \"\"\"\n        Return streaming statistics\n        \"\"\"\n        end_time = time.time()\n        execution_time = end_time - self.start_time if self.start_time else 0\n        \n        return {\n            \"messages_processed\": self.message_count,\n            \"tokens_streamed\": self.token_count,\n            \"total_events\": len(self.event_log),\n            \"nodes_executed\": list(self.nodes_executed),\n            \"execution_time_seconds\": round(execution_time, 2),\n            \"events_per_second\": round(len(self.event_log) / execution_time, 2) if execution_time > 0 else 0\n        }\n\n# Test custom event handler\nasync def test_custom_handler():\n    handler = StreamEventHandler()\n    config = {\"configurable\": {\"thread_id\": \"handler_test\"}}\n    \n    print(\"Testing custom event handler...\")\n    print(\"=\"*50)\n    \n    # Use your handler with astream_events\n    async for event in graph.astream_events(\n        {\"messages\": [HumanMessage(content=\"Write a short story about a robot learning to paint\")]},\n        config,\n        version=\"v2\"\n    ):\n        handler.handle_event(event)\n    \n    # Show final statistics\n    stats = handler.get_statistics()\n    print(\"\\n\\n\" + \"=\"*50)\n    print(\"STREAMING STATISTICS:\")\n    print(\"=\"*50)\n    for key, value in stats.items():\n        print(f\"{key.replace('_', ' ').title()}: {value}\")\n    \n    # Show event type breakdown\n    event_types = {}\n    for event_info in handler.event_log:\n        event_type = event_info['event']\n        event_types[event_type] = event_types.get(event_type, 0) + 1\n    \n    print(f\"\\nEvent Type Breakdown:\")\n    for event_type, count in sorted(event_types.items()):\n        print(f\"  {event_type}: {count}\")\n\n# Run the test\nawait test_custom_handler()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Streaming Performance Analysis\n",
    "\n",
    "Analyze the performance characteristics of different streaming approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport asyncio\n\nasync def benchmark_streaming_modes():\n    \"\"\"\n    Benchmark different streaming modes for performance\n    \"\"\"\n    test_input = {\"messages\": [HumanMessage(content=\"Write a detailed explanation of quantum computing\")]}\n    \n    print(\"ð STREAMING PERFORMANCE BENCHMARK\")\n    print(\"=\"*60)\n    \n    # Benchmark 'values' mode\n    print(\"Benchmarking 'values' mode...\")\n    config_values = {\"configurable\": {\"thread_id\": \"benchmark_values\"}}\n    start_time = time.time()\n    \n    # Run with values mode and measure time\n    chunk_count = 0\n    for chunk in graph.stream(test_input, config_values, stream_mode=\"values\"):\n        chunk_count += 1\n        # Don't print to avoid cluttering output\n        \n    values_time = time.time() - start_time\n    print(f\"  â Completed in {values_time:.2f}s with {chunk_count} chunks\")\n    \n    # Benchmark 'updates' mode\n    print(\"\\nBenchmarking 'updates' mode...\")\n    config_updates = {\"configurable\": {\"thread_id\": \"benchmark_updates\"}}\n    start_time = time.time()\n    \n    # Run with updates mode and measure time\n    chunk_count = 0\n    for chunk in graph.stream(test_input, config_updates, stream_mode=\"updates\"):\n        chunk_count += 1\n        \n    updates_time = time.time() - start_time\n    print(f\"  â Completed in {updates_time:.2f}s with {chunk_count} chunks\")\n    \n    # Benchmark token streaming\n    print(\"\\nBenchmarking token streaming...\")\n    config_tokens = {\"configurable\": {\"thread_id\": \"benchmark_tokens\"}}\n    start_time = time.time()\n    \n    # Run with token streaming and measure time\n    event_count = 0\n    async for event in graph.astream_events(test_input, config_tokens, version=\"v2\"):\n        event_count += 1\n        # Count all events but don't process to get pure overhead\n        \n    token_time = time.time() - start_time\n    print(f\"  â Completed in {token_time:.2f}s with {event_count} events\")\n    \n    # Report results\n    print(\"\\n\" + \"=\"*60)\n    print(\"ð PERFORMANCE RESULTS\")\n    print(\"=\"*60)\n    \n    results = [\n        (\"Values mode\", values_time, \"Full state after each node\"),\n        (\"Updates mode\", updates_time, \"Only state changes\"),\n        (\"Token streaming\", token_time, \"All events including tokens\")\n    ]\n    \n    # Sort by performance\n    results.sort(key=lambda x: x[1])\n    \n    for i, (mode, exec_time, description) in enumerate(results):\n        rank = \"ð¥\" if i == 0 else \"ð¥\" if i == 1 else \"ð¥\"\n        print(f\"{rank} {mode}: {exec_time:.2f}s - {description}\")\n    \n    # Calculate overhead\n    baseline = results[0][1]  # Fastest time\n    print(f\"\\nStreaming Overhead Analysis:\")\n    for mode, exec_time, _ in results:\n        overhead = ((exec_time - baseline) / baseline) * 100\n        print(f\"  {mode}: +{overhead:.1f}% overhead\")\n\n# Run benchmark\nawait benchmark_streaming_modes()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Reflection Questions\n\nAnswer these questions to test your understanding:\n\n1. **What's the difference between 'values' and 'updates' streaming modes?**\n   - Your answer: Values mode streams the complete state after each node executes, while updates mode streams only the changes/updates made by each node. Values mode gives you full context but more data, updates mode is more efficient but only shows what changed.\n\n2. **When would you use token-level streaming vs. state-level streaming?**\n   - Your answer: Token-level streaming is ideal for real-time chat interfaces where you want to show AI responses as they're generated, providing immediate user feedback. State-level streaming is better for monitoring graph execution progress, debugging workflows, or when you need to see how data flows through your pipeline.\n\n3. **How does streaming work with breakpoints and human-in-the-loop workflows?**\n   - Your answer: Streaming allows you to monitor execution up to a breakpoint, see what operations are about to be performed, get user approval or input, then continue streaming the remaining execution. This provides transparency into the decision-making process before critical operations.\n\n4. **What are the performance implications of different streaming modes?**\n   - Your answer: Updates mode is generally most efficient (least data transferred), values mode has moderate overhead (full state each time), and token-level streaming has the highest overhead (many small events) but provides the richest real-time experience. Choose based on your use case requirements.\n\n5. **How can streaming be used for real-time monitoring and user feedback?**\n   - Your answer: Streaming enables progress bars, real-time status updates, live chat responses, system monitoring dashboards, error detection during execution, and interactive debugging. It transforms batch operations into observable, interactive processes.\n\n## Key Takeaways\n\nAfter completing this notebook, you should understand:\n\nâ **Different streaming modes serve different purposes:**\n   - `values`: Full state monitoring and debugging\n   - `updates`: Efficient change tracking\n   - `events`: Token-level real-time feedback\n\nâ **Streaming integrates seamlessly with human-in-the-loop patterns:**\n   - Monitor execution until breakpoints\n   - Provide transparent decision-making\n   - Enable interactive approval workflows\n\nâ **Performance considerations matter:**\n   - Choose the right streaming mode for your use case\n   - Balance real-time feedback with system efficiency\n   - Consider network and processing overhead\n\nâ **Custom event handlers enable sophisticated monitoring:**\n   - Process different event types appropriately\n   - Maintain metrics and analytics\n   - Create interactive dashboards and controls"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Challenge: Multi-Modal Streaming Dashboard\n\n**Advanced Exercise:** Create a comprehensive streaming dashboard that demonstrates mastery of all streaming concepts. Your dashboard should:\n\n### Core Requirements:\n- â **Multi-Stream Management**: Handle multiple concurrent graph executions\n- â **Real-Time Updates**: Display live progress across different streaming modes  \n- â **Interactive Controls**: Pause, resume, cancel operations\n- â **Performance Analytics**: Track metrics and identify bottlenecks\n- â **Error Handling**: Gracefully handle failures and provide recovery options\n\n### Advanced Features:\n- ð¯ **Smart Filtering**: Show/hide specific event types or nodes\n- ð **Visual Analytics**: Generate performance charts and statistics\n- ð **Search & Replay**: Search through event history and replay executions\n- ð¨ **Alerting System**: Notify when streams exceed time thresholds\n- ð¾ **Export Capabilities**: Save execution logs and metrics to files\n\n### Extension Ideas:\n1. **Historical Analysis**: Compare performance across multiple runs\n2. **Resource Monitoring**: Track memory and CPU usage during streaming\n3. **Custom Visualizations**: Create graphs showing token generation rates\n4. **Integration Testing**: Test streaming with different LLM providers\n5. **Load Testing**: Simulate high-volume concurrent streaming scenarios\n\nThis challenge combines all the concepts from this notebook into a real-world application that could be used for monitoring production LangGraph systems!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class StreamingDashboard:\n    def __init__(self):\n        self.active_streams = {}\n        self.metrics = {}\n        self.event_history = []\n        self.user_interactions = []\n        self.dashboard_active = False\n    \n    async def start_stream_monitoring(self, graph, input_data, stream_id, stream_mode=\"updates\"):\n        \"\"\"\n        Start monitoring a graph stream with a unique ID\n        \"\"\"\n        print(f\"ð¯ Starting stream monitoring for: {stream_id}\")\n        self.active_streams[stream_id] = {\n            \"status\": \"running\",\n            \"start_time\": time.time(),\n            \"messages_count\": 0,\n            \"current_node\": None,\n            \"progress\": 0.0\n        }\n        \n        config = {\"configurable\": {\"thread_id\": f\"dashboard_{stream_id}\"}}\n        \n        try:\n            if stream_mode == \"events\":\n                # Monitor with detailed events\n                async for event in graph.astream_events(input_data, config, version=\"v2\"):\n                    await self._process_event(stream_id, event)\n                    \n            else:\n                # Monitor with state updates\n                for chunk in graph.stream(input_data, config, stream_mode=stream_mode):\n                    await self._process_chunk(stream_id, chunk)\n                    \n        except Exception as e:\n            self.active_streams[stream_id][\"status\"] = f\"error: {e}\"\n        else:\n            self.active_streams[stream_id][\"status\"] = \"completed\"\n            self.active_streams[stream_id][\"end_time\"] = time.time()\n        \n        print(f\"â Stream {stream_id} completed\")\n    \n    async def _process_event(self, stream_id, event):\n        \"\"\"Process individual events from astream_events\"\"\"\n        event_type = event.get('event', 'unknown')\n        node_name = event['metadata'].get('langgraph_node', 'unknown')\n        \n        # Update stream info\n        if stream_id in self.active_streams:\n            self.active_streams[stream_id][\"current_node\"] = node_name\n            \n        # Log event\n        self.event_history.append({\n            \"stream_id\": stream_id,\n            \"event_type\": event_type,\n            \"node\": node_name,\n            \"timestamp\": time.time()\n        })\n        \n        # Show real-time progress\n        if event_type == \"on_chat_model_stream\":\n            token = event['data'].get('chunk', {})\n            if hasattr(token, 'content') and token.content:\n                print(token.content, end='', flush=True)\n    \n    async def _process_chunk(self, stream_id, chunk):\n        \"\"\"Process chunks from regular streaming\"\"\"\n        for node_name, state_update in chunk.items():\n            if stream_id in self.active_streams:\n                self.active_streams[stream_id][\"current_node\"] = node_name\n                if \"messages\" in state_update:\n                    self.active_streams[stream_id][\"messages_count\"] += 1\n    \n    def display_dashboard(self):\n        \"\"\"\n        Display the current dashboard state\n        \"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"ð¥ï¸  LANGGRAPH STREAMING DASHBOARD\")\n        print(\"=\"*80)\n        \n        if not self.active_streams:\n            print(\"No active streams\")\n            return\n        \n        # Display active streams\n        for stream_id, info in self.active_streams.items():\n            status_emoji = \"ð\" if info[\"status\"] == \"running\" else \"â\" if info[\"status\"] == \"completed\" else \"â\"\n            \n            print(f\"{status_emoji} Stream: {stream_id}\")\n            print(f\"   Status: {info['status']}\")\n            print(f\"   Current Node: {info.get('current_node', 'N/A')}\")\n            print(f\"   Messages: {info.get('messages_count', 0)}\")\n            \n            if \"start_time\" in info:\n                elapsed = time.time() - info[\"start_time\"]\n                print(f\"   Runtime: {elapsed:.1f}s\")\n                \n            print(\"-\" * 40)\n        \n        # Display recent events\n        if self.event_history:\n            print(f\"\\nð Recent Events (last 5):\")\n            for event in self.event_history[-5:]:\n                print(f\"   {event['event_type']} in {event['node']} ({event['stream_id']})\")\n        \n        # Display metrics\n        total_events = len(self.event_history)\n        active_count = sum(1 for s in self.active_streams.values() if s[\"status\"] == \"running\")\n        completed_count = sum(1 for s in self.active_streams.values() if s[\"status\"] == \"completed\")\n        \n        print(f\"\\nð Metrics:\")\n        print(f\"   Total Events: {total_events}\")\n        print(f\"   Active Streams: {active_count}\")\n        print(f\"   Completed Streams: {completed_count}\")\n    \n    def handle_user_interaction(self, action, stream_id=None):\n        \"\"\"\n        Handle user interactions with the dashboard\n        \"\"\"\n        timestamp = time.time()\n        \n        if action == \"pause\" and stream_id:\n            # In real implementation, would pause the specific stream\n            print(f\"â¸ï¸ Pausing stream {stream_id}\")\n            \n        elif action == \"resume\" and stream_id:\n            print(f\"â¶ï¸ Resuming stream {stream_id}\")\n            \n        elif action == \"cancel\" and stream_id:\n            print(f\"â Cancelling stream {stream_id}\")\n            if stream_id in self.active_streams:\n                self.active_streams[stream_id][\"status\"] = \"cancelled\"\n                \n        elif action == \"refresh\":\n            self.display_dashboard()\n            \n        # Log interaction\n        self.user_interactions.append({\n            \"action\": action,\n            \"stream_id\": stream_id,\n            \"timestamp\": timestamp\n        })\n\n# Implement a comprehensive streaming dashboard test\nasync def test_streaming_dashboard():\n    \"\"\"\n    Test the streaming dashboard with multiple concurrent operations\n    \"\"\"\n    dashboard = StreamingDashboard()\n    \n    print(\"ð Testing Multi-Stream Dashboard\")\n    print(\"=\"*60)\n    \n    # Start multiple concurrent streams\n    tasks = []\n    \n    # Stream 1: Simple conversation\n    task1 = asyncio.create_task(\n        dashboard.start_stream_monitoring(\n            graph, \n            {\"messages\": [HumanMessage(content=\"Tell me about artificial intelligence\")]},\n            \"ai_chat\",\n            \"updates\"\n        )\n    )\n    tasks.append(task1)\n    \n    # Stream 2: Agent with tools\n    task2 = asyncio.create_task(\n        dashboard.start_stream_monitoring(\n            agent_graph,\n            {\"messages\": [HumanMessage(content=\"What's 15 * 23?\")]},\n            \"math_agent\", \n            \"updates\"\n        )\n    )\n    tasks.append(task2)\n    \n    # Stream 3: Processing pipeline  \n    task3 = asyncio.create_task(\n        dashboard.start_stream_monitoring(\n            processing_graph,\n            {\n                \"task\": \"Data Analysis Pipeline\",\n                \"progress\": 0.0,\n                \"status\": \"Starting\",\n                \"result\": \"\",\n                \"steps_completed\": []\n            },\n            \"data_pipeline\",\n            \"updates\"\n        )\n    )\n    tasks.append(task3)\n    \n    # Monitor progress\n    for i in range(5):  # Check status 5 times\n        await asyncio.sleep(2)  # Wait 2 seconds\n        print(f\"\\nâ° Status Update #{i+1}\")\n        dashboard.display_dashboard()\n    \n    # Simulate user interaction\n    dashboard.handle_user_interaction(\"refresh\")\n    \n    # Wait for all tasks to complete\n    await asyncio.gather(*tasks)\n    \n    # Final dashboard state\n    print(\"\\nð Final Dashboard State:\")\n    dashboard.display_dashboard()\n    \n    # Export metrics\n    print(f\"\\nð¤ Session Summary:\")\n    print(f\"   Total streams processed: {len(dashboard.active_streams)}\")\n    print(f\"   Total events captured: {len(dashboard.event_history)}\")\n    print(f\"   User interactions: {len(dashboard.user_interactions)}\")\n\n# Run the comprehensive dashboard test\nawait test_streaming_dashboard()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}